From 5fed7187a62d87f7f31654e9557a2623bae85187 Mon Sep 17 00:00:00 2001
From: Friedrich Vock <friedrich.vock@gmx.de>
Date: Tue, 4 Jun 2024 15:09:20 +0200
Subject: [PATCH 01/71] !29536

---
 src/amd/compiler/aco_interface.cpp        |   2 +
 src/amd/compiler/aco_ir.h                 |   1 +
 src/amd/compiler/aco_vectorize_spills.cpp | 252 ++++++++++++++++++++++
 src/amd/compiler/meson.build              |   1 +
 4 files changed, 256 insertions(+)
 create mode 100644 src/amd/compiler/aco_vectorize_spills.cpp

diff --git a/src/amd/compiler/aco_interface.cpp b/src/amd/compiler/aco_interface.cpp
index db96e17d0384d..396908155d1e8 100644
--- a/src/amd/compiler/aco_interface.cpp
+++ b/src/amd/compiler/aco_interface.cpp
@@ -153,6 +153,8 @@ aco_postprocess_shader(const struct aco_compiler_options* options,
          schedule_program(program.get(), live_vars);
       validate(program.get());
 
+      vectorize_spills(program.get());
+
       /* Register Allocation */
       register_allocation(program.get(), live_vars);
 
diff --git a/src/amd/compiler/aco_ir.h b/src/amd/compiler/aco_ir.h
index bbac03eb41006..655cf6c59da02 100644
--- a/src/amd/compiler/aco_ir.h
+++ b/src/amd/compiler/aco_ir.h
@@ -2191,6 +2191,7 @@ void insert_wait_states(Program* program);
 bool dealloc_vgprs(Program* program);
 void insert_NOPs(Program* program);
 void form_hard_clauses(Program* program);
+void vectorize_spills(Program* program);
 unsigned emit_program(Program* program, std::vector<uint32_t>& code,
                       std::vector<struct aco_symbol>* symbols = NULL, bool append_endpgm = true);
 /**
diff --git a/src/amd/compiler/aco_vectorize_spills.cpp b/src/amd/compiler/aco_vectorize_spills.cpp
new file mode 100644
index 0000000000000..9f755dbdbf9c5
--- /dev/null
+++ b/src/amd/compiler/aco_vectorize_spills.cpp
@@ -0,0 +1,252 @@
+/*
+ * Copyright Â© 2024 Valve Corporation
+ *
+ * SPDX-License-Identifier: MIT
+ */
+
+#include "aco_builder.h"
+#include "aco_ir.h"
+
+#include <unordered_set>
+
+namespace aco {
+
+struct vectorize_ctx {
+   std::vector<aco_ptr<Instruction>> instrs_to_vectorize;
+
+   std::vector<aco_ptr<Instruction>> vectors;
+   std::vector<aco_ptr<Instruction>> vectorized_instrs;
+
+   std::vector<unsigned> component_idxs;
+
+   std::unordered_set<unsigned> killed_soffset_ids;
+   std::unordered_set<unsigned> seen_soffset_ids;
+
+   std::vector<aco_ptr<Instruction>>::iterator insert_point;
+   Block* block;
+   Program* program;
+};
+
+void
+vectorize_and_insert(vectorize_ctx& ctx, bool store)
+{
+   std::sort(ctx.instrs_to_vectorize.begin(), ctx.instrs_to_vectorize.end(),
+             [](const auto& one, const auto& other)
+             { return one->scratch().offset < other->scratch().offset; });
+
+   Builder instr_bld(ctx.program, &ctx.vectorized_instrs);
+
+   for (unsigned i = 0; i < ctx.instrs_to_vectorize.size(); ++i) {
+      ctx.component_idxs.push_back(i);
+      for (auto j = i + 1; j < ctx.instrs_to_vectorize.size(); ++j) {
+         const auto& component = ctx.instrs_to_vectorize[ctx.component_idxs.back()];
+         const auto& instr = ctx.instrs_to_vectorize[j];
+         /* skip stores with unrelated soffset */
+         if (instr->operands[1].tempId() != component->operands[1].tempId())
+            continue;
+         int16_t next_offset;
+         if (store)
+            next_offset = component->scratch().offset + (int16_t)component->operands[2].bytes();
+         else
+            next_offset = component->scratch().offset + (int16_t)component->definitions[0].bytes();
+
+         /* there's a gap, can't vectorize across it */
+         if (instr->scratch().offset > next_offset)
+            break;
+         /* XXX: Hitting this means there are intersecting stores. This shouldn't happen! */
+         if (instr->scratch().offset != next_offset)
+            break;
+
+         if (instr->operands[1].isKill())
+            ctx.killed_soffset_ids.insert(instr->operands[1].tempId());
+
+         ctx.component_idxs.push_back(j);
+      }
+
+      if (ctx.component_idxs.empty())
+         continue;
+
+      size_t comp_idx = 0;
+      while (comp_idx < ctx.component_idxs.size()) {
+         size_t vector_size = 4;
+         while (vector_size > ctx.component_idxs.size() - comp_idx)
+            vector_size >>= 1;
+
+         auto& first_component = ctx.instrs_to_vectorize[ctx.component_idxs[comp_idx]];
+
+         if (vector_size == 1) {
+            ctx.vectorized_instrs.emplace_back(std::move(first_component));
+            ++comp_idx;
+            continue;
+         }
+
+         if (store) {
+            Temp vec_tmp = ctx.program->allocateTmp(RegClass(RegType::vgpr, vector_size));
+            Instruction* vec =
+               create_instruction(aco_opcode::p_create_vector, Format::PSEUDO, vector_size, 1);
+            for (unsigned c = 0; c < vector_size; ++c) {
+               auto& component = ctx.instrs_to_vectorize[ctx.component_idxs[comp_idx + c]];
+               vec->operands[c] = component->operands[2];
+            }
+            vec->definitions[0] = Definition(vec_tmp);
+            ctx.vectors.emplace_back(vec);
+
+            aco_opcode opcode;
+            switch (vector_size) {
+            case 4: opcode = aco_opcode::scratch_store_dwordx4; break;
+            case 2: opcode = aco_opcode::scratch_store_dwordx2; break;
+            default: unreachable("invalid vector size");
+            }
+
+            Operand vec_op = Operand(vec_tmp);
+            vec_op.setFirstKill(true);
+            instr_bld.scratch(opcode, Operand(v1), first_component->operands[1], vec_op,
+                              first_component->scratch().offset, first_component->scratch().sync);
+         } else {
+            Temp vec_tmp = ctx.program->allocateTmp(RegClass(RegType::vgpr, vector_size));
+
+            aco_opcode opcode;
+            switch (vector_size) {
+            case 4: opcode = aco_opcode::scratch_load_dwordx4; break;
+            case 2: opcode = aco_opcode::scratch_load_dwordx2; break;
+            default: unreachable("invalid vector size");
+            }
+
+            instr_bld.scratch(opcode, Definition(vec_tmp), Operand(v1),
+                              first_component->operands[1], first_component->scratch().offset,
+                              first_component->scratch().sync);
+
+            Instruction* vec =
+               create_instruction(aco_opcode::p_split_vector, Format::PSEUDO, 1, vector_size);
+            for (unsigned c = 0; c < vector_size; ++c) {
+               auto& component = ctx.instrs_to_vectorize[ctx.component_idxs[comp_idx + c]];
+               vec->definitions[c] = component->definitions[0];
+            }
+            vec->operands[0] = Operand(vec_tmp);
+            vec->operands[0].setFirstKill(true);
+            ctx.vectors.emplace_back(vec);
+         }
+         comp_idx += vector_size;
+      }
+
+      for (unsigned j = 0; j < ctx.component_idxs.size(); ++j) {
+         auto idx = ctx.component_idxs[j];
+         ctx.instrs_to_vectorize.erase(ctx.instrs_to_vectorize.begin() + (idx - j));
+      }
+      /* Adjust for deleted instruction */
+      --i;
+
+      ctx.component_idxs.clear();
+   }
+
+   for (auto it = ctx.vectorized_instrs.rbegin(); it != ctx.vectorized_instrs.rend(); ++it) {
+      auto soffset_id = (*it)->operands[1].tempId();
+      if (ctx.seen_soffset_ids.find(soffset_id) == ctx.seen_soffset_ids.end()) {
+         if (ctx.killed_soffset_ids.find(soffset_id) != ctx.killed_soffset_ids.end())
+            (*it)->operands[1].setFirstKill(true);
+         ctx.seen_soffset_ids.insert(soffset_id);
+      }
+   }
+
+   if (store) {
+      ctx.insert_point =
+         ctx.block->instructions.insert(ctx.insert_point, std::move_iterator(ctx.vectors.begin()),
+                                        std::move_iterator(ctx.vectors.end()));
+      ctx.insert_point += ctx.vectors.size();
+      ctx.insert_point = ctx.block->instructions.insert(
+         ctx.insert_point, std::move_iterator(ctx.vectorized_instrs.rbegin()),
+         std::move_iterator(ctx.vectorized_instrs.rend()));
+      ctx.insert_point += ctx.vectorized_instrs.size();
+   } else {
+      ctx.insert_point = ctx.block->instructions.insert(
+         ctx.insert_point, std::move_iterator(ctx.vectorized_instrs.rbegin()),
+         std::move_iterator(ctx.vectorized_instrs.rend()));
+      ctx.insert_point += ctx.vectorized_instrs.size();
+      ctx.insert_point =
+         ctx.block->instructions.insert(ctx.insert_point, std::move_iterator(ctx.vectors.begin()),
+                                        std::move_iterator(ctx.vectors.end()));
+      ctx.insert_point += ctx.vectors.size();
+   }
+
+   ctx.vectors.clear();
+   ctx.vectorized_instrs.clear();
+   ctx.instrs_to_vectorize.clear();
+   ctx.seen_soffset_ids.clear();
+   ctx.killed_soffset_ids.clear();
+}
+
+void
+vectorize_spills(Program* program)
+{
+   vectorize_ctx ctx;
+   ctx.program = program;
+
+   for (auto& block : program->blocks) {
+      ctx.block = &block;
+      IDSet conflicting_temps;
+
+      /* Try vectorizing stores */
+      for (auto it = block.instructions.begin(); it != block.instructions.end();) {
+         bool vectorize_now = !(*it)->isVMEM() && it != block.instructions.begin();
+
+         /* Only look for stores that kill their operand. We can move/combine these with other
+          * instructions without affecting register demand.
+          */
+         if ((*it)->opcode == aco_opcode::scratch_store_dword && (*it)->operands[2].isKill() &&
+             !(*it)->operands[2].regClass().is_subdword()) {
+            if (conflicting_temps.count((*it)->operands[2].tempId())) {
+               vectorize_now = true;
+               --it;
+            } else {
+               bool first = ctx.instrs_to_vectorize.empty();
+               ctx.instrs_to_vectorize.emplace_back(std::move(*it));
+               it = block.instructions.erase(it);
+               if (first)
+                  ctx.insert_point = it;
+               continue;
+            }
+         }
+
+         if (vectorize_now) {
+            auto clause_size = it - ctx.insert_point;
+            vectorize_and_insert(ctx, true);
+            it = ctx.insert_point + clause_size;
+            conflicting_temps = IDSet();
+         } else {
+            for (auto& def : (*it)->definitions)
+               if (def.isTemp())
+                  conflicting_temps.insert(def.tempId());
+         }
+         ++it;
+      }
+      /* Try vectorizing loads */
+      for (auto it = block.instructions.begin(); it != block.instructions.end();) {
+         bool vectorize_now = !(*it)->isVMEM() && it != block.instructions.begin();
+         for (auto& op : (*it)->operands) {
+            if (op.isTemp() && conflicting_temps.count(op.tempId())) {
+               vectorize_now = true;
+               --it;
+            }
+         }
+
+         /* Loads that kill their definition are dead and shouldn't appear with spilling */
+         if (!vectorize_now && (*it)->opcode == aco_opcode::scratch_load_dword &&
+             !(*it)->definitions[0].isKill() && !(*it)->definitions[0].regClass().is_subdword()) {
+            ctx.instrs_to_vectorize.emplace_back(std::move(*it));
+            conflicting_temps.insert((*it)->definitions[0].tempId());
+            it = block.instructions.erase(it);
+            continue;
+         }
+
+         if (vectorize_now) {
+            ctx.insert_point = it;
+            vectorize_and_insert(ctx, false);
+            it = ctx.insert_point;
+            conflicting_temps = IDSet();
+         }
+         ++it;
+      }
+   }
+}
+
+} // namespace aco
diff --git a/src/amd/compiler/meson.build b/src/amd/compiler/meson.build
index d303629364563..f7024fede70d4 100644
--- a/src/amd/compiler/meson.build
+++ b/src/amd/compiler/meson.build
@@ -66,6 +66,7 @@ libaco_files = files(
   'aco_statistics.cpp',
   'aco_util.h',
   'aco_validate.cpp',
+  'aco_vectorize_spills.cpp',
 )
 
 cpp_args_aco = cpp.get_supported_arguments(['-fno-exceptions', '-fno-rtti', '-Wimplicit-fallthrough', '-Wshadow'])
-- 
GitLab


From e5765385e430ff663ef676935e0ea8f81f029841 Mon Sep 17 00:00:00 2001
From: Friedrich Vock <friedrich.vock@gmx.de>
Date: Thu, 6 Jun 2024 12:28:35 +0200
Subject: [PATCH 02/71] !29575

---
 .../compiler/aco_instruction_selection.cpp    | 21 +++++++++++--------
 src/amd/vulkan/radv_cmd_buffer.c              |  3 ++-
 2 files changed, 14 insertions(+), 10 deletions(-)

diff --git a/src/amd/compiler/aco_instruction_selection.cpp b/src/amd/compiler/aco_instruction_selection.cpp
index 21d8acee0c8d1..6fbb6f08c6bf4 100644
--- a/src/amd/compiler/aco_instruction_selection.cpp
+++ b/src/amd/compiler/aco_instruction_selection.cpp
@@ -12464,15 +12464,18 @@ select_rt_prolog(Program* program, ac_shader_config* config,
    }
 
    /* Make fixup operations a no-op if this is not a converted 2D dispatch. */
-   bld.sopc(aco_opcode::s_cmp_lg_u32, Definition(scc, s1),
-            Operand::c32(ACO_RT_CONVERTED_2D_LAUNCH_SIZE), Operand(out_launch_size_y, s1));
-   bld.sop2(Builder::s_cselect, Definition(vcc, bld.lm),
-            Operand::c32_or_c64(-1u, program->wave_size == 64),
-            Operand::c32_or_c64(0, program->wave_size == 64), Operand(scc, s1));
-   bld.vop2(aco_opcode::v_cndmask_b32, Definition(out_launch_ids[0], v1),
-            Operand(tmp_invocation_idx, v1), Operand(out_launch_ids[0], v1), Operand(vcc, bld.lm));
-   bld.vop2(aco_opcode::v_cndmask_b32, Definition(out_launch_ids[1], v1), Operand::zero(),
-            Operand(out_launch_ids[1], v1), Operand(vcc, bld.lm));
+   if (program->gfx_level >= GFX9) {
+      bld.sopc(aco_opcode::s_cmp_lg_u32, Definition(scc, s1),
+               Operand::c32(ACO_RT_CONVERTED_2D_LAUNCH_SIZE), Operand(out_launch_size_y, s1));
+      bld.sop2(Builder::s_cselect, Definition(vcc, bld.lm),
+               Operand::c32_or_c64(-1u, program->wave_size == 64),
+               Operand::c32_or_c64(0, program->wave_size == 64), Operand(scc, s1));
+      bld.vop2(aco_opcode::v_cndmask_b32, Definition(out_launch_ids[0], v1),
+               Operand(tmp_invocation_idx, v1), Operand(out_launch_ids[0], v1),
+               Operand(vcc, bld.lm));
+      bld.vop2(aco_opcode::v_cndmask_b32, Definition(out_launch_ids[1], v1), Operand::zero(),
+               Operand(out_launch_ids[1], v1), Operand(vcc, bld.lm));
+   }
 
    /* jump to raygen */
    bld.sop1(aco_opcode::s_setpc_b64, Operand(out_uniform_shader_addr, s2));
diff --git a/src/amd/vulkan/radv_cmd_buffer.c b/src/amd/vulkan/radv_cmd_buffer.c
index 828e9cbf1668c..2776428c453c7 100644
--- a/src/amd/vulkan/radv_cmd_buffer.c
+++ b/src/amd/vulkan/radv_cmd_buffer.c
@@ -12145,7 +12145,8 @@ radv_trace_rays(struct radv_cmd_buffer *cmd_buffer, VkTraceRaysIndirectCommand2K
 
    /* Since the workgroup size is 8x4 (or 8x8), 1D dispatches can only fill 8 threads per wave at most. To increase
     * occupancy, it's beneficial to convert to a 2D dispatch in these cases. */
-   if (tables && tables->height == 1 && tables->width >= cmd_buffer->state.rt_prolog->info.cs.block_size[0])
+   if (tables && tables->height == 1 && tables->width >= cmd_buffer->state.rt_prolog->info.cs.block_size[0] &&
+       pdev->info.gfx_level >= GFX9)
       tables->height = ACO_RT_CONVERTED_2D_LAUNCH_SIZE;
 
    struct radv_dispatch_info info = {0};
-- 
GitLab


From ab759267c9ee2f30cef35d0a2d02b53b8a4347f6 Mon Sep 17 00:00:00 2001
From: Friedrich Vock <friedrich.vock@gmx.de>
Date: Thu, 6 Jun 2024 20:44:22 +0200
Subject: [PATCH 03/71] !29576

---
 src/amd/compiler/aco_ir.h                    |  12 +-
 src/amd/compiler/aco_live_var_analysis.cpp   |  73 +++---
 src/amd/compiler/aco_register_allocation.cpp | 255 ++++++++++++-------
 src/amd/compiler/aco_spill.cpp               |  71 +++++-
 4 files changed, 278 insertions(+), 133 deletions(-)

diff --git a/src/amd/compiler/aco_ir.h b/src/amd/compiler/aco_ir.h
index 655cf6c59da02..2b68f47a8b993 100644
--- a/src/amd/compiler/aco_ir.h
+++ b/src/amd/compiler/aco_ir.h
@@ -456,9 +456,9 @@ static constexpr PhysReg scc{253};
 class Operand final {
 public:
    constexpr Operand()
-       : reg_(PhysReg{128}), isTemp_(false), isFixed_(true), isConstant_(false), isKill_(false),
-         isUndef_(true), isFirstKill_(false), constSize(0), isLateKill_(false), is16bit_(false),
-         is24bit_(false), signext(false)
+       : reg_(PhysReg{128}), isTemp_(false), isFixed_(true), isPrecolored_(false), isConstant_(false),
+         isKill_(false), isUndef_(true), isFirstKill_(false), constSize(0), isLateKill_(false),
+         is16bit_(false), is24bit_(false), signext(false)
    {}
 
    explicit Operand(Temp r) noexcept
@@ -730,6 +730,11 @@ public:
 
    constexpr bool isFixed() const noexcept { return isFixed_; }
 
+   /* Indicates whether the register this operand is fixed to can be changed. */
+   constexpr bool isPrecolored() const noexcept { return isPrecolored_; }
+
+   constexpr void setPrecolored(bool precolored) noexcept { isPrecolored_ = precolored; }
+
    constexpr PhysReg physReg() const noexcept { return reg_; }
 
    constexpr void setFixed(PhysReg reg) noexcept
@@ -866,6 +871,7 @@ private:
       struct {
          uint8_t isTemp_ : 1;
          uint8_t isFixed_ : 1;
+         uint8_t isPrecolored_ : 1;
          uint8_t isConstant_ : 1;
          uint8_t isKill_ : 1;
          uint8_t isUndef_ : 1;
diff --git a/src/amd/compiler/aco_live_var_analysis.cpp b/src/amd/compiler/aco_live_var_analysis.cpp
index f3c8cd9bae7a8..4d96a40c0d67f 100644
--- a/src/amd/compiler/aco_live_var_analysis.cpp
+++ b/src/amd/compiler/aco_live_var_analysis.cpp
@@ -32,20 +32,6 @@ get_live_changes(aco_ptr<Instruction>& instr)
    return changes;
 }
 
-void
-handle_def_fixed_to_op(RegisterDemand* demand, RegisterDemand demand_before, Instruction* instr,
-                       int op_idx)
-{
-   /* Usually the register demand before an instruction would be considered part of the previous
-    * instruction, since it's not greater than the register demand for that previous instruction.
-    * Except, it can be greater in the case of an definition fixed to a non-killed operand: the RA
-    * needs to reserve space between the two instructions for the definition (containing a copy of
-    * the operand).
-    */
-   demand_before += instr->definitions[0].getTemp();
-   demand->update(demand_before);
-}
-
 RegisterDemand
 get_temp_registers(aco_ptr<Instruction>& instr)
 {
@@ -58,18 +44,37 @@ get_temp_registers(aco_ptr<Instruction>& instr)
          temp_registers += def.getTemp();
    }
 
+   /* Usually the register demand before an instruction would be considered part of the previous
+    * instruction, since it's not greater than the register demand for that previous instruction.
+    * Except, it can be greater in case we need to copy operands around: the RA needs to reserve
+    * space between the two instructions for the copies.
+    */
+   RegisterDemand demand_between;
+
    for (Operand op : instr->operands) {
       if (op.isTemp() && op.isLateKill() && op.isFirstKill())
          temp_registers += op.getTemp();
+      if (op.isTemp() && op.isFixed() && !op.isFirstKill()) {
+         for (Operand op2 : instr->operands) {
+            if (op2 == op)
+               break;
+
+            if (op2.tempId() == op.tempId() && op2.isFixed() && op2.physReg() != op.physReg()) {
+               if (op.isKillBeforeDef())
+                  demand_between += op.getTemp();
+               else
+                  temp_registers += op.getTemp();
+               break;
+            }
+         }
+      }
    }
 
    int op_idx = get_op_fixed_to_def(instr.get());
-   if (op_idx != -1 && !instr->operands[op_idx].isKill()) {
-      RegisterDemand before_instr;
-      before_instr -= get_live_changes(instr);
-      handle_def_fixed_to_op(&temp_registers, before_instr, instr.get(), op_idx);
-   }
+   if (op_idx != -1 && !instr->operands[op_idx].isKill())
+      demand_between += instr->definitions[0].getTemp();
 
+   temp_registers.update(demand_between - get_live_changes(instr));
    return temp_registers;
 }
 
@@ -112,13 +117,17 @@ process_live_temps_per_block(Program* program, live& lives, Block* block, unsign
 {
    std::vector<RegisterDemand>& register_demand = lives.register_demand[block->index];
    RegisterDemand new_demand;
+   unsigned linear_vgpr_demand = 0;
 
    register_demand.resize(block->instructions.size());
    IDSet live = lives.live_out[block->index];
 
    /* initialize register demand */
-   for (unsigned t : live)
+   for (unsigned t : live) {
       new_demand += Temp(t, program->temp_rc[t]);
+      if (program->temp_rc[t].is_linear_vgpr())
+         linear_vgpr_demand += program->temp_rc[t].size();
+   }
    new_demand.sgpr -= phi_info[block->index].logical_phi_sgpr_ops;
 
    /* traverse the instructions backwards */
@@ -138,6 +147,10 @@ process_live_temps_per_block(Program* program, live& lives, Block* block, unsign
          }
          if (definition.isFixed() && definition.physReg() == vcc)
             program->needs_vcc = true;
+         if (definition.isFixed() && definition.regClass().type() == RegType::vgpr &&
+             !definition.regClass().is_linear_vgpr())
+            block->register_demand.update(
+               RegisterDemand((int16_t)(definition.physReg().reg() - 256 + linear_vgpr_demand), 0));
 
          const Temp temp = definition.getTemp();
          const size_t n = live.erase(temp.id());
@@ -145,8 +158,9 @@ process_live_temps_per_block(Program* program, live& lives, Block* block, unsign
          if (n) {
             new_demand -= temp;
             definition.setKill(false);
+            if (temp.regClass().is_linear_vgpr())
+               linear_vgpr_demand -= temp.size();
          } else {
-            register_demand[idx] += temp;
             definition.setKill(true);
          }
       }
@@ -178,18 +192,14 @@ process_live_temps_per_block(Program* program, live& lives, Block* block, unsign
                      insn->operands[j].setKill(true);
                   }
                }
-               if (operand.isLateKill())
-                  register_demand[idx] += temp;
                new_demand += temp;
+               if (operand.regClass().is_linear_vgpr())
+                  linear_vgpr_demand += temp.size();
             }
          }
       }
 
-      int op_idx = get_op_fixed_to_def(insn);
-      if (op_idx != -1 && !insn->operands[op_idx].isKill()) {
-         RegisterDemand before_instr = new_demand;
-         handle_def_fixed_to_op(&register_demand[idx], before_instr, insn, op_idx);
-      }
+      register_demand[idx] += get_temp_registers(block->instructions[idx]);
    }
 
    /* handle phi definitions */
@@ -467,6 +477,9 @@ live_var_analysis(Program* program)
 
    program->needs_vcc = program->gfx_level >= GFX10;
 
+   for (auto& block : program->blocks)
+      block.register_demand = RegisterDemand();
+
    /* this implementation assumes that the block idx corresponds to the block's position in
     * program->blocks vector */
    while (worklist) {
@@ -481,11 +494,9 @@ live_var_analysis(Program* program)
       result.register_demand[block.index].back().sgpr -= phi_info[block.index].linear_phi_ops;
 
       /* update block's register demand */
-      if (program->progress < CompilationProgress::after_ra) {
-         block.register_demand = RegisterDemand();
+      if (program->progress < CompilationProgress::after_ra)
          for (RegisterDemand& demand : result.register_demand[block.index])
             block.register_demand.update(demand);
-      }
 
       new_demand.update(block.register_demand);
    }
diff --git a/src/amd/compiler/aco_register_allocation.cpp b/src/amd/compiler/aco_register_allocation.cpp
index 440e6bd036bc4..99f01d22dd504 100644
--- a/src/amd/compiler/aco_register_allocation.cpp
+++ b/src/amd/compiler/aco_register_allocation.cpp
@@ -32,6 +32,19 @@ get_subdword_definition_info(Program* program, const aco_ptr<Instruction>& instr
 void add_subdword_definition(Program* program, aco_ptr<Instruction>& instr, PhysReg reg,
                              bool allow_16bit_write);
 
+struct parallelcopy {
+   constexpr parallelcopy() : skip_renaming(false) {}
+   constexpr parallelcopy(Operand op_, Definition def_) : op(op_), def(def_), skip_renaming(false)
+   {}
+   constexpr parallelcopy(Operand op_, Definition def_, bool skip_renaming_)
+       : op(op_), def(def_), skip_renaming(skip_renaming_)
+   {}
+
+   Operand op;
+   Definition def;
+   bool skip_renaming;
+};
+
 struct assignment {
    PhysReg reg;
    RegClass rc;
@@ -258,7 +271,11 @@ public:
    std::array<uint32_t, 512> regs;
    std::map<uint32_t, std::array<uint32_t, 4>> subdword_regs;
 
-   const uint32_t& operator[](PhysReg index) const { return regs[index]; }
+   const uint32_t& operator[](PhysReg index) const
+   {
+      assert(index.reg() < 512);
+      return regs[index];
+   }
 
    uint32_t& operator[](PhysReg index) { return regs[index]; }
 
@@ -339,7 +356,12 @@ public:
          fill(op.physReg(), op.size(), op.tempId());
    }
 
-   void clear(Operand op) { clear(op.physReg(), op.regClass()); }
+   void clear(Operand op)
+   {
+      if (op.isTemp() && get_id(op.physReg()) && !is_blocked(op.physReg()))
+         assert(get_id(op.physReg()) == op.tempId());
+      clear(op.physReg(), op.regClass());
+   }
 
    void fill(Definition def)
    {
@@ -779,22 +801,21 @@ enum UpdateRenames {
 MESA_DEFINE_CPP_ENUM_BITFIELD_OPERATORS(UpdateRenames);
 
 void
-update_renames(ra_ctx& ctx, RegisterFile& reg_file,
-               std::vector<std::pair<Operand, Definition>>& parallelcopies,
+update_renames(ra_ctx& ctx, RegisterFile& reg_file, std::vector<parallelcopy>& parallelcopies,
                aco_ptr<Instruction>& instr, UpdateRenames flags)
 {
    /* clear operands */
-   for (std::pair<Operand, Definition>& copy : parallelcopies) {
+   for (parallelcopy& copy : parallelcopies) {
       /* the definitions with id are not from this function and already handled */
-      if (copy.second.isTemp())
+      if (copy.def.isTemp())
          continue;
-      reg_file.clear(copy.first);
+      reg_file.clear(copy.op);
    }
 
    /* allocate id's and rename operands: this is done transparently here */
    auto it = parallelcopies.begin();
    while (it != parallelcopies.end()) {
-      if (it->second.isTemp()) {
+      if (it->def.isTemp()) {
          ++it;
          continue;
       }
@@ -802,9 +823,9 @@ update_renames(ra_ctx& ctx, RegisterFile& reg_file,
       /* check if we moved a definition: change the register and remove copy */
       bool is_def = false;
       for (Definition& def : instr->definitions) {
-         if (def.isTemp() && def.getTemp() == it->first.getTemp()) {
+         if (def.isTemp() && def.getTemp() == it->op.getTemp()) {
             // FIXME: ensure that the definition can use this reg
-            def.setFixed(it->second.physReg());
+            def.setFixed(it->def.physReg());
             reg_file.fill(def);
             ctx.assignments[def.tempId()].reg = def.physReg();
             it = parallelcopies.erase(it);
@@ -816,34 +837,59 @@ update_renames(ra_ctx& ctx, RegisterFile& reg_file,
          continue;
 
       /* check if we moved another parallelcopy definition */
-      for (std::pair<Operand, Definition>& other : parallelcopies) {
-         if (!other.second.isTemp())
+      for (parallelcopy& other : parallelcopies) {
+         if (!other.def.isTemp())
             continue;
-         if (it->first.getTemp() == other.second.getTemp()) {
-            other.second.setFixed(it->second.physReg());
-            ctx.assignments[other.second.tempId()].reg = other.second.physReg();
+         if (it->op.getTemp() == other.def.getTemp()) {
+            other.def.setFixed(it->def.physReg());
+            ctx.assignments[other.def.tempId()].reg = other.def.physReg();
             it = parallelcopies.erase(it);
             is_def = true;
+
+            /* We might alter the parallelcopies, back up what we need here */
+            Operand other_op = other.op;
+            Definition other_def = other.def;
             /* check if we moved an operand, again */
             bool fill = true;
             for (Operand& op : instr->operands) {
-               if (op.isTemp() && op.tempId() == other.second.tempId()) {
-                  // FIXME: ensure that the operand can use this reg
-                  op.setFixed(other.second.physReg());
-                  fill = (flags & fill_killed_ops) || !op.isKillBeforeDef();
+               if (op.isTemp() && op.tempId() == other_def.tempId()) {
+                  bool isKillBeforeDef = op.isFirstKillBeforeDef();
+                  if (!op.isPrecolored()) {
+                     op.setFixed(other_def.physReg());
+                  } else {
+                     assert(!op.isKillBeforeDef());
+                     /* If the operand is precolored, insert a fixup parallelcopy to a new temp
+                      * that is killed immediately.
+                      */
+                     Temp tmp = Temp(other_op.tempId(), op.regClass());
+                     Operand pc_op = Operand(tmp);
+                     pc_op.setFixed(other_op.physReg());
+                     Definition pc_def =
+                        Definition(ctx.program->allocateId(op.regClass()), op.regClass());
+                     pc_def.setFixed(op.physReg());
+                     ctx.assignments.emplace_back(pc_def.physReg(), pc_def.regClass());
+
+                     parallelcopies.emplace_back(pc_op, pc_def, true);
+
+                     op.setTemp(pc_def.getTemp());
+                     op.setFirstKill(true);
+                     if (flags & fill_killed_ops)
+                        reg_file.fill(pc_def);
+                  }
+                  fill = (flags & fill_killed_ops) || !isKillBeforeDef;
                }
             }
             if (fill)
-               reg_file.fill(other.second);
+               reg_file.fill(other_def);
             break;
          }
       }
       if (is_def)
          continue;
 
-      std::pair<Operand, Definition>& copy = *it;
-      copy.second.setTemp(ctx.program->allocateTmp(copy.second.regClass()));
-      ctx.assignments.emplace_back(copy.second.physReg(), copy.second.regClass());
+      parallelcopy& copy = *it;
+      copy.def.setTemp(ctx.program->allocateTmp(copy.def.regClass()));
+      ctx.assignments.emplace_back(copy.def.physReg(), copy.def.regClass());
       assert(ctx.assignments.size() == ctx.program->peekAllocationId());
 
       /* check if we moved an operand */
@@ -853,20 +899,20 @@ update_renames(ra_ctx& ctx, RegisterFile& reg_file,
          Operand& op = instr->operands[i];
          if (!op.isTemp())
             continue;
-         if (op.tempId() == copy.first.tempId()) {
+         if (op.tempId() == copy.op.tempId()) {
             /* only rename precolored operands if the copy-location matches */
-            bool omit_renaming = (flags & rename_precolored_ops) && op.isFixed() &&
-                                 op.physReg() != copy.second.physReg();
+            bool omit_renaming = (flags & rename_precolored_ops) && op.isPrecolored() &&
+                                 op.physReg() != copy.def.physReg();
 
             /* Omit renaming in some cases for p_create_vector in order to avoid
              * unnecessary shuffle code. */
             if (!(flags & rename_not_killed_ops) && !op.isKillBeforeDef()) {
                omit_renaming = true;
-               for (std::pair<Operand, Definition>& pc : parallelcopies) {
-                  PhysReg def_reg = pc.second.physReg();
-                  omit_renaming &= def_reg > copy.first.physReg()
-                                      ? (copy.first.physReg() + copy.first.size() <= def_reg.reg())
-                                      : (def_reg + pc.second.size() <= copy.first.physReg().reg());
+               for (parallelcopy& pc : parallelcopies) {
+                  PhysReg def_reg = pc.def.physReg();
+                  omit_renaming &= def_reg > copy.op.physReg()
+                                      ? (copy.op.physReg() + copy.op.size() <= def_reg.reg())
+                                      : (def_reg + pc.def.size() <= copy.op.physReg().reg());
                }
             }
 
@@ -880,15 +926,32 @@ update_renames(ra_ctx& ctx, RegisterFile& reg_file,
             if (omit_renaming)
                continue;
 
-            op.setTemp(copy.second.getTemp());
-            op.setFixed(copy.second.physReg());
+            op.setTemp(copy.def.getTemp());
+            op.setFixed(copy.def.physReg());
+
+            if ((flags & rename_precolored_ops)) {
+               bool copy_is_duplicated = false;
+               for (auto it2 = parallelcopies.begin(); it2 != it; ++it2) {
+                  if (it2->op.tempId() == copy.op.tempId()) {
+                     copy_is_duplicated = true;
+                     break;
+                  }
+               }
+               if (copy_is_duplicated) {
+                  if (first[omit_renaming])
+                     op.setFirstKill(true);
+                  else
+                     op.setKill(true);
+                  copy.skip_renaming = true;
+               }
+            }
 
             fill = (flags & fill_killed_ops) || !op.isKillBeforeDef();
          }
       }
 
       if (fill)
-         reg_file.fill(copy.second);
+         reg_file.fill(copy.def);
 
       ++it;
    }
@@ -1024,7 +1087,7 @@ collect_vars(ra_ctx& ctx, RegisterFile& reg_file, const PhysRegInterval reg_inte
 
 std::optional<PhysReg>
 get_reg_for_create_vector_copy(ra_ctx& ctx, RegisterFile& reg_file,
-                               std::vector<std::pair<Operand, Definition>>& parallelcopies,
+                               std::vector<parallelcopy>& parallelcopies,
                                aco_ptr<Instruction>& instr, const PhysRegInterval def_reg,
                                DefInfo info, unsigned id)
 {
@@ -1076,8 +1139,7 @@ get_reg_for_create_vector_copy(ra_ctx& ctx, RegisterFile& reg_file,
 }
 
 bool
-get_regs_for_copies(ra_ctx& ctx, RegisterFile& reg_file,
-                    std::vector<std::pair<Operand, Definition>>& parallelcopies,
+get_regs_for_copies(ra_ctx& ctx, RegisterFile& reg_file, std::vector<parallelcopy>& parallelcopies,
                     const std::vector<unsigned>& vars, aco_ptr<Instruction>& instr,
                     const PhysRegInterval def_reg)
 {
@@ -1227,9 +1289,8 @@ get_regs_for_copies(ra_ctx& ctx, RegisterFile& reg_file,
 }
 
 std::optional<PhysReg>
-get_reg_impl(ra_ctx& ctx, const RegisterFile& reg_file,
-             std::vector<std::pair<Operand, Definition>>& parallelcopies, const DefInfo& info,
-             aco_ptr<Instruction>& instr)
+get_reg_impl(ra_ctx& ctx, const RegisterFile& reg_file, std::vector<parallelcopy>& parallelcopies,
+             const DefInfo& info, aco_ptr<Instruction>& instr)
 {
    const PhysRegInterval& bounds = info.bounds;
    uint32_t size = info.size;
@@ -1363,7 +1424,7 @@ get_reg_impl(ra_ctx& ctx, const RegisterFile& reg_file,
       }
    }
 
-   std::vector<std::pair<Operand, Definition>> pc;
+   std::vector<parallelcopy> pc;
    if (!get_regs_for_copies(ctx, tmp_file, pc, vars, instr, best_win))
       return {};
 
@@ -1458,7 +1519,7 @@ struct IDAndInfo {
  */
 PhysReg
 compact_relocate_vars(ra_ctx& ctx, const std::vector<IDAndRegClass>& vars,
-                      std::vector<std::pair<Operand, Definition>>& parallelcopies, PhysReg start)
+                      std::vector<parallelcopy>& parallelcopies, PhysReg start)
 {
    /* This function assumes RegisterDemand/live_var_analysis rounds up sub-dword
     * temporary sizes to dwords.
@@ -1602,7 +1663,7 @@ get_reg_vector(ra_ctx& ctx, const RegisterFile& reg_file, Temp temp, aco_ptr<Ins
 
 bool
 compact_linear_vgprs(ra_ctx& ctx, const RegisterFile& reg_file,
-                     std::vector<std::pair<Operand, Definition>>& parallelcopies)
+                     std::vector<parallelcopy>& parallelcopies)
 {
    PhysRegInterval linear_vgpr_bounds = get_reg_bounds(ctx, RegType::vgpr, true);
    int zeros = reg_file.count_zero(linear_vgpr_bounds);
@@ -1628,7 +1689,7 @@ compact_linear_vgprs(ra_ctx& ctx, const RegisterFile& reg_file,
  */
 PhysReg
 alloc_linear_vgpr(ra_ctx& ctx, const RegisterFile& reg_file, aco_ptr<Instruction>& instr,
-                  std::vector<std::pair<Operand, Definition>>& parallelcopies)
+                  std::vector<parallelcopy>& parallelcopies)
 {
    assert(instr->opcode == aco_opcode::p_start_linear_vgpr);
    assert(instr->definitions.size() == 1 && instr->definitions[0].bytes() % 4 == 0);
@@ -1664,7 +1725,7 @@ alloc_linear_vgpr(ra_ctx& ctx, const RegisterFile& reg_file, aco_ptr<Instruction
    }
 
    /* Find new assignments for blocking vars. */
-   std::vector<std::pair<Operand, Definition>> pc;
+   std::vector<parallelcopy> pc;
    if (!ctx.policy.skip_optimistic_path &&
        get_regs_for_copies(ctx, tmp_file, pc, blocking_vars, instr, reg_win)) {
       parallelcopies.insert(parallelcopies.end(), pc.begin(), pc.end());
@@ -1720,7 +1781,7 @@ should_compact_linear_vgprs(ra_ctx& ctx, live& live_vars, const RegisterFile& re
 
 PhysReg
 get_reg(ra_ctx& ctx, const RegisterFile& reg_file, Temp temp,
-        std::vector<std::pair<Operand, Definition>>& parallelcopies, aco_ptr<Instruction>& instr,
+        std::vector<parallelcopy>& parallelcopies, aco_ptr<Instruction>& instr,
         int operand_index = -1)
 {
    auto split_vec = ctx.split_vectors.find(temp.id());
@@ -1791,7 +1852,7 @@ get_reg(ra_ctx& ctx, const RegisterFile& reg_file, Temp temp,
       return *res;
 
    /* try compacting the linear vgprs to make more space */
-   std::vector<std::pair<Operand, Definition>> pc;
+   std::vector<parallelcopy> pc;
    if (info.rc.type() == RegType::vgpr && (ctx.block->kind & block_kind_top_level) &&
        compact_linear_vgprs(ctx, reg_file, pc)) {
       parallelcopies.insert(parallelcopies.end(), pc.begin(), pc.end());
@@ -1799,8 +1860,8 @@ get_reg(ra_ctx& ctx, const RegisterFile& reg_file, Temp temp,
       /* We don't need to fill the copy definitions in because we don't care about the linear VGPR
        * space here. */
       RegisterFile tmp_file(reg_file);
-      for (std::pair<Operand, Definition>& copy : pc)
-         tmp_file.clear(copy.first);
+      for (parallelcopy& copy : pc)
+         tmp_file.clear(copy.op);
 
       return get_reg(ctx, tmp_file, temp, parallelcopies, instr, operand_index);
    }
@@ -1858,8 +1919,7 @@ get_reg(ra_ctx& ctx, const RegisterFile& reg_file, Temp temp,
 
 PhysReg
 get_reg_create_vector(ra_ctx& ctx, const RegisterFile& reg_file, Temp temp,
-                      std::vector<std::pair<Operand, Definition>>& parallelcopies,
-                      aco_ptr<Instruction>& instr)
+                      std::vector<parallelcopy>& parallelcopies, aco_ptr<Instruction>& instr)
 {
    RegClass rc = temp.regClass();
    /* create_vector instructions have different costs w.r.t. register coalescing */
@@ -1978,7 +2038,7 @@ get_reg_create_vector(ra_ctx& ctx, const RegisterFile& reg_file, Temp temp,
    std::vector<unsigned> vars = collect_vars(ctx, tmp_file, PhysRegInterval{best_pos, size});
 
    bool success = false;
-   std::vector<std::pair<Operand, Definition>> pc;
+   std::vector<parallelcopy> pc;
    success = get_regs_for_copies(ctx, tmp_file, pc, vars, instr, PhysRegInterval{best_pos, size});
 
    if (!success) {
@@ -2069,8 +2129,7 @@ operand_can_use_reg(amd_gfx_level gfx_level, aco_ptr<Instruction>& instr, unsign
 
 void
 handle_fixed_operands(ra_ctx& ctx, RegisterFile& register_file,
-                      std::vector<std::pair<Operand, Definition>>& parallelcopy,
-                      aco_ptr<Instruction>& instr)
+                      std::vector<parallelcopy>& parallelcopy, aco_ptr<Instruction>& instr)
 {
    assert(instr->operands.size() <= 128);
 
@@ -2136,8 +2195,8 @@ handle_fixed_operands(ra_ctx& ctx, RegisterFile& register_file,
 
 void
 get_reg_for_operand(ra_ctx& ctx, RegisterFile& register_file,
-                    std::vector<std::pair<Operand, Definition>>& parallelcopy,
-                    aco_ptr<Instruction>& instr, Operand& operand, unsigned operand_index)
+                    std::vector<parallelcopy>& parallelcopy, aco_ptr<Instruction>& instr,
+                    Operand& operand, unsigned operand_index)
 {
    /* clear the operand in case it's only a stride mismatch */
    PhysReg src = ctx.assignments[operand.tempId()].reg;
@@ -2156,12 +2215,12 @@ get_reg_phi(ra_ctx& ctx, IDSet& live_in, RegisterFile& register_file,
             std::vector<aco_ptr<Instruction>>& instructions, Block& block,
             aco_ptr<Instruction>& phi, Temp tmp)
 {
-   std::vector<std::pair<Operand, Definition>> parallelcopy;
+   std::vector<parallelcopy> parallelcopy;
    PhysReg reg = get_reg(ctx, register_file, tmp, parallelcopy, phi);
    update_renames(ctx, register_file, parallelcopy, phi, rename_not_killed_ops);
 
    /* process parallelcopy */
-   for (std::pair<Operand, Definition> pc : parallelcopy) {
+   for (struct parallelcopy pc : parallelcopy) {
       /* see if it's a copy from a different phi */
       // TODO: prefer moving some previous phis over live-ins
       // TODO: somehow prevent phis fixed before the RA from being updated (shouldn't be a
@@ -2169,34 +2228,33 @@ get_reg_phi(ra_ctx& ctx, IDSet& live_in, RegisterFile& register_file,
       Instruction* prev_phi = NULL;
       std::vector<aco_ptr<Instruction>>::iterator phi_it;
       for (phi_it = instructions.begin(); phi_it != instructions.end(); ++phi_it) {
-         if ((*phi_it)->definitions[0].tempId() == pc.first.tempId())
+         if ((*phi_it)->definitions[0].tempId() == pc.op.tempId())
             prev_phi = phi_it->get();
       }
       if (prev_phi) {
          /* if so, just update that phi's register */
-         prev_phi->definitions[0].setFixed(pc.second.physReg());
+         prev_phi->definitions[0].setFixed(pc.def.physReg());
          register_file.fill(prev_phi->definitions[0]);
-         ctx.assignments[prev_phi->definitions[0].tempId()] = {pc.second.physReg(),
-                                                               pc.second.regClass()};
+         ctx.assignments[prev_phi->definitions[0].tempId()] = {pc.def.physReg(), pc.def.regClass()};
          continue;
       }
 
       /* rename */
-      std::unordered_map<unsigned, Temp>::iterator orig_it = ctx.orig_names.find(pc.first.tempId());
-      Temp orig = orig_it != ctx.orig_names.end() ? orig_it->second : pc.first.getTemp();
-      ctx.orig_names[pc.second.tempId()] = orig;
-      ctx.renames[block.index][orig.id()] = pc.second.getTemp();
+      std::unordered_map<unsigned, Temp>::iterator orig_it = ctx.orig_names.find(pc.op.tempId());
+      Temp orig = orig_it != ctx.orig_names.end() ? orig_it->second : pc.op.getTemp();
+      ctx.orig_names[pc.def.tempId()] = orig;
+      ctx.renames[block.index][orig.id()] = pc.def.getTemp();
 
       /* otherwise, this is a live-in and we need to create a new phi
        * to move it in this block's predecessors */
       aco_opcode opcode =
-         pc.first.getTemp().is_linear() ? aco_opcode::p_linear_phi : aco_opcode::p_phi;
+         pc.op.getTemp().is_linear() ? aco_opcode::p_linear_phi : aco_opcode::p_phi;
       Block::edge_vec& preds =
-         pc.first.getTemp().is_linear() ? block.linear_preds : block.logical_preds;
+         pc.op.getTemp().is_linear() ? block.linear_preds : block.logical_preds;
       aco_ptr<Instruction> new_phi{create_instruction(opcode, Format::PSEUDO, preds.size(), 1)};
-      new_phi->definitions[0] = pc.second;
+      new_phi->definitions[0] = pc.def;
       for (unsigned i = 0; i < preds.size(); i++)
-         new_phi->operands[i] = Operand(pc.first);
+         new_phi->operands[i] = Operand(pc.op);
       instructions.emplace_back(std::move(new_phi));
 
       /* Remove from live_in, because handle_loop_phis() would re-create this phi later if this is
@@ -2869,7 +2927,7 @@ optimize_encoding(ra_ctx& ctx, RegisterFile& register_file, aco_ptr<Instruction>
 }
 
 void
-emit_parallel_copy_internal(ra_ctx& ctx, std::vector<std::pair<Operand, Definition>>& parallelcopy,
+emit_parallel_copy_internal(ra_ctx& ctx, std::vector<parallelcopy>& parallelcopy,
                             aco_ptr<Instruction>& instr,
                             std::vector<aco_ptr<Instruction>>& instructions, bool temp_in_scc,
                             RegisterFile& register_file)
@@ -2884,24 +2942,24 @@ emit_parallel_copy_internal(ra_ctx& ctx, std::vector<std::pair<Operand, Definiti
    bool sgpr_operands_alias_defs = false;
    uint64_t sgpr_operands[4] = {0, 0, 0, 0};
    for (unsigned i = 0; i < parallelcopy.size(); i++) {
-      linear_vgpr |= parallelcopy[i].first.regClass().is_linear_vgpr();
+      linear_vgpr |= parallelcopy[i].op.regClass().is_linear_vgpr();
 
-      if (temp_in_scc && parallelcopy[i].first.isTemp() &&
-          parallelcopy[i].first.getTemp().type() == RegType::sgpr) {
+      if (temp_in_scc && parallelcopy[i].op.isTemp() &&
+          parallelcopy[i].op.getTemp().type() == RegType::sgpr) {
          if (!sgpr_operands_alias_defs) {
-            unsigned reg = parallelcopy[i].first.physReg().reg();
-            unsigned size = parallelcopy[i].first.getTemp().size();
+            unsigned reg = parallelcopy[i].op.physReg().reg();
+            unsigned size = parallelcopy[i].op.getTemp().size();
             sgpr_operands[reg / 64u] |= u_bit_consecutive64(reg % 64u, size);
 
-            reg = parallelcopy[i].second.physReg().reg();
-            size = parallelcopy[i].second.getTemp().size();
+            reg = parallelcopy[i].def.physReg().reg();
+            size = parallelcopy[i].def.getTemp().size();
             if (sgpr_operands[reg / 64u] & u_bit_consecutive64(reg % 64u, size))
                sgpr_operands_alias_defs = true;
          }
       }
 
-      pc->operands[i] = parallelcopy[i].first;
-      pc->definitions[i] = parallelcopy[i].second;
+      pc->operands[i] = parallelcopy[i].op;
+      pc->definitions[i] = parallelcopy[i].def;
       assert(pc->operands[i].size() == pc->definitions[i].size());
 
       /* it might happen that the operand is already renamed. we have to restore the
@@ -2910,7 +2968,8 @@ emit_parallel_copy_internal(ra_ctx& ctx, std::vector<std::pair<Operand, Definiti
          ctx.orig_names.find(pc->operands[i].tempId());
       Temp orig = it != ctx.orig_names.end() ? it->second : pc->operands[i].getTemp();
       ctx.orig_names[pc->definitions[i].tempId()] = orig;
-      ctx.renames[ctx.block->index][orig.id()] = pc->definitions[i].getTemp();
+      if (!parallelcopy[i].skip_renaming)
+         ctx.renames[ctx.block->index][orig.id()] = pc->definitions[i].getTemp();
    }
 
    if (temp_in_scc && (sgpr_operands_alias_defs || linear_vgpr)) {
@@ -2937,18 +2996,18 @@ emit_parallel_copy_internal(ra_ctx& ctx, std::vector<std::pair<Operand, Definiti
 }
 
 void
-emit_parallel_copy(ra_ctx& ctx, std::vector<std::pair<Operand, Definition>>& parallelcopy,
+emit_parallel_copy(ra_ctx& ctx, std::vector<parallelcopy>& parallelcopy,
                    aco_ptr<Instruction>& instr, std::vector<aco_ptr<Instruction>>& instructions,
                    bool temp_in_scc, RegisterFile& register_file)
 {
    if (parallelcopy.empty())
       return;
 
-   std::vector<std::pair<Operand, Definition>> linear_vgpr;
+   std::vector<struct parallelcopy> linear_vgpr;
    if (ctx.num_linear_vgprs) {
       unsigned next = 0;
       for (unsigned i = 0; i < parallelcopy.size(); i++) {
-         if (parallelcopy[i].first.regClass().is_linear_vgpr()) {
+         if (parallelcopy[i].def.regClass().is_linear_vgpr()) {
             linear_vgpr.push_back(parallelcopy[i]);
             continue;
          }
@@ -3053,7 +3112,7 @@ register_allocation(Program* program, live& live_vars, ra_test_policy policy)
             continue;
          }
 
-         std::vector<std::pair<Operand, Definition>> parallelcopy;
+         std::vector<parallelcopy> parallelcopy;
          bool temp_in_scc = register_file[scc];
 
          if (instr->opcode == aco_opcode::p_branch) {
@@ -3075,6 +3134,8 @@ register_allocation(Program* program, live& live_vars, ra_test_policy policy)
             operand.setTemp(read_variable(ctx, operand.getTemp(), block.index));
             assert(ctx.assignments[operand.tempId()].assigned);
 
+            operand.setPrecolored(operand.isFixed());
+
             fixed |=
                operand.isFixed() && ctx.assignments[operand.tempId()].reg != operand.physReg();
          }
@@ -3094,6 +3155,24 @@ register_allocation(Program* program, live& live_vars, ra_test_policy policy)
          if (fixed)
             handle_fixed_operands(ctx, register_file, parallelcopy, instr);
 
+         for (unsigned i = 0; i < instr->operands.size(); ++i) {
+            Operand& op = instr->operands[i];
+            if (op.isTemp() && op.isKillBeforeDef() && !op.isFirstKill()) {
+               bool found_op = false;
+               for (unsigned j = 0; j < i; ++j) {
+                  if (instr->operands[j].isTemp() && instr->operands[j].getTemp() == op.getTemp()) {
+                     found_op = true;
+                     break;
+                  }
+               }
+               if (!found_op) {
+                  fprintf(stderr, "Operand %u is not marked first-kill, but is!\n", i);
+                  aco_print_instr(GFX10_3, instr.get(), stderr, print_kill);
+                  abort();
+               }
+            }
+         }
+
          for (unsigned i = 0; i < instr->operands.size(); ++i) {
             auto& operand = instr->operands[i];
             if (!operand.isTemp() || operand.isFixed())
@@ -3147,7 +3226,9 @@ register_allocation(Program* program, live& live_vars, ra_test_policy policy)
                RegisterFile tmp_file(register_file);
                /* re-enable the killed operands, so that we don't move the blocking vars there */
                for (const Operand& op : instr->operands) {
-                  if (op.isTemp() && op.isFirstKillBeforeDef())
+                  if (op.isPrecolored())
+                     tmp_file.block(op.physReg(), op.regClass());
+                  else if (op.isTemp() && op.isKillBeforeDef())
                      tmp_file.fill(op);
                }
 
@@ -3349,7 +3430,7 @@ register_allocation(Program* program, live& live_vars, ra_test_policy policy)
          bool temp_in_scc =
             register_file[scc] || (!br->operands.empty() && br->operands[0].physReg() == scc);
 
-         std::vector<std::pair<Operand, Definition>> parallelcopy;
+         std::vector<parallelcopy> parallelcopy;
          compact_linear_vgprs(ctx, register_file, parallelcopy);
          update_renames(ctx, register_file, parallelcopy, br, rename_not_killed_ops);
          emit_parallel_copy_internal(ctx, parallelcopy, br, instructions, temp_in_scc, register_file);
diff --git a/src/amd/compiler/aco_spill.cpp b/src/amd/compiler/aco_spill.cpp
index 10b8992578d88..172addd3c22af 100644
--- a/src/amd/compiler/aco_spill.cpp
+++ b/src/amd/compiler/aco_spill.cpp
@@ -998,15 +998,21 @@ process_block(spill_ctx& ctx, unsigned block_idx, Block* block, RegisterDemand s
       /* check if register demand is low enough before and after the current instruction */
       if (block->register_demand.exceeds(ctx.target_pressure)) {
 
+         RegisterDemand demand_before = get_demand_before(ctx, block_idx, idx);
          RegisterDemand new_demand = ctx.live_vars.register_demand[block_idx][idx];
-         new_demand.update(get_demand_before(ctx, block_idx, idx));
+         new_demand.update(demand_before);
 
          /* if reg pressure is too high, spill variable with furthest next use */
          while ((new_demand - spilled_registers).exceeds(ctx.target_pressure)) {
             float score = 0.0;
             Temp to_spill;
+            unsigned operand_idx = -1u;
+            unsigned respill_slot = -1u;
+
             unsigned do_rematerialize = 0;
             unsigned avoid_respill = 0;
+            unsigned avoid_operand = 0;
+
             RegType type = RegType::sgpr;
             if (new_demand.vgpr - spilled_registers.vgpr > ctx.target_pressure.vgpr)
                type = RegType::vgpr;
@@ -1022,26 +1028,67 @@ process_block(spill_ctx& ctx, unsigned block_idx, Block* block, RegisterDemand s
                if (avoid_respill > loop_variable || do_rematerialize > can_rematerialize)
                   continue;
 
-               if (can_rematerialize > do_rematerialize || loop_variable > avoid_respill ||
-                   ctx.ssa_infos[t].score() > score) {
-                  /* Don't spill operands */
-                  if (std::any_of(instr->operands.begin(), instr->operands.end(),
-                                  [&](Operand& op) { return op.isTemp() && op.getTemp() == var; }))
+               unsigned cur_operand_idx = -1u;
+               bool can_spill = true;
+               for (auto it = instr->operands.begin(); it != instr->operands.end(); ++it) {
+                  if (!it->isTemp() || it->getTemp() != var)
                      continue;
+                  cur_operand_idx = it - instr->operands.begin();
+                  if (it->isLateKill() || it->isKill() || it->regClass().type() != type)
+                     can_spill = false;
+                  break;
+               }
+               if (!can_spill)
+                  continue;
+               unsigned avoids_operand = cur_operand_idx == -1u;
+
+               if (can_rematerialize > do_rematerialize || loop_variable > avoid_respill ||
+                   avoids_operand > avoid_operand || ctx.ssa_infos[t].score() > score) {
+                  bool is_spilled_operand = !avoids_operand && reloads.count(var);
 
                   to_spill = var;
                   score = ctx.ssa_infos[t].score();
                   do_rematerialize = can_rematerialize;
-                  avoid_respill = loop_variable;
+                  avoid_respill = loop_variable || is_spilled_operand;
+                  avoid_operand = avoids_operand;
+                  operand_idx = cur_operand_idx;
+
+                  /* This variable is spilled at the loop-header of the current loop.
+                   * Re-use the spill-slot in order to avoid an extra store.
+                   */
+                  if (loop_variable)
+                     respill_slot = ctx.loop.back().spills[var];
+                  else if (is_spilled_operand)
+                     respill_slot = reloads[var].second;
                }
             }
-            assert(score != 0.0);
+            assert(score > 0.0);
+
+            if (operand_idx != -1u) {
+               bool recalculate_demand = false;
+               Operand& spilled_op = instr->operands[operand_idx];
+               spilled_op.setFirstKill(true);
+               for (auto it = instr->operands.begin() + operand_idx + 1;
+                    it != instr->operands.end(); ++it) {
+                  if (it->isTemp() && it->getTemp() == to_spill) {
+                     it->setKill(true);
+
+                     /* If this operand had a live-through duplicate fixed to another register,
+                      * then that copy is killed too.
+                      */
+                     if (spilled_op.isFixed() && it->isFixed() &&
+                         spilled_op.physReg() != it->physReg())
+                        recalculate_demand = true;
+                  }
+               }
+
+               if (recalculate_demand)
+                  ctx.live_vars.register_demand[block_idx][idx] =
+                     demand_before + get_live_changes(instr) + get_temp_registers(instr);
+            }
 
             if (avoid_respill) {
-               /* This variable is spilled at the loop-header of the current loop.
-                * Re-use the spill-slot in order to avoid an extra store.
-                */
-               current_spills[to_spill] = ctx.loop.back().spills[to_spill];
+               current_spills[to_spill] = respill_slot;
                spilled_registers += to_spill;
                continue;
             }
-- 
GitLab


From b852c9e570079defa4d0371a9d43fbbd8ed4677e Mon Sep 17 00:00:00 2001
From: Friedrich Vock <friedrich.vock@gmx.de>
Date: Sun, 7 Jan 2024 23:06:01 +0100
Subject: [PATCH 04/71] !29577

---
 src/amd/vulkan/radv_pipeline.c             |   4 +-
 src/compiler/nir/meson.build               |   1 +
 src/compiler/nir/nir.c                     |   8 +-
 src/compiler/nir/nir.h                     |  49 +++-
 src/compiler/nir/nir_builder.h             |  23 ++
 src/compiler/nir/nir_divergence_analysis.c |   9 +-
 src/compiler/nir/nir_functions.c           |   5 +-
 src/compiler/nir/nir_gather_info.c         |   6 +-
 src/compiler/nir/nir_inline_helpers.h      |   2 +
 src/compiler/nir/nir_lower_memory_model.c  |  33 +--
 src/compiler/nir/nir_opt_call.c            | 264 +++++++++++++++++++++
 src/compiler/nir/nir_print.c               |   7 +
 src/compiler/nir/nir_sweep.c               |   6 +-
 src/compiler/nir/nir_validate.c            |   5 +
 src/compiler/spirv/vtn_cfg.c               |   3 +
 15 files changed, 393 insertions(+), 32 deletions(-)
 create mode 100644 src/compiler/nir/nir_opt_call.c

diff --git a/src/amd/vulkan/radv_pipeline.c b/src/amd/vulkan/radv_pipeline.c
index e77f501978eda..a4de95225f959 100644
--- a/src/amd/vulkan/radv_pipeline.c
+++ b/src/amd/vulkan/radv_pipeline.c
@@ -435,7 +435,9 @@ radv_postprocess_nir(struct radv_device *device, const struct radv_graphics_stat
          NIR_PASS(_, stage->nir, nir_opt_shrink_stores, !instance->drirc.disable_shrink_image_store);
 
          /* Gather info again, to update whether 8/16-bit are used. */
-         nir_shader_gather_info(stage->nir, nir_shader_get_entrypoint(stage->nir));
+         nir_foreach_function_impl (impl, stage->nir)
+            if (impl->function->is_entrypoint || impl->function->is_exported)
+               nir_shader_gather_info(stage->nir, impl);
       }
    }
 
diff --git a/src/compiler/nir/meson.build b/src/compiler/nir/meson.build
index 42ff91799df53..a4ac18e959588 100644
--- a/src/compiler/nir/meson.build
+++ b/src/compiler/nir/meson.build
@@ -235,6 +235,7 @@ files_libnir = files(
   'nir_normalize_cubemap_coords.c',
   'nir_opt_access.c',
   'nir_opt_barriers.c',
+  'nir_opt_call.c',
   'nir_opt_combine_stores.c',
   'nir_opt_comparison_pre.c',
   'nir_opt_conditional_discard.c',
diff --git a/src/compiler/nir/nir.c b/src/compiler/nir/nir.c
index 619d509e4f2e1..c0248fc2fa16d 100644
--- a/src/compiler/nir/nir.c
+++ b/src/compiler/nir/nir.c
@@ -502,6 +502,8 @@ nir_function_create(nir_shader *shader, const char *name)
    func->is_preamble = false;
    func->dont_inline = false;
    func->should_inline = false;
+   func->noreturn = false;
+   func->uniform_call = false;
    func->is_subroutine = false;
    func->subroutine_index = 0;
    func->num_subroutine_types = 0;
@@ -1539,8 +1541,8 @@ nir_def_rewrite_uses_src(nir_def *def, nir_src new_src)
    nir_def_rewrite_uses(def, new_src.ssa);
 }
 
-static bool
-is_instr_between(nir_instr *start, nir_instr *end, nir_instr *between)
+bool
+nir_instr_is_between(nir_instr *start, nir_instr *end, nir_instr *between)
 {
    assert(start->block == end->block);
 
@@ -1584,7 +1586,7 @@ nir_def_rewrite_uses_after(nir_def *def, nir_def *new_ssa,
           * not be dominated by after_me is if it is between def and after_me in
           * the instruction list.
           */
-         if (is_instr_between(def->parent_instr, after_me, nir_src_parent_instr(use_src)))
+         if (nir_instr_is_between(def->parent_instr, after_me, nir_src_parent_instr(use_src)))
             continue;
       }
 
diff --git a/src/compiler/nir/nir.h b/src/compiler/nir/nir.h
index 0418507866da8..1ad3438bc2fa2 100644
--- a/src/compiler/nir/nir.h
+++ b/src/compiler/nir/nir.h
@@ -1861,10 +1861,19 @@ bool nir_deref_instr_remove_if_unused(nir_deref_instr *instr);
 
 unsigned nir_deref_instr_array_stride(nir_deref_instr *instr);
 
+typedef enum {
+   nir_call_abi_rt_raygen_amd,
+   nir_call_abi_rt_traversal_amd,
+   nir_call_abi_rt_any_hit_amd,
+} nir_call_abi;
+
 typedef struct {
    nir_instr instr;
 
    struct nir_function *callee;
+   nir_src indirect_callee;
+
+   nir_call_abi abi;
 
    unsigned num_params;
    nir_src params[];
@@ -3060,6 +3069,14 @@ nir_block_ends_in_break(nir_block *block)
           nir_instr_as_jump(instr)->type == nir_jump_break;
 }
 
+static inline void
+nir_block_free_liveness_info(nir_block *block)
+{
+   ralloc_free(block->live_in);
+   ralloc_free(block->live_out);
+   block->live_in = block->live_out = NULL;
+}
+
 #define nir_foreach_instr(instr, block) \
    foreach_list_typed(nir_instr, instr, node, &(block)->instr_list)
 #define nir_foreach_instr_reverse(instr, block) \
@@ -3522,12 +3539,21 @@ nir_cf_list_is_empty_block(struct exec_list *cf_list)
 typedef struct {
    uint8_t num_components;
    uint8_t bit_size;
-
-   /* True if this paramater is actually the function return variable */
+   /* True if this parameter is a deref used for returning values */
    bool is_return;
 
+   /* True if this parameter contains divergent values */
+   bool is_divergent;
+
    /* The type of the function param */
    const struct glsl_type *type;
+
+   /* True if this parameter is unused and does not need to be preserved
+    * for potential other indirect callees.
+    * Useful if removing unused parameters from the list is not possible,
+    * e.g. for ABI reasons.
+    */
+   bool discardable;
 } nir_parameter;
 
 typedef struct nir_function {
@@ -3556,6 +3582,14 @@ typedef struct nir_function {
    bool should_inline;
    bool dont_inline; /* from SPIR-V */
 
+   /* Only meaningful for indirectly-called functions.
+    * Denotes that the function pointer used to call the
+    * function will be the same across all active invocations.
+    */
+   bool uniform_call;
+   /* Function terminates the shader and will never return */
+   bool noreturn;
+
    /**
     * Is this function a subroutine type declaration
     * e.g. subroutine void type1(float arg1);
@@ -4829,6 +4863,8 @@ void nir_instr_clear_src(nir_instr *instr, nir_src *src);
 
 void nir_instr_move_src(nir_instr *dest_instr, nir_src *dest, nir_src *src);
 
+bool nir_instr_is_between(nir_instr *start, nir_instr *end, nir_instr *between);
+
 void nir_def_init(nir_instr *instr, nir_def *def,
                   unsigned num_components, unsigned bit_size);
 static inline void
@@ -6489,6 +6525,15 @@ bool nir_opt_combine_barriers(nir_shader *shader,
                               void *data);
 bool nir_opt_barrier_modes(nir_shader *shader);
 
+typedef bool (*can_remat_cb)(nir_instr *instr);
+
+struct nir_minimize_call_live_states_options {
+   can_remat_cb can_remat;
+};
+
+bool nir_minimize_call_live_states(nir_shader *shader,
+                                   struct nir_minimize_call_live_states_options *options);
+
 bool nir_opt_combine_stores(nir_shader *shader, nir_variable_mode modes);
 
 bool nir_copy_prop_impl(nir_function_impl *impl);
diff --git a/src/compiler/nir/nir_builder.h b/src/compiler/nir/nir_builder.h
index d588dd65a518c..fa16f2a9a10ce 100644
--- a/src/compiler/nir/nir_builder.h
+++ b/src/compiler/nir/nir_builder.h
@@ -2137,6 +2137,23 @@ nir_build_call(nir_builder *build, nir_function *func, size_t count,
    nir_builder_instr_insert(build, &call->instr);
 }
 
+static inline void
+nir_build_indirect_call(nir_builder *build, nir_function *func, nir_def *callee,
+                        nir_call_abi abi, size_t count, nir_def **args)
+{
+   assert(count == func->num_params && "parameter count must match");
+   assert(!func->impl && "cannot call directly defined functions indirectly");
+   nir_call_instr *call = nir_call_instr_create(build->shader, func);
+
+   for (unsigned i = 0; i < count; ++i) {
+      call->params[i] = nir_src_for_ssa(args[i]);
+   }
+   call->indirect_callee = nir_src_for_ssa(callee);
+   call->abi = abi;
+
+   nir_builder_instr_insert(build, &call->instr);
+}
+
 /*
  * Call a given nir_function * with a variadic number of nir_def * arguments.
  *
@@ -2149,6 +2166,12 @@ nir_build_call(nir_builder *build, nir_function *func, size_t count,
       nir_build_call(build, func, ARRAY_SIZE(args), args); \
    } while (0)
 
+#define nir_call_indirect(build, func, callee, abi, ...)                           \
+   do {                                                                            \
+      nir_def *_args[] = { __VA_ARGS__ };                                          \
+      nir_build_indirect_call(build, func, callee, abi, ARRAY_SIZE(_args), _args); \
+   } while (0)
+
 nir_def *
 nir_compare_func(nir_builder *b, enum compare_func func,
                  nir_def *src0, nir_def *src1);
diff --git a/src/compiler/nir/nir_divergence_analysis.c b/src/compiler/nir/nir_divergence_analysis.c
index ecd88303c1550..5519c2e49490f 100644
--- a/src/compiler/nir/nir_divergence_analysis.c
+++ b/src/compiler/nir/nir_divergence_analysis.c
@@ -953,9 +953,10 @@ update_instr_divergence(nir_instr *instr, struct divergence_state *state)
       return visit_def(&nir_instr_as_undef(instr)->def, state);
    case nir_instr_type_deref:
       return visit_deref(state->shader, nir_instr_as_deref(instr), state);
+   case nir_instr_type_call:
+      return false;
    case nir_instr_type_jump:
    case nir_instr_type_phi:
-   case nir_instr_type_call:
    case nir_instr_type_parallel_copy:
    default:
       unreachable("NIR divergence analysis: Unsupported instruction type.");
@@ -1223,7 +1224,8 @@ nir_divergence_analysis(nir_shader *shader)
       .first_visit = true,
    };
 
-   visit_cf_list(&nir_shader_get_entrypoint(shader)->body, &state);
+   nir_foreach_function_impl(impl, shader)
+      visit_cf_list(&impl->body, &state);
 }
 
 /* Compute divergence between vertices of the same primitive. This uses
@@ -1242,7 +1244,8 @@ nir_vertex_divergence_analysis(nir_shader *shader)
       .first_visit = true,
    };
 
-   visit_cf_list(&nir_shader_get_entrypoint(shader)->body, &state);
+   nir_foreach_function_impl(impl, shader)
+      visit_cf_list(&impl->body, &state);
 }
 
 bool
diff --git a/src/compiler/nir/nir_functions.c b/src/compiler/nir/nir_functions.c
index 61134177139cf..f9b6030dbcb59 100644
--- a/src/compiler/nir/nir_functions.c
+++ b/src/compiler/nir/nir_functions.c
@@ -156,7 +156,10 @@ static bool inline_functions_pass(nir_builder *b,
       return false;
 
    nir_call_instr *call = nir_instr_as_call(instr);
-   assert(call->callee->impl);
+   if (!call->callee->impl)
+      return false;
+
+   assert(!call->indirect_callee.ssa);
 
    if (b->shader->options->driver_functions &&
        b->shader->info.stage == MESA_SHADER_KERNEL) {
diff --git a/src/compiler/nir/nir_gather_info.c b/src/compiler/nir/nir_gather_info.c
index 159ce20c5d481..e69de688a5f6b 100644
--- a/src/compiler/nir/nir_gather_info.c
+++ b/src/compiler/nir/nir_gather_info.c
@@ -934,8 +934,10 @@ gather_func_info(nir_function_impl *func, nir_shader *shader,
             nir_call_instr *call = nir_instr_as_call(instr);
             nir_function_impl *impl = call->callee->impl;
 
-            assert(impl || !"nir_shader_gather_info only works with linked shaders");
-            gather_func_info(impl, shader, visited_funcs, dead_ctx);
+            if (!call->indirect_callee.ssa)
+               assert(impl || !"nir_shader_gather_info only works with linked shaders");
+            if (impl)
+               gather_func_info(impl, shader, visited_funcs, dead_ctx);
             break;
          }
          default:
diff --git a/src/compiler/nir/nir_inline_helpers.h b/src/compiler/nir/nir_inline_helpers.h
index d4f0303c43989..0180cb6979dff 100644
--- a/src/compiler/nir/nir_inline_helpers.h
+++ b/src/compiler/nir/nir_inline_helpers.h
@@ -100,6 +100,8 @@ nir_foreach_src(nir_instr *instr, nir_foreach_src_cb cb, void *state)
    }
    case nir_instr_type_call: {
       nir_call_instr *call = nir_instr_as_call(instr);
+      if (call->indirect_callee.ssa && !_nir_visit_src(&call->indirect_callee, cb, state))
+         return false;
       for (unsigned i = 0; i < call->num_params; i++) {
          if (!_nir_visit_src(&call->params[i], cb, state))
             return false;
diff --git a/src/compiler/nir/nir_lower_memory_model.c b/src/compiler/nir/nir_lower_memory_model.c
index 81fbcf4203cdb..e7ba9747ede83 100644
--- a/src/compiler/nir/nir_lower_memory_model.c
+++ b/src/compiler/nir/nir_lower_memory_model.c
@@ -229,21 +229,24 @@ nir_lower_memory_model(nir_shader *shader)
 {
    bool progress = false;
 
-   nir_function_impl *impl = nir_shader_get_entrypoint(shader);
-   struct exec_list *cf_list = &impl->body;
-
-   uint32_t modes = 0;
-   foreach_list_typed(nir_cf_node, cf_node, node, cf_list)
-      progress |= lower_make_visible(cf_node, &modes);
-
-   modes = 0;
-   foreach_list_typed_reverse(nir_cf_node, cf_node, node, cf_list)
-      progress |= lower_make_available(cf_node, &modes);
-
-   if (progress)
-      nir_metadata_preserve(impl, nir_metadata_block_index | nir_metadata_dominance);
-   else
-      nir_metadata_preserve(impl, nir_metadata_all);
+   nir_foreach_function_impl(impl, shader) {
+      bool impl_progress = false;
+      struct exec_list *cf_list = &impl->body;
+
+      uint32_t modes = 0;
+      foreach_list_typed(nir_cf_node, cf_node, node, cf_list)
+         impl_progress |= lower_make_visible(cf_node, &modes);
+
+      modes = 0;
+      foreach_list_typed_reverse(nir_cf_node, cf_node, node, cf_list)
+         impl_progress |= lower_make_available(cf_node, &modes);
+
+      if (impl_progress)
+         nir_metadata_preserve(impl, nir_metadata_block_index | nir_metadata_dominance);
+      else
+         nir_metadata_preserve(impl, nir_metadata_all);
+      progress |= impl_progress;
+   }
 
    return progress;
 }
diff --git a/src/compiler/nir/nir_opt_call.c b/src/compiler/nir/nir_opt_call.c
new file mode 100644
index 0000000000000..5d5b4a79b9f21
--- /dev/null
+++ b/src/compiler/nir/nir_opt_call.c
@@ -0,0 +1,264 @@
+/*
+ * Copyright Â© 2024 Valve Corporation
+ * SPDX-License-Identifier: MIT
+ */
+
+#include "nir.h"
+#include "nir_builder.h"
+#include "nir_phi_builder.h"
+
+struct call_liveness_entry {
+   struct list_head list;
+   nir_call_instr *instr;
+   const BITSET_WORD *live_set;
+};
+
+static bool
+can_remat_instr(nir_instr *instr, struct nir_minimize_call_live_states_options *options)
+{
+   switch (instr->type) {
+   case nir_instr_type_alu:
+   case nir_instr_type_load_const:
+   case nir_instr_type_undef:
+      return true;
+   case nir_instr_type_intrinsic:
+      return options->can_remat && options->can_remat(instr);
+   default:
+      return false;
+   }
+}
+
+static void
+remat_ssa_def(nir_builder *b, nir_def *def, struct hash_table *remap_table,
+              struct hash_table *phi_value_table, struct nir_phi_builder *phi_builder,
+              BITSET_WORD *def_blocks)
+{
+   memset(def_blocks, 0, BITSET_WORDS(b->impl->num_blocks) * sizeof(BITSET_WORD));
+   BITSET_SET(def_blocks, def->parent_instr->block->index);
+   BITSET_SET(def_blocks, nir_cursor_current_block(b->cursor)->index);
+   struct nir_phi_builder_value *val = nir_phi_builder_add_value(phi_builder, def->num_components, def->bit_size, def_blocks);
+   _mesa_hash_table_insert(phi_value_table, def, val);
+
+   nir_instr *clone = nir_instr_clone_deep(b->shader, def->parent_instr, remap_table);
+   nir_builder_instr_insert(b, clone);
+   nir_def *new_def = nir_instr_def(clone);
+
+   _mesa_hash_table_insert(remap_table, def, new_def);
+   if (nir_cursor_current_block(b->cursor)->index != def->parent_instr->block->index)
+      nir_phi_builder_value_set_block_def(val, def->parent_instr->block, def);
+   nir_phi_builder_value_set_block_def(val, nir_cursor_current_block(b->cursor), new_def);
+}
+
+struct remat_chain_check_data {
+   struct nir_minimize_call_live_states_options *options;
+   struct hash_table *remap_table;
+   unsigned chain_length;
+};
+
+static bool
+can_remat_chain(nir_src *src, void *data)
+{
+   struct remat_chain_check_data *check_data = data;
+
+   if (_mesa_hash_table_search(check_data->remap_table, src->ssa))
+      return true;
+
+   if (check_data->chain_length >= 16)
+      return false;
+
+   if (!can_remat_instr(src->ssa->parent_instr, check_data->options))
+      return false;
+
+   struct remat_chain_check_data new_check_data = {
+      .options = check_data->options,
+      .remap_table = check_data->remap_table,
+      .chain_length = check_data->chain_length + 1,
+   };
+   return nir_foreach_src(src->ssa->parent_instr, can_remat_chain, &new_check_data);
+}
+
+struct remat_chain_data {
+   nir_builder *b;
+   struct hash_table *remap_table;
+   struct hash_table *phi_value_table;
+   struct nir_phi_builder *phi_builder;
+   BITSET_WORD *def_blocks;
+};
+
+static bool
+do_remat_chain(nir_src *src, void *data)
+{
+   struct remat_chain_data *remat_data = data;
+
+   if (_mesa_hash_table_search(remat_data->remap_table, src->ssa))
+      return true;
+
+   nir_foreach_src(src->ssa->parent_instr, do_remat_chain, remat_data);
+
+   remat_ssa_def(remat_data->b, src->ssa, remat_data->remap_table, remat_data->phi_value_table, remat_data->phi_builder, remat_data->def_blocks);
+   return true;
+}
+
+struct src_rewrite_ctx {
+   struct hash_table *phi_value_table;
+   nir_instr *instr;
+};
+
+static bool
+rewrite_instr_src_from_phi_builder(nir_src *src, void *data)
+{
+   struct src_rewrite_ctx *ctx = data;
+
+   if (nir_src_is_const(*src)) {
+      nir_builder b = nir_builder_at(nir_before_instr(ctx->instr));
+      nir_src_rewrite(src, nir_build_imm(&b, src->ssa->num_components, src->ssa->bit_size, nir_src_as_const_value(*src)));
+      return true;
+   }
+
+   nir_block *block;
+   if (nir_src_parent_instr(src)->type == nir_instr_type_phi) {
+      nir_phi_src *phi_src = exec_node_data(nir_phi_src, src, src);
+      block = phi_src->pred;
+   } else {
+      block = nir_src_parent_instr(src)->block;
+   }
+
+   struct hash_entry *entry = _mesa_hash_table_search(ctx->phi_value_table, src->ssa);
+   if (!entry)
+      return true;
+
+   nir_def *new_def = nir_phi_builder_value_get_block_def(entry->data, block);
+
+   bool can_rewrite = true;
+   if (new_def->parent_instr->block == block && new_def->index != UINT32_MAX)
+      can_rewrite = nir_src_parent_instr(src) != nir_block_first_instr(block) &&
+                    !nir_instr_is_between(nir_block_first_instr(block),
+                                          new_def->parent_instr,
+                                          nir_src_parent_instr(src));
+
+   if (can_rewrite)
+      nir_src_rewrite(src, new_def);
+   return true;
+}
+
+static bool
+nir_minimize_call_live_states_impl(nir_function_impl *impl,
+                                   struct nir_minimize_call_live_states_options *options)
+{
+   nir_metadata_require(impl, nir_metadata_block_index | nir_metadata_live_defs | nir_metadata_dominance);
+   bool progress = false;
+   void *mem_ctx = ralloc_context(impl->function->shader);
+
+   struct list_head call_list;
+   list_inithead(&call_list);
+   unsigned num_defs = impl->ssa_alloc;
+
+   nir_def **rematerializable = rzalloc_array_size(mem_ctx, sizeof(nir_def *), num_defs);
+
+   nir_foreach_block(block, impl) {
+      nir_foreach_instr(instr, block) {
+         nir_def *def = nir_instr_def(instr);
+         if (def &&
+             can_remat_instr(instr, options)) {
+            rematerializable[def->index] = def;
+         }
+
+         if (instr->type != nir_instr_type_call)
+            continue;
+         nir_call_instr *call = nir_instr_as_call(instr);
+         if (!call->indirect_callee.ssa)
+            continue;
+
+         struct call_liveness_entry *entry = ralloc_size(mem_ctx, sizeof(struct call_liveness_entry));
+         entry->instr = call;
+         entry->live_set = nir_get_live_defs(nir_after_instr(instr), mem_ctx);
+         list_addtail(&entry->list, &call_list);
+      }
+   }
+
+   const unsigned block_words = BITSET_WORDS(impl->num_blocks);
+   BITSET_WORD *def_blocks = ralloc_array(mem_ctx, BITSET_WORD, block_words);
+
+   list_for_each_entry(struct call_liveness_entry, entry, &call_list, list) {
+      unsigned i;
+
+      nir_builder b = nir_builder_create(impl);
+      b.cursor = nir_after_instr(&entry->instr->instr);
+
+      struct nir_phi_builder *builder = nir_phi_builder_create(impl);
+      struct hash_table *phi_value_table =
+         _mesa_pointer_hash_table_create(mem_ctx);
+      struct hash_table *remap_table =
+         _mesa_pointer_hash_table_create(mem_ctx);
+
+      BITSET_FOREACH_SET(i, entry->live_set, num_defs) {
+         if (!rematerializable[i] || _mesa_hash_table_search(remap_table, rematerializable[i]))
+            continue;
+
+         progress = true;
+         assert(!_mesa_hash_table_search(phi_value_table, rematerializable[i]));
+
+         struct remat_chain_check_data check_data = {
+            .options = options,
+            .remap_table = remap_table,
+            .chain_length = 1,
+         };
+
+         if (!nir_foreach_src(rematerializable[i]->parent_instr, can_remat_chain, &check_data))
+            continue;
+
+         struct remat_chain_data remat_data = {
+            .b = &b,
+            .remap_table = remap_table,
+            .phi_value_table = phi_value_table,
+            .phi_builder = builder,
+            .def_blocks = def_blocks,
+         };
+
+         nir_foreach_src(rematerializable[i]->parent_instr, do_remat_chain, &remat_data);
+
+         remat_ssa_def(&b, rematerializable[i], remap_table, phi_value_table, builder, def_blocks);
+      }
+      _mesa_hash_table_destroy(remap_table, NULL);
+
+      nir_foreach_block(block, impl) {
+         nir_foreach_instr(instr, block) {
+            if (instr->type == nir_instr_type_phi)
+               continue;
+
+            struct src_rewrite_ctx ctx = {
+               .phi_value_table = phi_value_table,
+               .instr = instr,
+            };
+            nir_foreach_src(instr, rewrite_instr_src_from_phi_builder, &ctx);
+         }
+      }
+
+      nir_phi_builder_finish(builder);
+      _mesa_hash_table_destroy(phi_value_table, NULL);
+   }
+
+   ralloc_free(mem_ctx);
+
+   nir_foreach_block(block, impl)
+      nir_block_free_liveness_info(block);
+   nir_metadata_preserve(impl, nir_metadata_block_index | nir_metadata_dominance);
+   return progress;
+}
+
+/* Tries to rematerialize as many live vars as possible after calls.
+ * Note: nir_opt_cse will undo any rematerializations done by this pass,
+ * so it shouldn't be run afterward.
+ */
+bool
+nir_minimize_call_live_states(nir_shader *shader,
+                              struct nir_minimize_call_live_states_options *options)
+{
+   bool progress = false;
+
+   nir_foreach_function_impl(impl, shader) {
+      progress |= nir_minimize_call_live_states_impl(impl, options);
+   }
+
+   return progress;
+}
\ No newline at end of file
diff --git a/src/compiler/nir/nir_print.c b/src/compiler/nir/nir_print.c
index b41d45e7bf42b..3889a393b1946 100644
--- a/src/compiler/nir/nir_print.c
+++ b/src/compiler/nir/nir_print.c
@@ -1871,7 +1871,14 @@ print_call_instr(nir_call_instr *instr, print_state *state)
 
    print_no_dest_padding(state);
 
+   bool indirect = instr->indirect_callee.ssa;
+
    fprintf(fp, "call %s ", instr->callee->name);
+   if (indirect) {
+      fprintf(fp, "(indirect ");
+      print_src(&instr->indirect_callee, state, nir_type_invalid);
+      fprintf(fp, ") ");
+   }
 
    for (unsigned i = 0; i < instr->num_params; i++) {
       if (i != 0)
diff --git a/src/compiler/nir/nir_sweep.c b/src/compiler/nir/nir_sweep.c
index 9acd60a60b875..8b06a6b5ac54e 100644
--- a/src/compiler/nir/nir_sweep.c
+++ b/src/compiler/nir/nir_sweep.c
@@ -50,11 +50,7 @@ sweep_block(nir_shader *nir, nir_block *block)
    /* sweep_impl will mark all metadata invalid.  We can safely release all of
     * this here.
     */
-   ralloc_free(block->live_in);
-   block->live_in = NULL;
-
-   ralloc_free(block->live_out);
-   block->live_out = NULL;
+   nir_block_free_liveness_info(block);
 
    nir_foreach_instr(instr, block) {
       gc_mark_live(nir->gctx, instr);
diff --git a/src/compiler/nir/nir_validate.c b/src/compiler/nir/nir_validate.c
index 4bd938afeee22..8b2cacb6d323a 100644
--- a/src/compiler/nir/nir_validate.c
+++ b/src/compiler/nir/nir_validate.c
@@ -941,6 +941,11 @@ validate_call_instr(nir_call_instr *instr, validate_state *state)
 {
    validate_assert(state, instr->num_params == instr->callee->num_params);
 
+   if (instr->indirect_callee.ssa) {
+      validate_assert(state, !instr->callee->impl);
+      validate_src(&instr->indirect_callee, state);
+   }
+
    for (unsigned i = 0; i < instr->num_params; i++) {
       validate_sized_src(&instr->params[i], state,
                          instr->callee->params[i].bit_size,
diff --git a/src/compiler/spirv/vtn_cfg.c b/src/compiler/spirv/vtn_cfg.c
index 3feb1acf1991e..183048d94a0de 100644
--- a/src/compiler/spirv/vtn_cfg.c
+++ b/src/compiler/spirv/vtn_cfg.c
@@ -55,6 +55,7 @@ glsl_type_add_to_function_params(const struct glsl_type *type,
       func->params[(*param_idx)++] = (nir_parameter) {
          .num_components = glsl_get_vector_elements(type),
          .bit_size = glsl_get_bit_size(type),
+         .type = type,
       };
    } else if (glsl_type_is_array_or_matrix(type)) {
       unsigned elems = glsl_get_length(type);
@@ -216,6 +217,8 @@ vtn_cfg_handle_prepass_instruction(struct vtn_builder *b, SpvOp opcode,
          func->params[idx++] = (nir_parameter) {
             .num_components = nir_address_format_num_components(addr_format),
             .bit_size = nir_address_format_bit_size(addr_format),
+            .is_return = true,
+            .type = func_type->return_type->type,
          };
       }
 
-- 
GitLab


From 1a422290955ccee71055d0442982ba6bbb5b8d71 Mon Sep 17 00:00:00 2001
From: Friedrich Vock <friedrich.vock@gmx.de>
Date: Fri, 22 Mar 2024 07:35:25 +0100
Subject: [PATCH 05/71] !29578

---
 .../nir/radv_nir_lower_hit_attrib_derefs.c    |  4 +-
 src/amd/vulkan/radv_shader.c                  |  2 +-
 src/asahi/clc/asahi_clc.c                     |  3 +-
 src/broadcom/compiler/nir_to_vir.c            |  2 +-
 src/broadcom/vulkan/v3dv_pipeline.c           |  2 +-
 src/compiler/nir/nir.h                        |  4 +-
 src/compiler/nir/nir_split_vars.c             | 42 +++++++++++++++----
 src/compiler/nir/tests/vars_tests.cpp         | 28 ++++++-------
 src/gallium/drivers/radeonsi/si_shader_nir.c  |  2 +-
 src/gallium/frontends/lavapipe/lvp_pipeline.c |  2 +-
 src/intel/compiler/brw_nir.c                  |  4 +-
 src/intel/compiler/elk/elk_nir.c              |  2 +-
 src/microsoft/clc/clc_compiler.c              |  2 +-
 src/microsoft/spirv_to_dxil/dxil_spirv_nir.c  |  4 +-
 src/nouveau/compiler/nak_nir.c                |  4 +-
 src/nouveau/vulkan/nvk_codegen.c              |  4 +-
 16 files changed, 69 insertions(+), 42 deletions(-)

diff --git a/src/amd/vulkan/nir/radv_nir_lower_hit_attrib_derefs.c b/src/amd/vulkan/nir/radv_nir_lower_hit_attrib_derefs.c
index 2cc5f61d876cb..92d1fba679fef 100644
--- a/src/amd/vulkan/nir/radv_nir_lower_hit_attrib_derefs.c
+++ b/src/amd/vulkan/nir/radv_nir_lower_hit_attrib_derefs.c
@@ -107,11 +107,11 @@ radv_nir_lower_rt_vars(nir_shader *shader, nir_variable_mode mode, uint32_t base
 {
    bool progress = false;
 
-   progress |= nir_split_struct_vars(shader, mode);
+   progress |= nir_split_struct_vars(shader, mode, glsl_get_natural_size_align_bytes);
    progress |= nir_lower_indirect_derefs(shader, mode, UINT32_MAX);
-   progress |= nir_split_array_vars(shader, mode);
 
    progress |= nir_lower_vars_to_explicit_types(shader, mode, glsl_get_natural_size_align_bytes);
+   progress |= nir_split_array_vars(shader, mode, glsl_get_natural_size_align_bytes);
 
    struct lower_hit_attrib_deref_args args = {
       .mode = mode,
diff --git a/src/amd/vulkan/radv_shader.c b/src/amd/vulkan/radv_shader.c
index 644d3d7ed4bc9..35d72b81da65a 100644
--- a/src/amd/vulkan/radv_shader.c
+++ b/src/amd/vulkan/radv_shader.c
@@ -127,7 +127,7 @@ radv_optimize_nir(struct nir_shader *shader, bool optimize_conservatively)
    do {
       progress = false;
 
-      NIR_LOOP_PASS(progress, skip, shader, nir_split_array_vars, nir_var_function_temp);
+      NIR_LOOP_PASS(progress, skip, shader, nir_split_array_vars, nir_var_function_temp, NULL);
       NIR_LOOP_PASS(progress, skip, shader, nir_shrink_vec_array_vars, nir_var_function_temp);
 
       if (!shader->info.var_copies_lowered) {
diff --git a/src/asahi/clc/asahi_clc.c b/src/asahi/clc/asahi_clc.c
index 98ea951bfc21f..0ccf543f02002 100644
--- a/src/asahi/clc/asahi_clc.c
+++ b/src/asahi/clc/asahi_clc.c
@@ -162,7 +162,8 @@ optimize(nir_shader *nir)
       NIR_PASS(progress, nir, nir_opt_loop_unroll);
 
       NIR_PASS(progress, nir, nir_split_var_copies);
-      NIR_PASS(progress, nir, nir_split_struct_vars, nir_var_function_temp);
+      NIR_PASS(progress, nir, nir_split_struct_vars, nir_var_function_temp,
+               NULL);
    } while (progress);
 }
 
diff --git a/src/broadcom/compiler/nir_to_vir.c b/src/broadcom/compiler/nir_to_vir.c
index 0b2f7c5455e1e..c09613a160da4 100644
--- a/src/broadcom/compiler/nir_to_vir.c
+++ b/src/broadcom/compiler/nir_to_vir.c
@@ -2063,7 +2063,7 @@ v3d_optimize_nir(struct v3d_compile *c, struct nir_shader *s)
         do {
                 progress = false;
 
-                NIR_PASS(progress, s, nir_split_array_vars, nir_var_function_temp);
+                NIR_PASS(progress, s, nir_split_array_vars, nir_var_function_temp, NULL);
                 NIR_PASS(progress, s, nir_shrink_vec_array_vars, nir_var_function_temp);
                 NIR_PASS(progress, s, nir_opt_deref);
 
diff --git a/src/broadcom/vulkan/v3dv_pipeline.c b/src/broadcom/vulkan/v3dv_pipeline.c
index 5fca36f0d013d..bf155a542a99d 100644
--- a/src/broadcom/vulkan/v3dv_pipeline.c
+++ b/src/broadcom/vulkan/v3dv_pipeline.c
@@ -295,7 +295,7 @@ preprocess_nir(nir_shader *nir)
    NIR_PASS(_, nir, nir_lower_global_vars_to_local);
 
    NIR_PASS(_, nir, nir_split_var_copies);
-   NIR_PASS(_, nir, nir_split_struct_vars, nir_var_function_temp);
+   NIR_PASS(_, nir, nir_split_struct_vars, nir_var_function_temp, NULL);
 
    v3d_optimize_nir(NULL, nir);
 
diff --git a/src/compiler/nir/nir.h b/src/compiler/nir/nir.h
index 1ad3438bc2fa2..b3120ad1a8355 100644
--- a/src/compiler/nir/nir.h
+++ b/src/compiler/nir/nir.h
@@ -5285,10 +5285,10 @@ void nir_group_loads(nir_shader *shader, nir_load_grouping grouping,
                      unsigned max_distance);
 
 bool nir_shrink_vec_array_vars(nir_shader *shader, nir_variable_mode modes);
-bool nir_split_array_vars(nir_shader *shader, nir_variable_mode modes);
+bool nir_split_array_vars(nir_shader *shader, nir_variable_mode modes, glsl_type_size_align_func type_info);
 bool nir_split_var_copies(nir_shader *shader);
 bool nir_split_per_member_structs(nir_shader *shader);
-bool nir_split_struct_vars(nir_shader *shader, nir_variable_mode modes);
+bool nir_split_struct_vars(nir_shader *shader, nir_variable_mode modes, glsl_type_size_align_func type_info);
 
 bool nir_lower_returns_impl(nir_function_impl *impl);
 bool nir_lower_returns(nir_shader *shader);
diff --git a/src/compiler/nir/nir_split_vars.c b/src/compiler/nir/nir_split_vars.c
index 3019718a3e5d2..fa88033cfc749 100644
--- a/src/compiler/nir/nir_split_vars.c
+++ b/src/compiler/nir/nir_split_vars.c
@@ -75,6 +75,7 @@ struct split_var_state {
    nir_function_impl *impl;
 
    nir_variable *base_var;
+   glsl_type_size_align_func type_info;
 };
 
 struct field {
@@ -88,6 +89,8 @@ struct field {
    /* The field currently being recursed */
    unsigned current_index;
 
+   unsigned location;
+
    nir_variable *var;
 };
 
@@ -147,6 +150,7 @@ static void
 init_field_for_type(struct field *field, struct field *parent,
                     const struct glsl_type *type,
                     const char *name,
+                    unsigned base_location,
                     struct split_var_state *state)
 {
    *field = (struct field){
@@ -172,7 +176,12 @@ init_field_for_type(struct field *field, struct field *parent,
          field->current_index = i;
          init_field_for_type(&field->fields[i], field,
                              glsl_get_struct_field(struct_type, i),
-                             field_name, state);
+                             field_name, base_location, state);
+         unsigned size, align;
+         if (state->type_info) {
+            state->type_info(glsl_get_struct_field(struct_type, i), &size, &align);
+            base_location += size;
+         }
       }
    } else {
       const struct glsl_type *var_type = type;
@@ -189,6 +198,9 @@ init_field_for_type(struct field *field, struct field *parent,
          field->var = nir_variable_create(state->shader, mode, var_type, name);
       }
       field->var->data.ray_query = state->base_var->data.ray_query;
+      /* If we have no type info, we can't determine locations accurately, so don't even try. */
+      if (state->type_info)
+         field->var->data.driver_location = base_location;
       field->var->constant_initializer = gather_constant_initializers(state->base_var->constant_initializer,
                                                                       field->var, state->base_var->type,
                                                                       root, state);
@@ -202,12 +214,14 @@ split_var_list_structs(nir_shader *shader,
                        nir_variable_mode mode,
                        struct hash_table *var_field_map,
                        struct set **complex_vars,
+                       glsl_type_size_align_func type_info,
                        void *mem_ctx)
 {
    struct split_var_state state = {
       .mem_ctx = mem_ctx,
       .shader = shader,
       .impl = impl,
+      .type_info = type_info,
    };
 
    struct exec_list split_vars;
@@ -240,7 +254,7 @@ split_var_list_structs(nir_shader *shader,
       state.base_var = var;
 
       struct field *root_field = ralloc(mem_ctx, struct field);
-      init_field_for_type(root_field, NULL, var->type, var->name, &state);
+      init_field_for_type(root_field, NULL, var->type, var->name, var->data.driver_location, &state);
       _mesa_hash_table_insert(var_field_map, var, root_field);
    }
 
@@ -346,7 +360,7 @@ split_struct_derefs_impl(nir_function_impl *impl,
  * variables of the given mode will contain a struct type.
  */
 bool
-nir_split_struct_vars(nir_shader *shader, nir_variable_mode modes)
+nir_split_struct_vars(nir_shader *shader, nir_variable_mode modes, glsl_type_size_align_func type_info)
 {
    void *mem_ctx = ralloc_context(NULL);
    struct hash_table *var_field_map =
@@ -361,6 +375,7 @@ nir_split_struct_vars(nir_shader *shader, nir_variable_mode modes)
                                                  global_modes,
                                                  var_field_map,
                                                  &complex_vars,
+                                                 type_info,
                                                  mem_ctx);
    }
 
@@ -373,6 +388,7 @@ nir_split_struct_vars(nir_shader *shader, nir_variable_mode modes)
                                                    nir_var_function_temp,
                                                    var_field_map,
                                                    &complex_vars,
+                                                   type_info,
                                                    mem_ctx);
       }
 
@@ -555,6 +571,8 @@ create_split_array_vars(struct array_var_info *var_info,
                         const char *name,
                         nir_shader *shader,
                         nir_function_impl *impl,
+                        glsl_type_size_align_func type_info,
+                        unsigned base_location,
                         void *mem_ctx)
 {
    while (level < var_info->num_levels && !var_info->levels[level].split) {
@@ -577,6 +595,8 @@ create_split_array_vars(struct array_var_info *var_info,
                                           var_info->split_var_type, name);
       }
       split->var->data.ray_query = var_info->base_var->data.ray_query;
+      if (type_info)
+         split->var->data.driver_location = base_location;
    } else {
       assert(var_info->levels[level].split);
       split->num_splits = var_info->levels[level].array_len;
@@ -585,7 +605,12 @@ create_split_array_vars(struct array_var_info *var_info,
       for (unsigned i = 0; i < split->num_splits; i++) {
          create_split_array_vars(var_info, level + 1, &split->splits[i],
                                  ralloc_asprintf(mem_ctx, "%s[%d]", name, i),
-                                 shader, impl, mem_ctx);
+                                 shader, impl, type_info, base_location, mem_ctx);
+         if (type_info) {
+            unsigned size, align;
+            type_info(var_info->split_var_type, &size, &align);
+            base_location += size;
+         }
       }
    }
 }
@@ -596,6 +621,7 @@ split_var_list_arrays(nir_shader *shader,
                       struct exec_list *vars,
                       nir_variable_mode mode,
                       struct hash_table *var_info_map,
+                      glsl_type_size_align_func type_info,
                       void *mem_ctx)
 {
    struct exec_list split_vars;
@@ -651,7 +677,7 @@ split_var_list_arrays(nir_shader *shader,
    nir_foreach_variable_in_list(var, &split_vars) {
       struct array_var_info *info = get_array_var_info(var, var_info_map);
       create_split_array_vars(info, 0, &info->root_split, var->name,
-                              shader, impl, mem_ctx);
+                              shader, impl, type_info, var->data.driver_location, mem_ctx);
    }
 
    return !exec_list_is_empty(&split_vars);
@@ -909,7 +935,7 @@ split_array_access_impl(nir_function_impl *impl,
  * so it's best to just run nir_split_struct_vars first.
  */
 bool
-nir_split_array_vars(nir_shader *shader, nir_variable_mode modes)
+nir_split_array_vars(nir_shader *shader, nir_variable_mode modes, glsl_type_size_align_func type_info)
 {
    void *mem_ctx = ralloc_context(NULL);
    struct hash_table *var_info_map = _mesa_pointer_hash_table_create(mem_ctx);
@@ -955,7 +981,7 @@ nir_split_array_vars(nir_shader *shader, nir_variable_mode modes)
       has_global_splits = split_var_list_arrays(shader, NULL,
                                                 &shader->variables,
                                                 modes,
-                                                var_info_map, mem_ctx);
+                                                var_info_map, type_info, mem_ctx);
    }
 
    bool progress = false;
@@ -965,7 +991,7 @@ nir_split_array_vars(nir_shader *shader, nir_variable_mode modes)
          has_local_splits = split_var_list_arrays(shader, impl,
                                                   &impl->locals,
                                                   nir_var_function_temp,
-                                                  var_info_map, mem_ctx);
+                                                  var_info_map, type_info, mem_ctx);
       }
 
       if (has_global_splits || has_local_splits) {
diff --git a/src/compiler/nir/tests/vars_tests.cpp b/src/compiler/nir/tests/vars_tests.cpp
index e7235b20d691b..be71ba5a4bb88 100644
--- a/src/compiler/nir/tests/vars_tests.cpp
+++ b/src/compiler/nir/tests/vars_tests.cpp
@@ -2001,7 +2001,7 @@ TEST_F(nir_split_vars_test, simple_split)
    ASSERT_EQ(count_derefs(nir_deref_type_array), 4);
    ASSERT_EQ(count_function_temp_vars(), 1);
 
-   bool progress = nir_split_array_vars(b->shader, nir_var_function_temp);
+   bool progress = nir_split_array_vars(b->shader, nir_var_function_temp, NULL);
    EXPECT_TRUE(progress);
 
    nir_validate_shader(b->shader, NULL);
@@ -2038,7 +2038,7 @@ TEST_F(nir_split_vars_test, simple_no_split_array_struct)
    ASSERT_EQ(count_derefs(nir_deref_type_struct), 4);
    ASSERT_EQ(count_function_temp_vars(), 2);
 
-   bool progress = nir_split_array_vars(b->shader, nir_var_function_temp);
+   bool progress = nir_split_array_vars(b->shader, nir_var_function_temp, NULL);
    EXPECT_TRUE(progress);
 
    nir_validate_shader(b->shader, NULL);
@@ -2068,7 +2068,7 @@ TEST_F(nir_split_vars_test, simple_split_shader_temp)
    ASSERT_EQ(count_derefs(nir_deref_type_array), 4);
    ASSERT_EQ(count_shader_temp_vars(), 1);
 
-   bool progress = nir_split_array_vars(b->shader, nir_var_shader_temp);
+   bool progress = nir_split_array_vars(b->shader, nir_var_shader_temp, NULL);
    EXPECT_TRUE(progress);
 
    nir_validate_shader(b->shader, NULL);
@@ -2090,7 +2090,7 @@ TEST_F(nir_split_vars_test, simple_oob)
    ASSERT_EQ(count_derefs(nir_deref_type_array), 6);
    ASSERT_EQ(count_function_temp_vars(), 1);
 
-   bool progress = nir_split_array_vars(b->shader, nir_var_function_temp);
+   bool progress = nir_split_array_vars(b->shader, nir_var_function_temp, NULL);
    EXPECT_TRUE(progress);
 
    nir_validate_shader(b->shader, NULL);
@@ -2112,7 +2112,7 @@ TEST_F(nir_split_vars_test, simple_unused)
    ASSERT_EQ(count_derefs(nir_deref_type_array), 2);
    ASSERT_EQ(count_function_temp_vars(), 1);
 
-   bool progress = nir_split_array_vars(b->shader, nir_var_function_temp);
+   bool progress = nir_split_array_vars(b->shader, nir_var_function_temp, NULL);
    EXPECT_TRUE(progress);
 
    nir_validate_shader(b->shader, NULL);
@@ -2139,7 +2139,7 @@ TEST_F(nir_split_vars_test, two_level_split)
    ASSERT_EQ(count_derefs(nir_deref_type_array), 20);
    ASSERT_EQ(count_function_temp_vars(), 1);
 
-   bool progress = nir_split_array_vars(b->shader, nir_var_function_temp);
+   bool progress = nir_split_array_vars(b->shader, nir_var_function_temp, NULL);
    EXPECT_TRUE(progress);
 
    nir_validate_shader(b->shader, NULL);
@@ -2164,7 +2164,7 @@ TEST_F(nir_split_vars_test, simple_dont_split)
    ASSERT_EQ(count_derefs(nir_deref_type_array), 4);
    ASSERT_EQ(count_function_temp_vars(), 1);
 
-   bool progress = nir_split_array_vars(b->shader, nir_var_function_temp);
+   bool progress = nir_split_array_vars(b->shader, nir_var_function_temp, NULL);
    EXPECT_FALSE(progress);
 
    nir_validate_shader(b->shader, NULL);
@@ -2194,7 +2194,7 @@ TEST_F(nir_split_vars_test, twolevel_dont_split_lvl_0)
    ASSERT_EQ(count_derefs(nir_deref_type_array), 28);
    ASSERT_EQ(count_function_temp_vars(), 1);
 
-   bool progress = nir_split_array_vars(b->shader, nir_var_function_temp);
+   bool progress = nir_split_array_vars(b->shader, nir_var_function_temp, NULL);
    EXPECT_TRUE(progress);
 
    nir_validate_shader(b->shader, NULL);
@@ -2225,7 +2225,7 @@ TEST_F(nir_split_vars_test, twolevel_dont_split_lvl_1)
    ASSERT_EQ(count_derefs(nir_deref_type_array), 28);
    ASSERT_EQ(count_function_temp_vars(), 1);
 
-   bool progress = nir_split_array_vars(b->shader, nir_var_function_temp);
+   bool progress = nir_split_array_vars(b->shader, nir_var_function_temp, NULL);
    EXPECT_TRUE(progress);
 
    nir_validate_shader(b->shader, NULL);
@@ -2254,7 +2254,7 @@ TEST_F(nir_split_vars_test, split_multiple_store)
    ASSERT_EQ(count_derefs(nir_deref_type_array), 8);
    ASSERT_EQ(count_function_temp_vars(), 2);
 
-   bool progress = nir_split_array_vars(b->shader, nir_var_function_temp);
+   bool progress = nir_split_array_vars(b->shader, nir_var_function_temp, NULL);
    EXPECT_TRUE(progress);
 
    nir_validate_shader(b->shader, NULL);
@@ -2286,7 +2286,7 @@ TEST_F(nir_split_vars_test, split_load_store)
    ASSERT_EQ(count_derefs(nir_deref_type_array), 12);
    ASSERT_EQ(count_function_temp_vars(), 2);
 
-   bool progress = nir_split_array_vars(b->shader, nir_var_function_temp);
+   bool progress = nir_split_array_vars(b->shader, nir_var_function_temp, NULL);
    EXPECT_TRUE(progress);
 
    nir_validate_shader(b->shader, NULL);
@@ -2318,7 +2318,7 @@ TEST_F(nir_split_vars_test, split_copy)
    ASSERT_EQ(count_derefs(nir_deref_type_array), 12);
    ASSERT_EQ(count_function_temp_vars(), 2);
 
-   bool progress = nir_split_array_vars(b->shader, nir_var_function_temp);
+   bool progress = nir_split_array_vars(b->shader, nir_var_function_temp, NULL);
    EXPECT_TRUE(progress);
 
    nir_validate_shader(b->shader, NULL);
@@ -2351,7 +2351,7 @@ TEST_F(nir_split_vars_test, split_wildcard_copy)
    ASSERT_EQ(count_function_temp_vars(), 2);
    ASSERT_EQ(count_intrinsics(nir_intrinsic_copy_deref), 1);
 
-   bool progress = nir_split_array_vars(b->shader, nir_var_function_temp);
+   bool progress = nir_split_array_vars(b->shader, nir_var_function_temp, NULL);
    EXPECT_TRUE(progress);
 
    nir_validate_shader(b->shader, NULL);
@@ -2415,7 +2415,7 @@ TEST_F(nir_split_vars_test, split_nested_struct_const_init)
 
    nir_validate_shader(b->shader, NULL);
 
-   bool progress = nir_split_struct_vars(b->shader, nir_var_mem_constant);
+   bool progress = nir_split_struct_vars(b->shader, nir_var_mem_constant, NULL);
    EXPECT_TRUE(progress);
 
    nir_validate_shader(b->shader, NULL);
diff --git a/src/gallium/drivers/radeonsi/si_shader_nir.c b/src/gallium/drivers/radeonsi/si_shader_nir.c
index 3480b765a42ab..5533385db049e 100644
--- a/src/gallium/drivers/radeonsi/si_shader_nir.c
+++ b/src/gallium/drivers/radeonsi/si_shader_nir.c
@@ -84,7 +84,7 @@ void si_nir_opts(struct si_screen *sscreen, struct nir_shader *nir, bool first)
       NIR_PASS(progress, nir, nir_lower_phis_to_scalar, false);
 
       if (first) {
-         NIR_PASS(progress, nir, nir_split_array_vars, nir_var_function_temp);
+         NIR_PASS(progress, nir, nir_split_array_vars, nir_var_function_temp, NULL);
          NIR_PASS(lower_alu_to_scalar, nir, nir_shrink_vec_array_vars, nir_var_function_temp);
          NIR_PASS(progress, nir, nir_opt_find_array_copies);
       }
diff --git a/src/gallium/frontends/lavapipe/lvp_pipeline.c b/src/gallium/frontends/lavapipe/lvp_pipeline.c
index 575121de882c8..ad532c8fe4aa5 100644
--- a/src/gallium/frontends/lavapipe/lvp_pipeline.c
+++ b/src/gallium/frontends/lavapipe/lvp_pipeline.c
@@ -228,7 +228,7 @@ optimize(nir_shader *nir)
       progress = false;
 
       NIR_PASS(progress, nir, nir_lower_flrp, 32|64, true);
-      NIR_PASS(progress, nir, nir_split_array_vars, nir_var_function_temp);
+      NIR_PASS(progress, nir, nir_split_array_vars, nir_var_function_temp, NULL);
       NIR_PASS(progress, nir, nir_shrink_vec_array_vars, nir_var_function_temp);
       NIR_PASS(progress, nir, nir_opt_deref);
       NIR_PASS(progress, nir, nir_lower_vars_to_ssa);
diff --git a/src/intel/compiler/brw_nir.c b/src/intel/compiler/brw_nir.c
index 40ef84d537943..d0ff37360eb13 100644
--- a/src/intel/compiler/brw_nir.c
+++ b/src/intel/compiler/brw_nir.c
@@ -731,7 +731,7 @@ brw_nir_optimize(nir_shader *nir,
        * code.
        */
       if (nir->info.stage != MESA_SHADER_KERNEL)
-         LOOP_OPT(nir_split_array_vars, nir_var_function_temp);
+         LOOP_OPT(nir_split_array_vars, nir_var_function_temp, NULL);
       LOOP_OPT(nir_shrink_vec_array_vars, nir_var_function_temp);
       LOOP_OPT(nir_opt_deref);
       if (LOOP_OPT(nir_opt_memcpy))
@@ -1054,7 +1054,7 @@ brw_preprocess_nir(const struct brw_compiler *compiler, nir_shader *nir,
    OPT(nir_lower_global_vars_to_local);
 
    OPT(nir_split_var_copies);
-   OPT(nir_split_struct_vars, nir_var_function_temp);
+   OPT(nir_split_struct_vars, nir_var_function_temp, NULL);
 
    brw_nir_optimize(nir, devinfo);
 
diff --git a/src/intel/compiler/elk/elk_nir.c b/src/intel/compiler/elk/elk_nir.c
index 639f600cdaabe..48234ebf1453a 100644
--- a/src/intel/compiler/elk/elk_nir.c
+++ b/src/intel/compiler/elk/elk_nir.c
@@ -932,7 +932,7 @@ elk_preprocess_nir(const struct elk_compiler *compiler, nir_shader *nir,
    OPT(nir_lower_global_vars_to_local);
 
    OPT(nir_split_var_copies);
-   OPT(nir_split_struct_vars, nir_var_function_temp);
+   OPT(nir_split_struct_vars, nir_var_function_temp, NULL);
 
    elk_nir_optimize(nir, is_scalar, devinfo);
 
diff --git a/src/microsoft/clc/clc_compiler.c b/src/microsoft/clc/clc_compiler.c
index 6440178f060ae..b3ccf895fca0d 100644
--- a/src/microsoft/clc/clc_compiler.c
+++ b/src/microsoft/clc/clc_compiler.c
@@ -957,7 +957,7 @@ clc_spirv_to_dxil(struct clc_libclc *lib,
    // While inserting new var derefs for our "logical" addressing mode, temporarily
    // switch the pointer size to 32-bit.
    nir->info.cs.ptr_size = 32;
-   NIR_PASS_V(nir, nir_split_struct_vars, nir_var_shader_temp);
+   NIR_PASS_V(nir, nir_split_struct_vars, nir_var_shader_temp, NULL);
    NIR_PASS_V(nir, dxil_nir_flatten_var_arrays, nir_var_shader_temp);
    NIR_PASS_V(nir, dxil_nir_lower_var_bit_size, nir_var_shader_temp,
               (supported_int_sizes & 16) ? 16 : 32, (supported_int_sizes & 64) ? 64 : 32);
diff --git a/src/microsoft/spirv_to_dxil/dxil_spirv_nir.c b/src/microsoft/spirv_to_dxil/dxil_spirv_nir.c
index 87afaf067a2d4..92ed9fa382b38 100644
--- a/src/microsoft/spirv_to_dxil/dxil_spirv_nir.c
+++ b/src/microsoft/spirv_to_dxil/dxil_spirv_nir.c
@@ -1024,7 +1024,7 @@ dxil_spirv_nir_passes(nir_shader *nir,
       NIR_PASS_V(nir, dxil_nir_split_unaligned_loads_stores, nir_var_mem_shared);
       NIR_PASS_V(nir, nir_lower_explicit_io, nir_var_mem_shared, nir_address_format_32bit_offset);
    } else {
-      NIR_PASS_V(nir, nir_split_struct_vars, nir_var_mem_shared);
+      NIR_PASS_V(nir, nir_split_struct_vars, nir_var_mem_shared, NULL);
       NIR_PASS_V(nir, dxil_nir_flatten_var_arrays, nir_var_mem_shared);
       NIR_PASS_V(nir, dxil_nir_lower_var_bit_size, nir_var_mem_shared,
                  conf->shader_model_max >= SHADER_MODEL_6_2 ? 16 : 32, 64);
@@ -1090,7 +1090,7 @@ dxil_spirv_nir_passes(nir_shader *nir,
    }
 
    NIR_PASS_V(nir, nir_remove_dead_variables, nir_var_function_temp, NULL);
-   NIR_PASS_V(nir, nir_split_struct_vars, nir_var_function_temp);
+   NIR_PASS_V(nir, nir_split_struct_vars, nir_var_function_temp, NULL);
    NIR_PASS_V(nir, dxil_nir_flatten_var_arrays, nir_var_function_temp);
    NIR_PASS_V(nir, dxil_nir_lower_var_bit_size, nir_var_function_temp,
               conf->shader_model_max >= SHADER_MODEL_6_2 ? 16 : 32, 64);
diff --git a/src/nouveau/compiler/nak_nir.c b/src/nouveau/compiler/nak_nir.c
index 6b0e22947e40c..bfd6baf47d3eb 100644
--- a/src/nouveau/compiler/nak_nir.c
+++ b/src/nouveau/compiler/nak_nir.c
@@ -108,7 +108,7 @@ optimize_nir(nir_shader *nir, const struct nak_compiler *nak, bool allow_copies)
        * code.
        */
       if (nir->info.stage != MESA_SHADER_KERNEL)
-         OPT(nir, nir_split_array_vars, nir_var_function_temp);
+         OPT(nir, nir_split_array_vars, nir_var_function_temp, NULL);
 
       OPT(nir, nir_shrink_vec_array_vars, nir_var_function_temp);
       OPT(nir, nir_opt_deref);
@@ -382,7 +382,7 @@ nak_preprocess_nir(nir_shader *nir, const struct nak_compiler *nak)
    OPT(nir, nir_lower_global_vars_to_local);
 
    OPT(nir, nir_split_var_copies);
-   OPT(nir, nir_split_struct_vars, nir_var_function_temp);
+   OPT(nir, nir_split_struct_vars, nir_var_function_temp, NULL);
 
    /* Optimize but allow copies because we haven't lowered them yet */
    optimize_nir(nir, nak, true /* allow_copies */);
diff --git a/src/nouveau/vulkan/nvk_codegen.c b/src/nouveau/vulkan/nvk_codegen.c
index 45e9818250a68..2647c3989dde7 100644
--- a/src/nouveau/vulkan/nvk_codegen.c
+++ b/src/nouveau/vulkan/nvk_codegen.c
@@ -104,7 +104,7 @@ lower_fragcoord_instr(nir_builder *b, nir_instr *instr, UNUSED void *_data)
 void
 nvk_cg_preprocess_nir(nir_shader *nir)
 {
-   NIR_PASS(_, nir, nir_split_struct_vars, nir_var_function_temp);
+   NIR_PASS(_, nir, nir_split_struct_vars, nir_var_function_temp, NULL);
    NIR_PASS(_, nir, nir_lower_vars_to_ssa);
 
    NIR_PASS(_, nir, nir_split_var_copies);
@@ -133,7 +133,7 @@ nvk_cg_optimize_nir(nir_shader *nir)
    do {
       progress = false;
 
-      NIR_PASS(progress, nir, nir_split_array_vars, nir_var_function_temp);
+      NIR_PASS(progress, nir, nir_split_array_vars, nir_var_function_temp, NULL);
       NIR_PASS(progress, nir, nir_shrink_vec_array_vars, nir_var_function_temp);
 
       if (!nir->info.var_copies_lowered) {
-- 
GitLab


From 5ca210f8c0d3b34d9ae3be857f075aa903d063f4 Mon Sep 17 00:00:00 2001
From: Friedrich Vock <friedrich.vock@gmx.de>
Date: Wed, 5 Jun 2024 11:27:06 +0200
Subject: [PATCH 06/71] nir/intrinsics: Add incoming/outgoing payload
 load/store instructions

With RT function calls, these are going to get lowered to:
- load/store_param (incoming payload)
- load/store_var (outgoing payload)
---
 src/compiler/nir/nir_intrinsics.py | 4 ++++
 1 file changed, 4 insertions(+)

diff --git a/src/compiler/nir/nir_intrinsics.py b/src/compiler/nir/nir_intrinsics.py
index 4ebd319cabe88..b97aeac0ea754 100644
--- a/src/compiler/nir/nir_intrinsics.py
+++ b/src/compiler/nir/nir_intrinsics.py
@@ -1644,6 +1644,10 @@ intrinsic("execute_miss_amd", src_comp=[1])
 # BASE=dword index
 intrinsic("load_hit_attrib_amd", dest_comp=1, bit_sizes=[32], indices=[BASE])
 intrinsic("store_hit_attrib_amd", src_comp=[1], indices=[BASE])
+intrinsic("load_incoming_ray_payload_amd", dest_comp=1, bit_sizes=[32], indices=[BASE])
+intrinsic("store_incoming_ray_payload_amd", src_comp=[1], indices=[BASE])
+intrinsic("load_outgoing_ray_payload_amd", dest_comp=1, bit_sizes=[32], indices=[BASE])
+intrinsic("store_outgoing_ray_payload_amd", src_comp=[1], indices=[BASE])
 
 # Load forced VRS rates.
 intrinsic("load_force_vrs_rates_amd", dest_comp=1, bit_sizes=[32], flags=[CAN_ELIMINATE, CAN_REORDER])
-- 
GitLab


From cf60fcbc09056da5e41f549ea96948bc6334ff1c Mon Sep 17 00:00:00 2001
From: Friedrich Vock <friedrich.vock@gmx.de>
Date: Thu, 6 Jun 2024 08:07:34 +0200
Subject: [PATCH 07/71] radv: Temporarily disable RT pipelines

---
 src/amd/vulkan/radv_physical_device.c | 4 ++++
 1 file changed, 4 insertions(+)

diff --git a/src/amd/vulkan/radv_physical_device.c b/src/amd/vulkan/radv_physical_device.c
index d4708b93bcea7..572dc61e68342 100644
--- a/src/amd/vulkan/radv_physical_device.c
+++ b/src/amd/vulkan/radv_physical_device.c
@@ -102,6 +102,10 @@ radv_calibrated_timestamps_enabled(const struct radv_physical_device *pdev)
 bool
 radv_enable_rt(const struct radv_physical_device *pdev, bool rt_pipelines)
 {
+   /* Temporarily under construction! */
+   if (rt_pipelines)
+      return false;
+
    if (pdev->info.gfx_level < GFX10_3 && !radv_emulate_rt(pdev))
       return false;
 
-- 
GitLab


From 494c76fe24c9f54b66830460f96b1a6c1b305408 Mon Sep 17 00:00:00 2001
From: Friedrich Vock <friedrich.vock@gmx.de>
Date: Wed, 5 Jun 2024 11:28:21 +0200
Subject: [PATCH 08/71] nir: Remove
 nir_intrinsic_load_rt_arg_scratch_offset_amd

Not needed anymore.
---
 src/amd/vulkan/nir/radv_nir_rt_shader.c    | 12 +-----------
 src/amd/vulkan/radv_pipeline_rt.c          |  1 -
 src/compiler/nir/nir_divergence_analysis.c |  1 -
 src/compiler/nir/nir_intrinsics.py         |  3 ---
 4 files changed, 1 insertion(+), 16 deletions(-)

diff --git a/src/amd/vulkan/nir/radv_nir_rt_shader.c b/src/amd/vulkan/nir/radv_nir_rt_shader.c
index b9b2ee3b9a062..938bfebe1d904 100644
--- a/src/amd/vulkan/nir/radv_nir_rt_shader.c
+++ b/src/amd/vulkan/nir/radv_nir_rt_shader.c
@@ -486,10 +486,6 @@ radv_lower_rt_instruction(nir_builder *b, nir_instr *instr, void *_data)
          nir_src_rewrite(&intr->src[1], nir_iadd_nuw(b, nir_load_var(b, vars->stack_ptr), intr->src[1].ssa));
       return true;
    }
-   case nir_intrinsic_load_rt_arg_scratch_offset_amd: {
-      ret = nir_load_var(b, vars->arg);
-      break;
-   }
    case nir_intrinsic_load_shader_record_ptr: {
       ret = nir_load_var(b, vars->shader_record_ptr);
       break;
@@ -1050,6 +1046,7 @@ lower_any_hit_for_intersection(nir_shader *any_hit)
          /* Scratch offset */
          .num_components = 1,
          .bit_size = 32,
+         .is_divergent = true,
       },
    };
    impl->function->num_params = ARRAY_SIZE(params);
@@ -1116,12 +1113,6 @@ lower_any_hit_for_intersection(nir_shader *any_hit)
                b->cursor = nir_before_instr(instr);
                nir_src_rewrite(&intrin->src[1], nir_iadd_nuw(b, scratch_offset, intrin->src[1].ssa));
                break;
-            case nir_intrinsic_load_rt_arg_scratch_offset_amd:
-               b->cursor = nir_after_instr(instr);
-               nir_def *arg_offset = nir_isub(b, &intrin->def, scratch_offset);
-               nir_def_rewrite_uses_after(&intrin->def, arg_offset, arg_offset->parent_instr);
-               break;
-
             default:
                break;
             }
@@ -1771,7 +1762,6 @@ radv_build_traversal_shader(struct radv_device *device, struct radv_ray_tracing_
    nir_store_var(&b, vars.cull_mask_and_flags, nir_load_cull_mask_and_flags_amd(&b), 0x1);
    nir_store_var(&b, vars.origin, nir_load_ray_world_origin(&b), 0x7);
    nir_store_var(&b, vars.direction, nir_load_ray_world_direction(&b), 0x7);
-   nir_store_var(&b, vars.arg, nir_load_rt_arg_scratch_offset_amd(&b), 0x1);
    nir_store_var(&b, vars.stack_ptr, nir_imm_int(&b, 0), 0x1);
 
    radv_build_traversal(device, pipeline, pCreateInfo, false, &b, &vars, false, info);
diff --git a/src/amd/vulkan/radv_pipeline_rt.c b/src/amd/vulkan/radv_pipeline_rt.c
index 2a69006d70dc3..0730ffcdbd3e2 100644
--- a/src/amd/vulkan/radv_pipeline_rt.c
+++ b/src/amd/vulkan/radv_pipeline_rt.c
@@ -306,7 +306,6 @@ should_move_rt_instruction(nir_intrinsic_instr *instr)
    switch (instr->intrinsic) {
    case nir_intrinsic_load_hit_attrib_amd:
       return nir_intrinsic_base(instr) < RADV_MAX_HIT_ATTRIB_DWORDS;
-   case nir_intrinsic_load_rt_arg_scratch_offset_amd:
    case nir_intrinsic_load_ray_flags:
    case nir_intrinsic_load_ray_object_origin:
    case nir_intrinsic_load_ray_world_origin:
diff --git a/src/compiler/nir/nir_divergence_analysis.c b/src/compiler/nir/nir_divergence_analysis.c
index 5519c2e49490f..f56ad90118a32 100644
--- a/src/compiler/nir/nir_divergence_analysis.c
+++ b/src/compiler/nir/nir_divergence_analysis.c
@@ -711,7 +711,6 @@ visit_intrinsic(nir_intrinsic_instr *instr, struct divergence_state *state)
    case nir_intrinsic_load_packed_passthrough_primitive_amd:
    case nir_intrinsic_load_initial_edgeflags_amd:
    case nir_intrinsic_gds_atomic_add_amd:
-   case nir_intrinsic_load_rt_arg_scratch_offset_amd:
    case nir_intrinsic_load_intersection_opaque_amd:
    case nir_intrinsic_load_vector_arg_amd:
    case nir_intrinsic_load_btd_stack_id_intel:
diff --git a/src/compiler/nir/nir_intrinsics.py b/src/compiler/nir/nir_intrinsics.py
index b97aeac0ea754..d4847c4487912 100644
--- a/src/compiler/nir/nir_intrinsics.py
+++ b/src/compiler/nir/nir_intrinsics.py
@@ -1614,9 +1614,6 @@ intrinsic("bvh64_intersect_ray_amd", [4, 2, 1, 3, 3, 3], 4, flags=[CAN_ELIMINATE
 # Return of a callable in raytracing pipelines
 intrinsic("rt_return_amd")
 
-# offset into scratch for the input callable data in a raytracing pipeline.
-system_value("rt_arg_scratch_offset_amd", 1)
-
 # Whether to call the anyhit shader for an intersection in an intersection shader.
 system_value("intersection_opaque_amd", 1, bit_sizes=[1])
 
-- 
GitLab


From 8f2d7c4616cb61ec4ba066344e253b9281d06e0d Mon Sep 17 00:00:00 2001
From: Friedrich Vock <friedrich.vock@gmx.de>
Date: Wed, 5 Jun 2024 11:31:55 +0200
Subject: [PATCH 09/71] radv/rt: Remove RT priorities

They have been been useful for ensuring reconvergence, but RT function
calls ensure that on their own now.
---
 src/amd/vulkan/nir/radv_nir_rt_shader.c | 37 -------------------------
 src/amd/vulkan/radv_cmd_buffer.c        |  2 +-
 src/amd/vulkan/radv_pipeline_rt.c       |  2 +-
 src/amd/vulkan/radv_shader.h            | 27 ------------------
 4 files changed, 2 insertions(+), 66 deletions(-)

diff --git a/src/amd/vulkan/nir/radv_nir_rt_shader.c b/src/amd/vulkan/nir/radv_nir_rt_shader.c
index 938bfebe1d904..cb8c4b436f836 100644
--- a/src/amd/vulkan/nir/radv_nir_rt_shader.c
+++ b/src/amd/vulkan/nir/radv_nir_rt_shader.c
@@ -1883,43 +1883,6 @@ lower_rt_instructions_monolithic(nir_shader *shader, struct radv_device *device,
    lower_hit_attribs(shader, hit_attribs, 0);
 }
 
-/** Select the next shader based on priorities:
- *
- * Detect the priority of the shader stage by the lowest bits in the address (low to high):
- *  - Raygen              - idx 0
- *  - Traversal           - idx 1
- *  - Closest Hit / Miss  - idx 2
- *  - Callable            - idx 3
- *
- *
- * This gives us the following priorities:
- * Raygen       :  Callable  >               >  Traversal  >  Raygen
- * Traversal    :            >  Chit / Miss  >             >  Raygen
- * CHit / Miss  :  Callable  >  Chit / Miss  >  Traversal  >  Raygen
- * Callable     :  Callable  >  Chit / Miss  >             >  Raygen
- */
-static nir_def *
-select_next_shader(nir_builder *b, nir_def *shader_addr, unsigned wave_size)
-{
-   gl_shader_stage stage = b->shader->info.stage;
-   nir_def *prio = nir_iand_imm(b, shader_addr, radv_rt_priority_mask);
-   nir_def *ballot = nir_ballot(b, 1, wave_size, nir_imm_bool(b, true));
-   nir_def *ballot_traversal = nir_ballot(b, 1, wave_size, nir_ieq_imm(b, prio, radv_rt_priority_traversal));
-   nir_def *ballot_hit_miss = nir_ballot(b, 1, wave_size, nir_ieq_imm(b, prio, radv_rt_priority_hit_miss));
-   nir_def *ballot_callable = nir_ballot(b, 1, wave_size, nir_ieq_imm(b, prio, radv_rt_priority_callable));
-
-   if (stage != MESA_SHADER_CALLABLE && stage != MESA_SHADER_INTERSECTION)
-      ballot = nir_bcsel(b, nir_ine_imm(b, ballot_traversal, 0), ballot_traversal, ballot);
-   if (stage != MESA_SHADER_RAYGEN)
-      ballot = nir_bcsel(b, nir_ine_imm(b, ballot_hit_miss, 0), ballot_hit_miss, ballot);
-   if (stage != MESA_SHADER_INTERSECTION)
-      ballot = nir_bcsel(b, nir_ine_imm(b, ballot_callable, 0), ballot_callable, ballot);
-
-   nir_def *lsb = nir_find_lsb(b, ballot);
-   nir_def *next = nir_read_invocation(b, shader_addr, lsb);
-   return nir_iand_imm(b, next, ~radv_rt_priority_mask);
-}
-
 static void
 radv_store_arg(nir_builder *b, const struct radv_shader_args *args, const struct radv_ray_tracing_stage_info *info,
                struct ac_arg arg, nir_def *value)
diff --git a/src/amd/vulkan/radv_cmd_buffer.c b/src/amd/vulkan/radv_cmd_buffer.c
index 2776428c453c7..20469c905e504 100644
--- a/src/amd/vulkan/radv_cmd_buffer.c
+++ b/src/amd/vulkan/radv_cmd_buffer.c
@@ -12206,7 +12206,7 @@ radv_trace_rays(struct radv_cmd_buffer *cmd_buffer, VkTraceRaysIndirectCommand2K
    const struct radv_userdata_info *shader_loc = radv_get_user_sgpr(rt_prolog, AC_UD_CS_TRAVERSAL_SHADER_ADDR);
    struct radv_shader *traversal_shader = cmd_buffer->state.shaders[MESA_SHADER_INTERSECTION];
    if (shader_loc->sgpr_idx != -1 && traversal_shader) {
-      uint64_t traversal_va = traversal_shader->va | radv_rt_priority_traversal;
+      uint64_t traversal_va = traversal_shader->va;
       radv_emit_shader_pointer(device, cmd_buffer->cs, base_reg + shader_loc->sgpr_idx * 4, traversal_va, true);
    }
 
diff --git a/src/amd/vulkan/radv_pipeline_rt.c b/src/amd/vulkan/radv_pipeline_rt.c
index 0730ffcdbd3e2..b7c7b6ce2c036 100644
--- a/src/amd/vulkan/radv_pipeline_rt.c
+++ b/src/amd/vulkan/radv_pipeline_rt.c
@@ -1040,7 +1040,7 @@ radv_rt_pipeline_create(VkDevice _device, VkPipelineCache _cache, const VkRayTra
       if (pipeline->groups[i].recursive_shader != VK_SHADER_UNUSED_KHR) {
          struct radv_shader *shader = pipeline->stages[pipeline->groups[i].recursive_shader].shader;
          if (shader)
-            pipeline->groups[i].handle.recursive_shader_ptr = shader->va | radv_get_rt_priority(shader->info.stage);
+            pipeline->groups[i].handle.recursive_shader_ptr = shader->va;
       }
    }
 
diff --git a/src/amd/vulkan/radv_shader.h b/src/amd/vulkan/radv_shader.h
index 0dfd3a13b3f44..c9b471ce79fbb 100644
--- a/src/amd/vulkan/radv_shader.h
+++ b/src/amd/vulkan/radv_shader.h
@@ -669,33 +669,6 @@ nir_shader *radv_build_traversal_shader(struct radv_device *device, struct radv_
                                         const VkRayTracingPipelineCreateInfoKHR *pCreateInfo,
                                         struct radv_ray_tracing_stage_info *info);
 
-enum radv_rt_priority {
-   radv_rt_priority_raygen = 0,
-   radv_rt_priority_traversal = 1,
-   radv_rt_priority_hit_miss = 2,
-   radv_rt_priority_callable = 3,
-   radv_rt_priority_mask = 0x3,
-};
-
-static inline enum radv_rt_priority
-radv_get_rt_priority(gl_shader_stage stage)
-{
-   switch (stage) {
-   case MESA_SHADER_RAYGEN:
-      return radv_rt_priority_raygen;
-   case MESA_SHADER_INTERSECTION:
-   case MESA_SHADER_ANY_HIT:
-      return radv_rt_priority_traversal;
-   case MESA_SHADER_CLOSEST_HIT:
-   case MESA_SHADER_MISS:
-      return radv_rt_priority_hit_miss;
-   case MESA_SHADER_CALLABLE:
-      return radv_rt_priority_callable;
-   default:
-      unreachable("Unimplemented RT shader stage.");
-   }
-}
-
 struct radv_shader_layout;
 enum radv_pipeline_type;
 
-- 
GitLab


From df25e25ca43ed553fe50ec10d3ad392454d719d1 Mon Sep 17 00:00:00 2001
From: Friedrich Vock <friedrich.vock@gmx.de>
Date: Wed, 5 Jun 2024 11:39:33 +0200
Subject: [PATCH 10/71] radv/rt: Refactor radv_nir_lower_rt_vars

Now we can use it on load/store instruction. Will be used for lowering
payloads to load/store_*_payload instructions.
---
 .../nir/radv_nir_lower_hit_attrib_derefs.c    | 93 ++++++++++++++-----
 1 file changed, 69 insertions(+), 24 deletions(-)

diff --git a/src/amd/vulkan/nir/radv_nir_lower_hit_attrib_derefs.c b/src/amd/vulkan/nir/radv_nir_lower_hit_attrib_derefs.c
index 92d1fba679fef..bd5d695cd046e 100644
--- a/src/amd/vulkan/nir/radv_nir_lower_hit_attrib_derefs.c
+++ b/src/amd/vulkan/nir/radv_nir_lower_hit_attrib_derefs.c
@@ -9,13 +9,19 @@
 #include "radv_constants.h"
 #include "radv_nir.h"
 
+typedef nir_def *(*load_intrin_cb)(nir_builder *b, unsigned base);
+typedef void (*store_intrin_cb)(nir_builder *b, nir_def *val, unsigned base);
+
 struct lower_hit_attrib_deref_args {
    nir_variable_mode mode;
    uint32_t base_offset;
+
+   load_intrin_cb load_cb;
+   store_intrin_cb store_cb;
 };
 
 static bool
-lower_hit_attrib_deref(nir_builder *b, nir_instr *instr, void *data)
+lower_rt_var_deref(nir_builder *b, nir_instr *instr, void *data)
 {
    if (instr->type != nir_instr_type_intrinsic)
       return false;
@@ -45,19 +51,16 @@ lower_hit_attrib_deref(nir_builder *b, nir_instr *instr, void *data)
          uint32_t comp_offset = offset % 4;
 
          if (bit_size == 64) {
-            components[comp] = nir_pack_64_2x32_split(b, nir_load_hit_attrib_amd(b, .base = base),
-                                                      nir_load_hit_attrib_amd(b, .base = base + 1));
+            components[comp] = nir_pack_64_2x32_split(b, args->load_cb(b, base), args->load_cb(b, base + 1));
          } else if (bit_size == 32) {
-            components[comp] = nir_load_hit_attrib_amd(b, .base = base);
+            components[comp] = args->load_cb(b, base);
          } else if (bit_size == 16) {
-            components[comp] =
-               nir_channel(b, nir_unpack_32_2x16(b, nir_load_hit_attrib_amd(b, .base = base)), comp_offset / 2);
+            components[comp] = nir_channel(b, nir_unpack_32_2x16(b, args->load_cb(b, base)), comp_offset / 2);
          } else if (bit_size == 8) {
-            components[comp] =
-               nir_channel(b, nir_unpack_bits(b, nir_load_hit_attrib_amd(b, .base = base), 8), comp_offset);
+            components[comp] = nir_channel(b, nir_unpack_bits(b, args->load_cb(b, base), 8), comp_offset);
          } else {
             assert(bit_size == 1);
-            components[comp] = nir_i2b(b, nir_load_hit_attrib_amd(b, .base = base));
+            components[comp] = nir_i2b(b, args->load_cb(b, base));
          }
       }
 
@@ -75,25 +78,25 @@ lower_hit_attrib_deref(nir_builder *b, nir_instr *instr, void *data)
          nir_def *component = nir_channel(b, value, comp);
 
          if (bit_size == 64) {
-            nir_store_hit_attrib_amd(b, nir_unpack_64_2x32_split_x(b, component), .base = base);
-            nir_store_hit_attrib_amd(b, nir_unpack_64_2x32_split_y(b, component), .base = base + 1);
+            args->store_cb(b, nir_unpack_64_2x32_split_x(b, component), base);
+            args->store_cb(b, nir_unpack_64_2x32_split_y(b, component), base + 1);
          } else if (bit_size == 32) {
-            nir_store_hit_attrib_amd(b, component, .base = base);
+            args->store_cb(b, component, base);
          } else if (bit_size == 16) {
-            nir_def *prev = nir_unpack_32_2x16(b, nir_load_hit_attrib_amd(b, .base = base));
+            nir_def *prev = nir_unpack_32_2x16(b, args->load_cb(b, base));
             nir_def *components[2];
             for (uint32_t word = 0; word < 2; word++)
                components[word] = (word == comp_offset / 2) ? nir_channel(b, value, comp) : nir_channel(b, prev, word);
-            nir_store_hit_attrib_amd(b, nir_pack_32_2x16(b, nir_vec(b, components, 2)), .base = base);
+            args->store_cb(b, nir_pack_32_2x16(b, nir_vec(b, components, 2)), base);
          } else if (bit_size == 8) {
-            nir_def *prev = nir_unpack_bits(b, nir_load_hit_attrib_amd(b, .base = base), 8);
+            nir_def *prev = nir_unpack_bits(b, args->load_cb(b, base), 8);
             nir_def *components[4];
             for (uint32_t byte = 0; byte < 4; byte++)
                components[byte] = (byte == comp_offset) ? nir_channel(b, value, comp) : nir_channel(b, prev, byte);
-            nir_store_hit_attrib_amd(b, nir_pack_32_4x8(b, nir_vec(b, components, 4)), .base = base);
+            args->store_cb(b, nir_pack_32_4x8(b, nir_vec(b, components, 4)), base);
          } else {
             assert(bit_size == 1);
-            nir_store_hit_attrib_amd(b, nir_b2i32(b, component), .base = base);
+            args->store_cb(b, nir_b2i32(b, component), base);
          }
       }
    }
@@ -103,22 +106,23 @@ lower_hit_attrib_deref(nir_builder *b, nir_instr *instr, void *data)
 }
 
 static bool
-radv_nir_lower_rt_vars(nir_shader *shader, nir_variable_mode mode, uint32_t base_offset)
+radv_nir_lower_rt_vars(nir_shader *shader, nir_variable_mode mode, load_intrin_cb load_cb, store_intrin_cb store_cb,
+                       uint32_t base_offset)
 {
    bool progress = false;
 
    progress |= nir_split_struct_vars(shader, mode, glsl_get_natural_size_align_bytes);
    progress |= nir_lower_indirect_derefs(shader, mode, UINT32_MAX);
-
-   progress |= nir_lower_vars_to_explicit_types(shader, mode, glsl_get_natural_size_align_bytes);
    progress |= nir_split_array_vars(shader, mode, glsl_get_natural_size_align_bytes);
 
    struct lower_hit_attrib_deref_args args = {
       .mode = mode,
       .base_offset = base_offset,
+      .load_cb = load_cb,
+      .store_cb = store_cb,
    };
 
-   progress |= nir_shader_instructions_pass(shader, lower_hit_attrib_deref,
+   progress |= nir_shader_instructions_pass(shader, lower_rt_var_deref,
                                             nir_metadata_block_index | nir_metadata_dominance, &args);
 
    if (progress) {
@@ -129,16 +133,57 @@ radv_nir_lower_rt_vars(nir_shader *shader, nir_variable_mode mode, uint32_t base
    return progress;
 }
 
+static nir_def *
+load_hit_attrib_cb(nir_builder *b, unsigned base)
+{
+   return nir_load_hit_attrib_amd(b, .base = base);
+}
+
+static void
+store_hit_attrib_cb(nir_builder *b, nir_def *val, unsigned base)
+{
+   nir_store_hit_attrib_amd(b, val, .base = base);
+}
+
 bool
 radv_nir_lower_hit_attrib_derefs(nir_shader *shader)
 {
-   return radv_nir_lower_rt_vars(shader, nir_var_ray_hit_attrib, 0);
+   bool progress = false;
+   progress |= nir_lower_vars_to_explicit_types(shader, nir_var_ray_hit_attrib, glsl_get_natural_size_align_bytes);
+   progress |= radv_nir_lower_rt_vars(shader, nir_var_ray_hit_attrib, load_hit_attrib_cb, store_hit_attrib_cb, 0);
+   return progress;
+}
+
+static nir_def *
+load_incoming_payload_cb(nir_builder *b, unsigned base)
+{
+   return nir_load_incoming_ray_payload_amd(b, .base = base);
+}
+
+static void
+store_incoming_payload_cb(nir_builder *b, nir_def *val, unsigned base)
+{
+   nir_store_incoming_ray_payload_amd(b, val, .base = base);
+}
+
+static nir_def *
+load_outgoing_payload_cb(nir_builder *b, unsigned base)
+{
+   return nir_load_outgoing_ray_payload_amd(b, .base = base);
+}
+
+static void
+store_outgoing_payload_cb(nir_builder *b, nir_def *val, unsigned base)
+{
+   nir_store_outgoing_ray_payload_amd(b, val, .base = base);
 }
 
 bool
 radv_nir_lower_ray_payload_derefs(nir_shader *shader, uint32_t offset)
 {
-   bool progress = radv_nir_lower_rt_vars(shader, nir_var_function_temp, RADV_MAX_HIT_ATTRIB_SIZE + offset);
-   progress |= radv_nir_lower_rt_vars(shader, nir_var_shader_call_data, RADV_MAX_HIT_ATTRIB_SIZE + offset);
+   bool progress = radv_nir_lower_rt_vars(shader, nir_var_function_temp, load_outgoing_payload_cb,
+                                          store_outgoing_payload_cb, offset);
+   progress |= radv_nir_lower_rt_vars(shader, nir_var_shader_call_data, load_incoming_payload_cb,
+                                      store_incoming_payload_cb, offset);
    return progress;
 }
-- 
GitLab


From 01af7aedfcf2e90254eea1351007651c6aa592ee Mon Sep 17 00:00:00 2001
From: Friedrich Vock <friedrich.vock@gmx.de>
Date: Wed, 5 Jun 2024 11:46:28 +0200
Subject: [PATCH 11/71] radv/rt: Pass maximum payload size to
 radv_rt_nir_to_asm

---
 src/amd/vulkan/radv_pipeline_rt.c | 27 ++++++++++++++++++++++-----
 1 file changed, 22 insertions(+), 5 deletions(-)

diff --git a/src/amd/vulkan/radv_pipeline_rt.c b/src/amd/vulkan/radv_pipeline_rt.c
index b7c7b6ce2c036..0e957695501d7 100644
--- a/src/amd/vulkan/radv_pipeline_rt.c
+++ b/src/amd/vulkan/radv_pipeline_rt.c
@@ -344,7 +344,7 @@ move_rt_instructions(nir_shader *shader)
 static VkResult
 radv_rt_nir_to_asm(struct radv_device *device, struct vk_pipeline_cache *cache,
                    const VkRayTracingPipelineCreateInfoKHR *pCreateInfo, struct radv_ray_tracing_pipeline *pipeline,
-                   bool monolithic, struct radv_shader_stage *stage, uint32_t *stack_size,
+                   bool monolithic, struct radv_shader_stage *stage, uint32_t *payload_size, uint32_t *stack_size,
                    struct radv_ray_tracing_stage_info *stage_info,
                    const struct radv_ray_tracing_stage_info *traversal_stage_info,
                    struct radv_serialized_shader_arena_block *replay_block, struct radv_shader **out_shader)
@@ -354,7 +354,7 @@ radv_rt_nir_to_asm(struct radv_device *device, struct vk_pipeline_cache *cache,
    bool keep_executable_info = radv_pipeline_capture_shaders(device, pipeline->base.base.create_flags);
    bool keep_statistic_info = radv_pipeline_capture_shader_stats(device, pipeline->base.base.create_flags);
 
-   radv_nir_lower_rt_io(stage->nir, monolithic, 0);
+   radv_nir_lower_rt_io(stage->nir, monolithic, 0, payload_size);
 
    /* Gather shader info. */
    nir_shader_gather_info(stage->nir, nir_shader_get_entrypoint(stage->nir));
@@ -563,6 +563,10 @@ radv_rt_compile_shaders(struct radv_device *device, struct vk_pipeline_cache *ca
    if (!stages)
       return VK_ERROR_OUT_OF_HOST_MEMORY;
 
+   uint32_t payload_size = 0;
+   if (pCreateInfo->pLibraryInterface)
+      payload_size = pCreateInfo->pLibraryInterface->maxPipelineRayPayloadSize;
+
    bool library = pipeline->base.base.create_flags & VK_PIPELINE_CREATE_2_LIBRARY_BIT_KHR;
 
    bool monolithic = !library;
@@ -581,6 +585,19 @@ radv_rt_compile_shaders(struct radv_device *device, struct vk_pipeline_cache *ca
 
       NIR_PASS(_, stage->nir, radv_nir_lower_hit_attrib_derefs);
 
+      nir_foreach_variable_with_modes (var, stage->nir, nir_var_shader_call_data) {
+         unsigned size, alignment;
+         glsl_get_natural_size_align_bytes(var->type, &size, &alignment);
+         payload_size = MAX2(payload_size, size);
+      }
+      nir_foreach_function_impl (impl, stage->nir) {
+         nir_foreach_variable_in_list (var, &impl->locals) {
+            unsigned size, alignment;
+            glsl_get_natural_size_align_bytes(var->type, &size, &alignment);
+            payload_size = MAX2(payload_size, size);
+         }
+      }
+
       rt_stages[i].info = radv_gather_ray_tracing_stage_info(stage->nir);
 
       stage->feedback.duration = os_time_get_nano() - stage_start;
@@ -645,8 +662,8 @@ radv_rt_compile_shaders(struct radv_device *device, struct vk_pipeline_cache *ca
 
          bool monolithic_raygen = monolithic && stage->stage == MESA_SHADER_RAYGEN;
 
-         result = radv_rt_nir_to_asm(device, cache, pCreateInfo, pipeline, monolithic_raygen, stage, &stack_size,
-                                     &rt_stages[idx].info, NULL, replay_block, &rt_stages[idx].shader);
+         result = radv_rt_nir_to_asm(device, cache, pCreateInfo, pipeline, monolithic_raygen, stage, &payload_size,
+                                     &stack_size, &rt_stages[idx].info, NULL, replay_block, &rt_stages[idx].shader);
          if (result != VK_SUCCESS)
             goto cleanup;
 
@@ -703,7 +720,7 @@ radv_rt_compile_shaders(struct radv_device *device, struct vk_pipeline_cache *ca
       .key = stage_keys[MESA_SHADER_INTERSECTION],
    };
    radv_shader_layout_init(pipeline_layout, MESA_SHADER_INTERSECTION, &traversal_stage.layout);
-   result = radv_rt_nir_to_asm(device, cache, pCreateInfo, pipeline, false, &traversal_stage, NULL, NULL,
+   result = radv_rt_nir_to_asm(device, cache, pCreateInfo, pipeline, false, &traversal_stage, &payload_size, NULL, NULL,
                                &traversal_info, NULL, &pipeline->base.base.shaders[MESA_SHADER_INTERSECTION]);
    ralloc_free(traversal_nir);
 
-- 
GitLab


From b5f6586cf2263a869cb9040021a710be580bf0bc Mon Sep 17 00:00:00 2001
From: Friedrich Vock <friedrich.vock@gmx.de>
Date: Wed, 5 Jun 2024 11:47:46 +0200
Subject: [PATCH 12/71] radv/rt: Track traversal shader stack size

---
 src/amd/vulkan/radv_pipeline_rt.c | 14 ++++++++------
 src/amd/vulkan/radv_pipeline_rt.h |  1 +
 2 files changed, 9 insertions(+), 6 deletions(-)

diff --git a/src/amd/vulkan/radv_pipeline_rt.c b/src/amd/vulkan/radv_pipeline_rt.c
index 0e957695501d7..406a9b8f738c9 100644
--- a/src/amd/vulkan/radv_pipeline_rt.c
+++ b/src/amd/vulkan/radv_pipeline_rt.c
@@ -720,8 +720,9 @@ radv_rt_compile_shaders(struct radv_device *device, struct vk_pipeline_cache *ca
       .key = stage_keys[MESA_SHADER_INTERSECTION],
    };
    radv_shader_layout_init(pipeline_layout, MESA_SHADER_INTERSECTION, &traversal_stage.layout);
-   result = radv_rt_nir_to_asm(device, cache, pCreateInfo, pipeline, false, &traversal_stage, &payload_size, NULL, NULL,
-                               &traversal_info, NULL, &pipeline->base.base.shaders[MESA_SHADER_INTERSECTION]);
+   result = radv_rt_nir_to_asm(device, cache, pCreateInfo, pipeline, false, &traversal_stage, &payload_size,
+                               &pipeline->traversal_stack_size, NULL, &traversal_info, NULL,
+                               &pipeline->base.base.shaders[MESA_SHADER_INTERSECTION]);
    ralloc_free(traversal_nir);
 
 cleanup:
@@ -782,10 +783,11 @@ compute_rt_stack_size(const VkRayTracingPipelineCreateInfoKHR *pCreateInfo, stru
          unreachable("Invalid stage type in RT shader");
       }
    }
-   pipeline->stack_size =
-      raygen_size +
-      MIN2(pCreateInfo->maxPipelineRayRecursionDepth, 1) * MAX2(chit_miss_size, intersection_size + any_hit_size) +
-      MAX2(0, (int)(pCreateInfo->maxPipelineRayRecursionDepth) - 1) * chit_miss_size + 2 * callable_size;
+   pipeline->stack_size = raygen_size +
+                          MIN2(pCreateInfo->maxPipelineRayRecursionDepth, 1) *
+                             (chit_miss_size + intersection_size + any_hit_size + pipeline->traversal_stack_size) +
+                          MAX2(0, (int)(pCreateInfo->maxPipelineRayRecursionDepth) - 1) * chit_miss_size +
+                          2 * callable_size;
 }
 
 static void
diff --git a/src/amd/vulkan/radv_pipeline_rt.h b/src/amd/vulkan/radv_pipeline_rt.h
index baab7aacd9998..65dbc93044486 100644
--- a/src/amd/vulkan/radv_pipeline_rt.h
+++ b/src/amd/vulkan/radv_pipeline_rt.h
@@ -26,6 +26,7 @@ struct radv_ray_tracing_pipeline {
    unsigned group_count;
 
    uint32_t stack_size;
+   uint32_t traversal_stack_size;
 
    /* set if any shaders from this pipeline require robustness2 in the merged traversal shader */
    bool traversal_storage_robustness2 : 1;
-- 
GitLab


From e2ccd4a967bfac96df3df75ee01e4f46747a1195 Mon Sep 17 00:00:00 2001
From: Friedrich Vock <friedrich.vock@gmx.de>
Date: Wed, 5 Jun 2024 11:48:48 +0200
Subject: [PATCH 13/71] radv/rt: Set stack size to scratch_bytes_per_wave

---
 src/amd/vulkan/radv_pipeline_rt.c | 3 +++
 1 file changed, 3 insertions(+)

diff --git a/src/amd/vulkan/radv_pipeline_rt.c b/src/amd/vulkan/radv_pipeline_rt.c
index 406a9b8f738c9..8c1ea937f8b43 100644
--- a/src/amd/vulkan/radv_pipeline_rt.c
+++ b/src/amd/vulkan/radv_pipeline_rt.c
@@ -440,6 +440,9 @@ radv_rt_nir_to_asm(struct radv_device *device, struct vk_pipeline_cache *cache,
       shader = radv_shader_create(device, cache, binary, keep_executable_info || dump_shader);
 
    if (shader) {
+      if (stack_size)
+         *stack_size += DIV_ROUND_UP(shader->config.scratch_bytes_per_wave, shader->info.wave_size);
+
       radv_shader_generate_debug_info(device, dump_shader, keep_executable_info, binary, shader, shaders, num_shaders,
                                       &stage->info);
 
-- 
GitLab


From a448a1e13a1760e4a9cfab1ea282644688f2f8f7 Mon Sep 17 00:00:00 2001
From: Friedrich Vock <friedrich.vock@gmx.de>
Date: Wed, 5 Jun 2024 12:15:20 +0200
Subject: [PATCH 14/71] radv/rt: Use radv_get_rt_shader_entrypoint instead of
 nir_shader_get_entrypoint

---
 src/amd/vulkan/nir/radv_nir_rt_shader.c | 2 +-
 src/amd/vulkan/radv_pipeline_rt.c       | 2 +-
 src/amd/vulkan/radv_shader.h            | 9 +++++++++
 3 files changed, 11 insertions(+), 2 deletions(-)

diff --git a/src/amd/vulkan/nir/radv_nir_rt_shader.c b/src/amd/vulkan/nir/radv_nir_rt_shader.c
index cb8c4b436f836..10131fe786fa6 100644
--- a/src/amd/vulkan/nir/radv_nir_rt_shader.c
+++ b/src/amd/vulkan/nir/radv_nir_rt_shader.c
@@ -1650,7 +1650,7 @@ radv_build_traversal(struct radv_device *device, struct radv_ray_tracing_pipelin
       radv_build_end_trace_token(b, vars, original_tmax, nir_load_var(b, trav_vars.hit),
                                  nir_load_var(b, iteration_instance_count));
 
-   nir_metadata_preserve(nir_shader_get_entrypoint(b->shader), nir_metadata_none);
+   nir_metadata_preserve(radv_get_rt_shader_entrypoint(b->shader), nir_metadata_none);
    radv_nir_lower_hit_attrib_derefs(b->shader);
 
    /* Register storage for hit attributes */
diff --git a/src/amd/vulkan/radv_pipeline_rt.c b/src/amd/vulkan/radv_pipeline_rt.c
index 8c1ea937f8b43..6757979b8dc3a 100644
--- a/src/amd/vulkan/radv_pipeline_rt.c
+++ b/src/amd/vulkan/radv_pipeline_rt.c
@@ -410,7 +410,7 @@ radv_rt_nir_to_asm(struct radv_device *device, struct vk_pipeline_cache *cache,
                             pipeline, monolithic, traversal_stage_info);
 
       /* Info might be out-of-date after inlining in radv_nir_lower_rt_abi(). */
-      nir_shader_gather_info(temp_stage.nir, nir_shader_get_entrypoint(temp_stage.nir));
+      nir_shader_gather_info(temp_stage.nir, radv_get_rt_shader_entrypoint(temp_stage.nir));
 
       radv_optimize_nir(temp_stage.nir, stage->key.optimisations_disabled);
       radv_postprocess_nir(device, NULL, &temp_stage);
diff --git a/src/amd/vulkan/radv_shader.h b/src/amd/vulkan/radv_shader.h
index c9b471ce79fbb..049b9d5d5575d 100644
--- a/src/amd/vulkan/radv_shader.h
+++ b/src/amd/vulkan/radv_shader.h
@@ -494,6 +494,15 @@ struct radv_shader_stage;
 void radv_optimize_nir(struct nir_shader *shader, bool optimize_conservatively);
 void radv_optimize_nir_algebraic(nir_shader *shader, bool opt_offsets, bool opt_mqsad);
 
+static inline nir_function_impl *
+radv_get_rt_shader_entrypoint(nir_shader *shader)
+{
+   nir_foreach_function_impl (impl, shader)
+      if (impl->function->is_entrypoint || impl->function->is_exported)
+         return impl;
+   return NULL;
+}
+
 void radv_nir_lower_rt_io(nir_shader *shader, bool monolithic, uint32_t payload_offset);
 
 struct radv_ray_tracing_stage_info;
-- 
GitLab


From 514e138d28cb33213410423931be9c2e2d0a2815 Mon Sep 17 00:00:00 2001
From: Friedrich Vock <friedrich.vock@gmx.de>
Date: Wed, 5 Jun 2024 11:54:05 +0200
Subject: [PATCH 15/71] radv/rt: Lower monolithic ray payload load/store
 instructions

---
 src/amd/vulkan/nir/radv_nir_rt_shader.c | 98 +++++++++++++++++--------
 src/amd/vulkan/radv_shader.h            |  2 +-
 2 files changed, 69 insertions(+), 31 deletions(-)

diff --git a/src/amd/vulkan/nir/radv_nir_rt_shader.c b/src/amd/vulkan/nir/radv_nir_rt_shader.c
index 10131fe786fa6..8d0bbe449ad94 100644
--- a/src/amd/vulkan/nir/radv_nir_rt_shader.c
+++ b/src/amd/vulkan/nir/radv_nir_rt_shader.c
@@ -723,12 +723,13 @@ lower_rt_instructions(nir_shader *shader, struct rt_variables *vars, bool late_l
    nir_shader_instructions_pass(shader, radv_lower_rt_instruction, nir_metadata_none, &data);
 }
 
-/* Lowers hit attributes to registers or shared memory. If hit_attribs is NULL, attributes are
+/* Lowers RT I/O vars to registers or shared memory. If hit_attribs is NULL, attributes are
  * lowered to shared memory. */
 static void
-lower_hit_attribs(nir_shader *shader, nir_variable **hit_attribs, uint32_t workgroup_size)
+lower_rt_storage(nir_shader *shader, nir_variable **hit_attribs, nir_deref_instr **payload_in,
+                 nir_variable **payload_out, uint32_t workgroup_size)
 {
-   nir_function_impl *impl = nir_shader_get_entrypoint(shader);
+   nir_function_impl *impl = radv_get_rt_shader_entrypoint(shader);
 
    nir_foreach_variable_with_modes (attrib, shader, nir_var_ray_hit_attrib)
       attrib->data.mode = nir_var_shader_temp;
@@ -742,29 +743,55 @@ lower_hit_attribs(nir_shader *shader, nir_variable **hit_attribs, uint32_t workg
 
          nir_intrinsic_instr *intrin = nir_instr_as_intrinsic(instr);
          if (intrin->intrinsic != nir_intrinsic_load_hit_attrib_amd &&
-             intrin->intrinsic != nir_intrinsic_store_hit_attrib_amd)
+             intrin->intrinsic != nir_intrinsic_store_hit_attrib_amd &&
+             intrin->intrinsic != nir_intrinsic_load_incoming_ray_payload_amd &&
+             intrin->intrinsic != nir_intrinsic_store_incoming_ray_payload_amd &&
+             intrin->intrinsic != nir_intrinsic_load_outgoing_ray_payload_amd &&
+             intrin->intrinsic != nir_intrinsic_store_outgoing_ray_payload_amd)
             continue;
 
          b.cursor = nir_after_instr(instr);
 
-         nir_def *offset;
-         if (!hit_attribs)
-            offset = nir_imul_imm(
-               &b, nir_iadd_imm(&b, nir_load_local_invocation_index(&b), nir_intrinsic_base(intrin) * workgroup_size),
-               sizeof(uint32_t));
-
-         if (intrin->intrinsic == nir_intrinsic_load_hit_attrib_amd) {
-            nir_def *ret;
-            if (hit_attribs)
-               ret = nir_load_var(&b, hit_attribs[nir_intrinsic_base(intrin)]);
+         if (intrin->intrinsic == nir_intrinsic_load_hit_attrib_amd ||
+             intrin->intrinsic == nir_intrinsic_store_hit_attrib_amd) {
+            nir_def *offset;
+            if (!hit_attribs)
+               offset = nir_imul_imm(
+                  &b,
+                  nir_iadd_imm(&b, nir_load_local_invocation_index(&b), nir_intrinsic_base(intrin) * workgroup_size),
+                  sizeof(uint32_t));
+
+            if (intrin->intrinsic == nir_intrinsic_load_hit_attrib_amd) {
+               nir_def *ret;
+               if (hit_attribs)
+                  ret = nir_load_var(&b, hit_attribs[nir_intrinsic_base(intrin)]);
+               else
+                  ret = nir_load_shared(&b, 1, 32, offset, .base = 0, .align_mul = 4);
+               nir_def_rewrite_uses(nir_instr_def(instr), ret);
+            } else {
+               if (hit_attribs)
+                  nir_store_var(&b, hit_attribs[nir_intrinsic_base(intrin)], intrin->src->ssa, 0x1);
+               else
+                  nir_store_shared(&b, intrin->src->ssa, offset, .base = 0, .align_mul = 4);
+            }
+         } else if (intrin->intrinsic == nir_intrinsic_load_incoming_ray_payload_amd ||
+                    intrin->intrinsic == nir_intrinsic_store_incoming_ray_payload_amd) {
+            if (!payload_in)
+               continue;
+            if (intrin->intrinsic == nir_intrinsic_load_incoming_ray_payload_amd)
+               nir_def_rewrite_uses(nir_instr_def(instr), nir_load_deref(&b, payload_in[nir_intrinsic_base(intrin)]));
             else
-               ret = nir_load_shared(&b, 1, 32, offset, .base = 0, .align_mul = 4);
-            nir_def_rewrite_uses(nir_instr_def(instr), ret);
-         } else {
-            if (hit_attribs)
-               nir_store_var(&b, hit_attribs[nir_intrinsic_base(intrin)], intrin->src->ssa, 0x1);
+               nir_store_deref(&b, payload_in[nir_intrinsic_base(intrin)], intrin->src->ssa, 0x1);
+         } else if (intrin->intrinsic == nir_intrinsic_load_outgoing_ray_payload_amd ||
+                    intrin->intrinsic == nir_intrinsic_store_outgoing_ray_payload_amd) {
+            if (!payload_out)
+               continue;
+            if (intrin->intrinsic == nir_intrinsic_load_outgoing_ray_payload_amd)
+               nir_def_rewrite_uses(nir_instr_def(instr), nir_load_var(&b, payload_out[nir_intrinsic_base(intrin)]));
             else
-               nir_store_shared(&b, intrin->src->ssa, offset, .base = 0, .align_mul = 4);
+               nir_store_var(&b, payload_out[nir_intrinsic_base(intrin)], intrin->src->ssa, 0x1);
+         } else {
+            continue;
          }
          nir_instr_remove(instr);
       }
@@ -1658,10 +1685,9 @@ radv_build_traversal(struct radv_device *device, struct radv_ray_tracing_pipelin
 
    if (!monolithic) {
       for (uint32_t i = 0; i < ARRAY_SIZE(hit_attribs); i++)
-         hit_attribs[i] =
-            nir_local_variable_create(nir_shader_get_entrypoint(b->shader), glsl_uint_type(), "ahit_attrib");
+         hit_attribs[i] = nir_variable_create(b->shader, nir_var_shader_temp, glsl_uint_type(), "ahit_attrib");
 
-      lower_hit_attribs(b->shader, hit_attribs, pdev->rt_wave_size);
+      lower_rt_storage(b->shader, hit_attribs, NULL, NULL, pdev->rt_wave_size);
    }
 
    /* Initialize follow-up shader. */
@@ -1857,10 +1883,11 @@ radv_count_hit_attrib_slots(nir_builder *b, nir_intrinsic_instr *instr, void *da
 
 static void
 lower_rt_instructions_monolithic(nir_shader *shader, struct radv_device *device,
-                                 struct radv_ray_tracing_pipeline *pipeline,
-                                 const VkRayTracingPipelineCreateInfoKHR *pCreateInfo, struct rt_variables *vars)
+                                 struct radv_ray_tracing_pipeline *pipeline, const struct radv_shader_info *info,
+                                 const VkRayTracingPipelineCreateInfoKHR *pCreateInfo, uint32_t payload_size,
+                                 struct rt_variables *vars)
 {
-   nir_function_impl *impl = nir_shader_get_entrypoint(shader);
+   nir_function_impl *impl = radv_get_rt_shader_entrypoint(shader);
 
    struct lower_rt_instruction_monolithic_state state = {
       .device = device,
@@ -1880,7 +1907,17 @@ lower_rt_instructions_monolithic(nir_shader *shader, struct radv_device *device,
    for (uint32_t i = 0; i < hit_attrib_count; i++)
       hit_attribs[i] = nir_local_variable_create(impl, glsl_uint_type(), "ahit_attrib");
 
-   lower_hit_attribs(shader, hit_attribs, 0);
+   nir_builder b = nir_builder_create(impl);
+   b.cursor = nir_before_impl(impl);
+   nir_variable **payload_vars = rzalloc_array_size(shader, sizeof(nir_variable *), DIV_ROUND_UP(payload_size, 4));
+   nir_deref_instr **payload_storage =
+      rzalloc_array_size(shader, sizeof(nir_variable *), DIV_ROUND_UP(payload_size, 4));
+   for (unsigned i = 0; i < DIV_ROUND_UP(payload_size, 4); ++i) {
+      payload_vars[i] = nir_variable_create(shader, nir_var_shader_temp, glsl_uint_type(), "_payload");
+      payload_storage[i] = nir_build_deref_var(&b, payload_vars[i]);
+   }
+
+   lower_rt_storage(shader, hit_attribs, payload_storage, payload_vars, info->wave_size);
 }
 
 static void
@@ -1895,8 +1932,9 @@ radv_store_arg(nir_builder *b, const struct radv_shader_args *args, const struct
 void
 radv_nir_lower_rt_abi(nir_shader *shader, const VkRayTracingPipelineCreateInfoKHR *pCreateInfo,
                       const struct radv_shader_args *args, const struct radv_shader_info *info, uint32_t *stack_size,
-                      bool resume_shader, struct radv_device *device, struct radv_ray_tracing_pipeline *pipeline,
-                      bool monolithic, const struct radv_ray_tracing_stage_info *traversal_info)
+                      bool resume_shader, uint32_t payload_size, struct radv_device *device,
+                      struct radv_ray_tracing_pipeline *pipeline, bool monolithic,
+                      const struct radv_ray_tracing_stage_info *traversal_info)
 {
    nir_function_impl *impl = nir_shader_get_entrypoint(shader);
 
@@ -1905,7 +1943,7 @@ radv_nir_lower_rt_abi(nir_shader *shader, const VkRayTracingPipelineCreateInfoKH
    struct rt_variables vars = create_rt_variables(shader, device, create_flags, monolithic);
 
    if (monolithic)
-      lower_rt_instructions_monolithic(shader, device, pipeline, pCreateInfo, &vars);
+      lower_rt_instructions_monolithic(shader, device, pipeline, info, pCreateInfo, payload_size, &vars);
 
    struct radv_rt_shader_info rt_info = {0};
 
diff --git a/src/amd/vulkan/radv_shader.h b/src/amd/vulkan/radv_shader.h
index 049b9d5d5575d..60489905ad2cb 100644
--- a/src/amd/vulkan/radv_shader.h
+++ b/src/amd/vulkan/radv_shader.h
@@ -509,7 +509,7 @@ struct radv_ray_tracing_stage_info;
 
 void radv_nir_lower_rt_abi(nir_shader *shader, const VkRayTracingPipelineCreateInfoKHR *pCreateInfo,
                            const struct radv_shader_args *args, const struct radv_shader_info *info,
-                           uint32_t *stack_size, bool resume_shader, struct radv_device *device,
+                           uint32_t *stack_size, bool resume_shader, uint32_t payload_size, struct radv_device *device,
                            struct radv_ray_tracing_pipeline *pipeline, bool monolithic,
                            const struct radv_ray_tracing_stage_info *traversal_info);
 
-- 
GitLab


From de1cecb5d5c533162fb1328609cef08b7f33ccb9 Mon Sep 17 00:00:00 2001
From: Friedrich Vock <friedrich.vock@gmx.de>
Date: Wed, 5 Jun 2024 11:56:09 +0200
Subject: [PATCH 16/71] radv/rt: Create RT functions to call

---
 src/amd/vulkan/nir/radv_nir_rt_shader.c | 249 +++++++++++++++++++++++-
 1 file changed, 245 insertions(+), 4 deletions(-)

diff --git a/src/amd/vulkan/nir/radv_nir_rt_shader.c b/src/amd/vulkan/nir/radv_nir_rt_shader.c
index 8d0bbe449ad94..23c49ba4f3198 100644
--- a/src/amd/vulkan/nir/radv_nir_rt_shader.c
+++ b/src/amd/vulkan/nir/radv_nir_rt_shader.c
@@ -165,6 +165,222 @@ lower_rt_derefs(nir_shader *shader)
    return progress;
 }
 
+enum radv_nir_raygen_function_arg {
+   RAYGEN_ARG_SHADER_RECORD_PTR = 0,
+   RAYGEN_ARG_COUNT,
+};
+
+enum radv_nir_traversal_function_arg {
+   TRAVERSAL_ARG_SHADER_RECORD_PTR = 0,
+   TRAVERSAL_ARG_ACCEL_STRUCT,
+   TRAVERSAL_ARG_CULL_MASK_AND_FLAGS,
+   TRAVERSAL_ARG_SBT_OFFSET,
+   TRAVERSAL_ARG_SBT_STRIDE,
+   TRAVERSAL_ARG_MISS_INDEX,
+   TRAVERSAL_ARG_RAY_ORIGIN,
+   TRAVERSAL_ARG_RAY_TMIN,
+   TRAVERSAL_ARG_RAY_DIRECTION,
+   TRAVERSAL_ARG_RAY_TMAX,
+   TRAVERSAL_ARG_PRIMITIVE_ID,
+   TRAVERSAL_ARG_INSTANCE_ADDR,
+   TRAVERSAL_ARG_GEOMETRY_ID_AND_FLAGS,
+   TRAVERSAL_ARG_HIT_KIND,
+   TRAVERSAL_ARG_PAYLOAD_BASE,
+};
+
+enum radv_nir_chit_miss_function_arg {
+   CHIT_MISS_ARG_SHADER_RECORD_PTR = 0,
+   CHIT_MISS_ARG_ACCEL_STRUCT,
+   CHIT_MISS_ARG_CULL_MASK_AND_FLAGS,
+   CHIT_MISS_ARG_SBT_OFFSET,
+   CHIT_MISS_ARG_SBT_STRIDE,
+   CHIT_MISS_ARG_MISS_INDEX,
+   CHIT_MISS_ARG_RAY_ORIGIN,
+   CHIT_MISS_ARG_RAY_TMIN,
+   CHIT_MISS_ARG_RAY_DIRECTION,
+   CHIT_MISS_ARG_RAY_TMAX,
+   CHIT_MISS_ARG_PRIMITIVE_ID,
+   CHIT_MISS_ARG_INSTANCE_ADDR,
+   CHIT_MISS_ARG_GEOMETRY_ID_AND_FLAGS,
+   CHIT_MISS_ARG_HIT_KIND,
+   CHIT_MISS_ARG_PAYLOAD_BASE,
+};
+
+static void
+radv_nir_init_function_params(nir_function *function, gl_shader_stage stage, unsigned payload_size)
+{
+   unsigned payload_base = -1u;
+
+   switch (stage) {
+   case MESA_SHADER_RAYGEN:
+      function->num_params = RAYGEN_ARG_COUNT;
+      function->params = rzalloc_array_size(function->shader, sizeof(nir_parameter), function->num_params);
+      function->params[RAYGEN_ARG_SHADER_RECORD_PTR].num_components = 1;
+      function->params[RAYGEN_ARG_SHADER_RECORD_PTR].bit_size = 64;
+      function->params[RAYGEN_ARG_SHADER_RECORD_PTR].is_divergent = true;
+      function->params[RAYGEN_ARG_SHADER_RECORD_PTR].type = glsl_uint64_t_type();
+      function->uniform_call = true;
+      function->noreturn = true;
+      break;
+   case MESA_SHADER_CALLABLE:
+      function->num_params = RAYGEN_ARG_COUNT + DIV_ROUND_UP(payload_size, 4);
+      function->params = rzalloc_array_size(function->shader, sizeof(nir_parameter), function->num_params);
+      function->params[RAYGEN_ARG_SHADER_RECORD_PTR].num_components = 1;
+      function->params[RAYGEN_ARG_SHADER_RECORD_PTR].bit_size = 64;
+      function->params[RAYGEN_ARG_SHADER_RECORD_PTR].is_divergent = true;
+      function->params[RAYGEN_ARG_SHADER_RECORD_PTR].type = glsl_uint64_t_type();
+
+      payload_base = RAYGEN_ARG_COUNT;
+      break;
+   case MESA_SHADER_INTERSECTION:
+      function->num_params = TRAVERSAL_ARG_PAYLOAD_BASE + DIV_ROUND_UP(payload_size, 4);
+      function->params = rzalloc_array_size(function->shader, sizeof(nir_parameter), function->num_params);
+      function->params[TRAVERSAL_ARG_SHADER_RECORD_PTR].num_components = 1;
+      function->params[TRAVERSAL_ARG_SHADER_RECORD_PTR].bit_size = 64;
+      function->params[TRAVERSAL_ARG_SHADER_RECORD_PTR].is_divergent = true;
+      function->params[TRAVERSAL_ARG_SHADER_RECORD_PTR].type = glsl_uint64_t_type();
+      function->params[TRAVERSAL_ARG_ACCEL_STRUCT].num_components = 1;
+      function->params[TRAVERSAL_ARG_ACCEL_STRUCT].bit_size = 64;
+      function->params[TRAVERSAL_ARG_ACCEL_STRUCT].is_divergent = true;
+      function->params[TRAVERSAL_ARG_ACCEL_STRUCT].type = glsl_uint64_t_type();
+      function->params[TRAVERSAL_ARG_CULL_MASK_AND_FLAGS].num_components = 1;
+      function->params[TRAVERSAL_ARG_CULL_MASK_AND_FLAGS].bit_size = 32;
+      function->params[TRAVERSAL_ARG_CULL_MASK_AND_FLAGS].is_divergent = true;
+      function->params[TRAVERSAL_ARG_CULL_MASK_AND_FLAGS].type = glsl_uint_type();
+      function->params[TRAVERSAL_ARG_SBT_OFFSET].num_components = 1;
+      function->params[TRAVERSAL_ARG_SBT_OFFSET].bit_size = 32;
+      function->params[TRAVERSAL_ARG_SBT_OFFSET].is_divergent = true;
+      function->params[TRAVERSAL_ARG_SBT_OFFSET].type = glsl_uint_type();
+      function->params[TRAVERSAL_ARG_SBT_STRIDE].num_components = 1;
+      function->params[TRAVERSAL_ARG_SBT_STRIDE].bit_size = 32;
+      function->params[TRAVERSAL_ARG_SBT_STRIDE].is_divergent = true;
+      function->params[TRAVERSAL_ARG_SBT_STRIDE].type = glsl_uint_type();
+      function->params[TRAVERSAL_ARG_MISS_INDEX].num_components = 1;
+      function->params[TRAVERSAL_ARG_MISS_INDEX].bit_size = 32;
+      function->params[TRAVERSAL_ARG_MISS_INDEX].is_divergent = true;
+      function->params[TRAVERSAL_ARG_MISS_INDEX].type = glsl_uint_type();
+      function->params[TRAVERSAL_ARG_RAY_ORIGIN].num_components = 3;
+      function->params[TRAVERSAL_ARG_RAY_ORIGIN].bit_size = 32;
+      function->params[TRAVERSAL_ARG_RAY_ORIGIN].is_divergent = true;
+      function->params[TRAVERSAL_ARG_RAY_ORIGIN].type = glsl_vector_type(GLSL_TYPE_FLOAT, 3);
+      function->params[TRAVERSAL_ARG_RAY_TMIN].num_components = 1;
+      function->params[TRAVERSAL_ARG_RAY_TMIN].bit_size = 32;
+      function->params[TRAVERSAL_ARG_RAY_TMIN].is_divergent = true;
+      function->params[TRAVERSAL_ARG_RAY_TMIN].type = glsl_float_type();
+      function->params[TRAVERSAL_ARG_RAY_DIRECTION].num_components = 3;
+      function->params[TRAVERSAL_ARG_RAY_DIRECTION].bit_size = 32;
+      function->params[TRAVERSAL_ARG_RAY_DIRECTION].is_divergent = true;
+      function->params[TRAVERSAL_ARG_RAY_DIRECTION].type = glsl_vector_type(GLSL_TYPE_FLOAT, 3);
+      function->params[TRAVERSAL_ARG_RAY_TMAX].num_components = 1;
+      function->params[TRAVERSAL_ARG_RAY_TMAX].bit_size = 32;
+      function->params[TRAVERSAL_ARG_RAY_TMAX].is_divergent = true;
+      function->params[TRAVERSAL_ARG_RAY_TMAX].type = glsl_float_type();
+      function->params[TRAVERSAL_ARG_PRIMITIVE_ID].num_components = 1;
+      function->params[TRAVERSAL_ARG_PRIMITIVE_ID].bit_size = 32;
+      function->params[TRAVERSAL_ARG_PRIMITIVE_ID].is_divergent = true;
+      function->params[TRAVERSAL_ARG_PRIMITIVE_ID].type = glsl_uint_type();
+      function->params[TRAVERSAL_ARG_INSTANCE_ADDR].num_components = 1;
+      function->params[TRAVERSAL_ARG_INSTANCE_ADDR].bit_size = 64;
+      function->params[TRAVERSAL_ARG_INSTANCE_ADDR].is_divergent = true;
+      function->params[TRAVERSAL_ARG_INSTANCE_ADDR].type = glsl_uint64_t_type();
+      function->params[TRAVERSAL_ARG_GEOMETRY_ID_AND_FLAGS].num_components = 1;
+      function->params[TRAVERSAL_ARG_GEOMETRY_ID_AND_FLAGS].bit_size = 32;
+      function->params[TRAVERSAL_ARG_GEOMETRY_ID_AND_FLAGS].is_divergent = true;
+      function->params[TRAVERSAL_ARG_GEOMETRY_ID_AND_FLAGS].type = glsl_uint_type();
+      function->params[TRAVERSAL_ARG_HIT_KIND].num_components = 1;
+      function->params[TRAVERSAL_ARG_HIT_KIND].bit_size = 32;
+      function->params[TRAVERSAL_ARG_HIT_KIND].is_divergent = true;
+      function->params[TRAVERSAL_ARG_HIT_KIND].type = glsl_uint_type();
+
+      payload_base = TRAVERSAL_ARG_PAYLOAD_BASE;
+      function->uniform_call = true;
+      break;
+   case MESA_SHADER_CLOSEST_HIT:
+   case MESA_SHADER_MISS:
+      function->num_params = 14 + DIV_ROUND_UP(payload_size, 4);
+      function->params = rzalloc_array_size(function->shader, sizeof(nir_parameter), function->num_params);
+      function->params[CHIT_MISS_ARG_SHADER_RECORD_PTR].num_components = 1;
+      function->params[CHIT_MISS_ARG_SHADER_RECORD_PTR].bit_size = 64;
+      function->params[CHIT_MISS_ARG_SHADER_RECORD_PTR].is_divergent = true;
+      function->params[CHIT_MISS_ARG_SHADER_RECORD_PTR].type = glsl_uint64_t_type();
+      function->params[CHIT_MISS_ARG_ACCEL_STRUCT].num_components = 1;
+      function->params[CHIT_MISS_ARG_ACCEL_STRUCT].bit_size = 64;
+      function->params[CHIT_MISS_ARG_ACCEL_STRUCT].is_divergent = true;
+      function->params[CHIT_MISS_ARG_ACCEL_STRUCT].discardable = true;
+      function->params[CHIT_MISS_ARG_ACCEL_STRUCT].type = glsl_uint64_t_type();
+      function->params[CHIT_MISS_ARG_CULL_MASK_AND_FLAGS].num_components = 1;
+      function->params[CHIT_MISS_ARG_CULL_MASK_AND_FLAGS].bit_size = 32;
+      function->params[CHIT_MISS_ARG_CULL_MASK_AND_FLAGS].is_divergent = true;
+      function->params[CHIT_MISS_ARG_CULL_MASK_AND_FLAGS].type = glsl_uint_type();
+      function->params[CHIT_MISS_ARG_SBT_OFFSET].num_components = 1;
+      function->params[CHIT_MISS_ARG_SBT_OFFSET].bit_size = 32;
+      function->params[CHIT_MISS_ARG_SBT_OFFSET].is_divergent = true;
+      function->params[CHIT_MISS_ARG_SBT_OFFSET].discardable = true;
+      function->params[CHIT_MISS_ARG_SBT_OFFSET].type = glsl_uint_type();
+      function->params[CHIT_MISS_ARG_SBT_STRIDE].num_components = 1;
+      function->params[CHIT_MISS_ARG_SBT_STRIDE].bit_size = 32;
+      function->params[CHIT_MISS_ARG_SBT_STRIDE].is_divergent = true;
+      function->params[CHIT_MISS_ARG_SBT_STRIDE].discardable = true;
+      function->params[CHIT_MISS_ARG_SBT_STRIDE].type = glsl_uint_type();
+      function->params[CHIT_MISS_ARG_MISS_INDEX].num_components = 1;
+      function->params[CHIT_MISS_ARG_MISS_INDEX].bit_size = 32;
+      function->params[CHIT_MISS_ARG_MISS_INDEX].is_divergent = true;
+      function->params[CHIT_MISS_ARG_MISS_INDEX].discardable = true;
+      function->params[CHIT_MISS_ARG_MISS_INDEX].type = glsl_uint_type();
+      function->params[CHIT_MISS_ARG_RAY_ORIGIN].num_components = 3;
+      function->params[CHIT_MISS_ARG_RAY_ORIGIN].bit_size = 32;
+      function->params[CHIT_MISS_ARG_RAY_ORIGIN].is_divergent = true;
+      function->params[CHIT_MISS_ARG_RAY_ORIGIN].type = glsl_vector_type(GLSL_TYPE_FLOAT, 3);
+      function->params[CHIT_MISS_ARG_RAY_TMIN].num_components = 1;
+      function->params[CHIT_MISS_ARG_RAY_TMIN].bit_size = 32;
+      function->params[CHIT_MISS_ARG_RAY_TMIN].is_divergent = true;
+      function->params[CHIT_MISS_ARG_RAY_TMIN].type = glsl_float_type();
+      function->params[CHIT_MISS_ARG_RAY_DIRECTION].num_components = 3;
+      function->params[CHIT_MISS_ARG_RAY_DIRECTION].bit_size = 32;
+      function->params[CHIT_MISS_ARG_RAY_DIRECTION].is_divergent = true;
+      function->params[CHIT_MISS_ARG_RAY_DIRECTION].type = glsl_vector_type(GLSL_TYPE_FLOAT, 3);
+      function->params[CHIT_MISS_ARG_RAY_TMAX].num_components = 1;
+      function->params[CHIT_MISS_ARG_RAY_TMAX].bit_size = 32;
+      function->params[CHIT_MISS_ARG_RAY_TMAX].is_divergent = true;
+      function->params[CHIT_MISS_ARG_RAY_TMAX].type = glsl_float_type();
+      function->params[CHIT_MISS_ARG_PRIMITIVE_ID].num_components = 1;
+      function->params[CHIT_MISS_ARG_PRIMITIVE_ID].bit_size = 32;
+      function->params[CHIT_MISS_ARG_PRIMITIVE_ID].is_divergent = true;
+      function->params[CHIT_MISS_ARG_PRIMITIVE_ID].type = glsl_uint_type();
+      function->params[CHIT_MISS_ARG_INSTANCE_ADDR].num_components = 1;
+      function->params[CHIT_MISS_ARG_INSTANCE_ADDR].bit_size = 64;
+      function->params[CHIT_MISS_ARG_INSTANCE_ADDR].is_divergent = true;
+      function->params[CHIT_MISS_ARG_INSTANCE_ADDR].type = glsl_uint64_t_type();
+      function->params[CHIT_MISS_ARG_GEOMETRY_ID_AND_FLAGS].num_components = 1;
+      function->params[CHIT_MISS_ARG_GEOMETRY_ID_AND_FLAGS].bit_size = 32;
+      function->params[CHIT_MISS_ARG_GEOMETRY_ID_AND_FLAGS].is_divergent = true;
+      function->params[CHIT_MISS_ARG_GEOMETRY_ID_AND_FLAGS].type = glsl_uint_type();
+      function->params[CHIT_MISS_ARG_HIT_KIND].num_components = 1;
+      function->params[CHIT_MISS_ARG_HIT_KIND].bit_size = 32;
+      function->params[CHIT_MISS_ARG_HIT_KIND].is_divergent = true;
+      function->params[CHIT_MISS_ARG_HIT_KIND].type = glsl_uint_type();
+
+      payload_base = CHIT_MISS_ARG_PAYLOAD_BASE;
+      break;
+   default:
+      unreachable("invalid RT stage");
+   }
+
+   if (payload_base != -1u) {
+      for (unsigned i = 0; i < DIV_ROUND_UP(payload_size, 4); ++i) {
+         function->params[payload_base + i].num_components = 1;
+         function->params[payload_base + i].bit_size = 32;
+         function->params[payload_base + i].is_divergent = true;
+         function->params[payload_base + i].is_return = true;
+         function->params[payload_base + i].type = glsl_uint_type();
+      }
+   }
+
+   /* Entrypoints can't have parameters. Consider RT stages as callable functions */
+   function->is_exported = true;
+   function->is_entrypoint = false;
+}
+
 /*
  * Global variables for an RT pipeline
  */
@@ -217,12 +433,19 @@ struct rt_variables {
    nir_variable *ahit_accept;
    nir_variable *ahit_terminate;
 
+   nir_variable **out_payload_storage;
+   unsigned payload_size;
+
+   nir_function *trace_ray_func;
+   nir_function *chit_miss_func;
+   nir_function *callable_func;
+
    unsigned stack_size;
 };
 
 static struct rt_variables
 create_rt_variables(nir_shader *shader, struct radv_device *device, const VkPipelineCreateFlags2KHR flags,
-                    bool monolithic)
+                    unsigned max_payload_size, bool monolithic)
 {
    struct rt_variables vars = {
       .device = device,
@@ -268,6 +491,23 @@ create_rt_variables(nir_shader *shader, struct radv_device *device, const VkPipe
    vars.ahit_accept = nir_variable_create(shader, nir_var_shader_temp, glsl_bool_type(), "ahit_accept");
    vars.ahit_terminate = nir_variable_create(shader, nir_var_shader_temp, glsl_bool_type(), "ahit_terminate");
 
+   if (max_payload_size)
+      vars.out_payload_storage = rzalloc_array_size(shader, DIV_ROUND_UP(max_payload_size, 4), sizeof(nir_variable *));
+   vars.payload_size = max_payload_size;
+   for (unsigned i = 0; i < DIV_ROUND_UP(max_payload_size, 4); ++i) {
+      vars.out_payload_storage[i] =
+         nir_variable_create(shader, nir_var_shader_temp, glsl_uint_type(), "out_payload_storage");
+   }
+
+   nir_function *trace_ray_func = nir_function_create(shader, "trace_ray_func");
+   radv_nir_init_function_params(trace_ray_func, MESA_SHADER_INTERSECTION, max_payload_size);
+   vars.trace_ray_func = trace_ray_func;
+   nir_function *chit_miss_func = nir_function_create(shader, "chit_miss_func");
+   radv_nir_init_function_params(chit_miss_func, MESA_SHADER_CLOSEST_HIT, max_payload_size);
+   vars.chit_miss_func = chit_miss_func;
+   nir_function *callable_func = nir_function_create(shader, "callable_func");
+   radv_nir_init_function_params(callable_func, MESA_SHADER_CALLABLE, max_payload_size);
+   vars.callable_func = callable_func;
    return vars;
 }
 
@@ -849,7 +1089,8 @@ insert_rt_case(nir_builder *b, nir_shader *shader, struct rt_variables *vars, ni
 
    nir_opt_dead_cf(shader);
 
-   struct rt_variables src_vars = create_rt_variables(shader, vars->device, vars->flags, vars->monolithic);
+   struct rt_variables src_vars =
+      create_rt_variables(shader, vars->device, vars->flags, vars->payload_size, vars->monolithic);
    map_rt_variables(var_remap, &src_vars, vars);
 
    NIR_PASS_V(shader, lower_rt_instructions, &src_vars, false, NULL);
@@ -1761,7 +2002,7 @@ radv_build_traversal_shader(struct radv_device *device, struct radv_ray_tracing_
    b.shader->info.workgroup_size[0] = 8;
    b.shader->info.workgroup_size[1] = pdev->rt_wave_size == 64 ? 8 : 4;
    b.shader->info.shared_size = pdev->rt_wave_size * MAX_STACK_ENTRY_COUNT * sizeof(uint32_t);
-   struct rt_variables vars = create_rt_variables(b.shader, device, create_flags, false);
+   struct rt_variables vars = create_rt_variables(b.shader, device, create_flags, false, 0);
 
    if (info->tmin.state == RADV_RT_CONST_ARG_STATE_VALID)
       nir_store_var(&b, vars.tmin, nir_imm_int(&b, info->tmin.value), 0x1);
@@ -1940,7 +2181,7 @@ radv_nir_lower_rt_abi(nir_shader *shader, const VkRayTracingPipelineCreateInfoKH
 
    const VkPipelineCreateFlagBits2KHR create_flags = vk_rt_pipeline_create_flags(pCreateInfo);
 
-   struct rt_variables vars = create_rt_variables(shader, device, create_flags, monolithic);
+   struct rt_variables vars = create_rt_variables(shader, device, create_flags, payload_size, monolithic);
 
    if (monolithic)
       lower_rt_instructions_monolithic(shader, device, pipeline, info, pCreateInfo, payload_size, &vars);
-- 
GitLab


From 4eab50de5a3b2b1802f4093d840b1d854e9b8e3d Mon Sep 17 00:00:00 2001
From: Friedrich Vock <friedrich.vock@gmx.de>
Date: Wed, 5 Jun 2024 12:04:36 +0200
Subject: [PATCH 17/71] radv/rt: Convert lower_rt_derefs to register payloads

All payloads alias the same registers by the time RT functions get
called. In order to pretend that the payload variables (represented by
function_temp vars) are separate, payload values are copied to the
"global" payload variables (shader_temp variables) just before a shader
call, and copied from there immediately after the shader call.
---
 src/amd/vulkan/nir/radv_nir_rt_shader.c | 85 ++++++++++++++++++++-----
 1 file changed, 68 insertions(+), 17 deletions(-)

diff --git a/src/amd/vulkan/nir/radv_nir_rt_shader.c b/src/amd/vulkan/nir/radv_nir_rt_shader.c
index 23c49ba4f3198..36c60fb2c52ed 100644
--- a/src/amd/vulkan/nir/radv_nir_rt_shader.c
+++ b/src/amd/vulkan/nir/radv_nir_rt_shader.c
@@ -124,6 +124,62 @@ radv_visit_inlined_shaders(nir_builder *b, nir_def *sbt_idx, bool can_have_null_
    free(cases);
 }
 
+static void
+lower_rt_deref_var(nir_shader *shader, nir_function_impl *impl, nir_instr *instr, struct hash_table *cloned_vars)
+{
+   nir_deref_instr *deref = nir_instr_as_deref(instr);
+   nir_variable *var = deref->var;
+   struct hash_entry *entry = _mesa_hash_table_search(cloned_vars, var);
+   if (!(var->data.mode & nir_var_function_temp) && !entry)
+      return;
+
+   hash_table_foreach (cloned_vars, cloned_entry) {
+      if (var == cloned_entry->data)
+         return;
+   }
+
+   nir_variable *new_var;
+   if (entry) {
+      new_var = entry->data;
+   } else {
+      new_var = nir_variable_clone(var, shader);
+      _mesa_hash_table_insert(cloned_vars, var, new_var);
+
+      exec_node_remove(&var->node);
+      var->data.mode = nir_var_shader_temp;
+      exec_list_push_tail(&shader->variables, &var->node);
+
+      exec_list_push_tail(&impl->locals, &new_var->node);
+   }
+
+   deref->modes = nir_var_shader_temp;
+
+   nir_foreach_use_safe (use, nir_instr_def(instr)) {
+      if (nir_src_is_if(use))
+         continue;
+
+      nir_instr *parent = nir_src_parent_instr(use);
+      if (parent->type != nir_instr_type_intrinsic)
+         continue;
+
+      nir_intrinsic_instr *intrin = nir_instr_as_intrinsic(parent);
+      if (intrin->intrinsic != nir_intrinsic_trace_ray && intrin->intrinsic != nir_intrinsic_execute_callable &&
+          intrin->intrinsic != nir_intrinsic_execute_closest_hit_amd &&
+          intrin->intrinsic != nir_intrinsic_execute_miss_amd)
+         continue;
+
+      nir_builder b = nir_builder_at(nir_before_instr(parent));
+      nir_deref_instr *old_deref = nir_build_deref_var(&b, var);
+      nir_deref_instr *new_deref = nir_build_deref_var(&b, new_var);
+
+      nir_copy_deref(&b, new_deref, old_deref);
+      b.cursor = nir_after_instr(parent);
+      nir_copy_deref(&b, old_deref, new_deref);
+
+      nir_src_rewrite(use, nir_instr_def(&new_deref->instr));
+   }
+}
+
 static bool
 lower_rt_derefs(nir_shader *shader)
 {
@@ -131,9 +187,7 @@ lower_rt_derefs(nir_shader *shader)
 
    bool progress = false;
 
-   nir_builder b = nir_builder_at(nir_before_impl(impl));
-
-   nir_def *arg_offset = nir_load_rt_arg_scratch_offset_amd(&b);
+   struct hash_table *cloned_vars = _mesa_pointer_hash_table_create(shader);
 
    nir_foreach_block (block, impl) {
       nir_foreach_instr_safe (instr, block) {
@@ -141,18 +195,18 @@ lower_rt_derefs(nir_shader *shader)
             continue;
 
          nir_deref_instr *deref = nir_instr_as_deref(instr);
-         if (!nir_deref_mode_is(deref, nir_var_shader_call_data))
+         if (!nir_deref_mode_is(deref, nir_var_function_temp))
             continue;
 
-         deref->modes = nir_var_function_temp;
-         progress = true;
-
          if (deref->deref_type == nir_deref_type_var) {
-            b.cursor = nir_before_instr(&deref->instr);
-            nir_deref_instr *replacement =
-               nir_build_deref_cast(&b, arg_offset, nir_var_function_temp, deref->var->type, 0);
-            nir_def_rewrite_uses(&deref->def, &replacement->def);
-            nir_instr_remove(&deref->instr);
+            lower_rt_deref_var(shader, impl, instr, cloned_vars);
+            progress = true;
+         } else {
+            assert(deref->deref_type != nir_deref_type_cast);
+            /* Parent modes might have changed, propagate change */
+            nir_deref_instr *parent = nir_src_as_deref(deref->parent);
+            if (parent->modes != deref->modes)
+               deref->modes = parent->modes;
          }
       }
    }
@@ -1128,12 +1182,9 @@ void
 radv_nir_lower_rt_io(nir_shader *nir, bool monolithic, uint32_t payload_offset)
 {
    if (!monolithic) {
-      NIR_PASS(_, nir, nir_lower_vars_to_explicit_types, nir_var_function_temp | nir_var_shader_call_data,
-               glsl_get_natural_size_align_bytes);
-
       NIR_PASS(_, nir, lower_rt_derefs);
-
-      NIR_PASS(_, nir, nir_lower_explicit_io, nir_var_function_temp, nir_address_format_32bit_offset);
+      NIR_PASS(_, nir, nir_split_var_copies);
+      NIR_PASS(_, nir, nir_lower_var_copies);
    } else {
       if (nir->info.stage == MESA_SHADER_RAYGEN) {
          /* Use nir_lower_vars_to_explicit_types to assign the payload locations. We call
-- 
GitLab


From 1833492ce1e909a9eb32b5f67cd98399d13c2a05 Mon Sep 17 00:00:00 2001
From: Friedrich Vock <friedrich.vock@gmx.de>
Date: Wed, 5 Jun 2024 12:09:15 +0200
Subject: [PATCH 18/71] radv/rt: Align radv_nir_lower_rt_io to new lowering

---
 src/amd/vulkan/nir/radv_nir_rt_shader.c | 12 +++++++-----
 src/amd/vulkan/radv_shader.h            |  2 +-
 2 files changed, 8 insertions(+), 6 deletions(-)

diff --git a/src/amd/vulkan/nir/radv_nir_rt_shader.c b/src/amd/vulkan/nir/radv_nir_rt_shader.c
index 36c60fb2c52ed..05fb4ce618433 100644
--- a/src/amd/vulkan/nir/radv_nir_rt_shader.c
+++ b/src/amd/vulkan/nir/radv_nir_rt_shader.c
@@ -1179,7 +1179,7 @@ radv_lower_payload_arg_to_offset(nir_builder *b, nir_intrinsic_instr *instr, voi
 }
 
 void
-radv_nir_lower_rt_io(nir_shader *nir, bool monolithic, uint32_t payload_offset)
+radv_nir_lower_rt_io(nir_shader *nir, bool monolithic, uint32_t payload_offset, uint32_t *payload_size)
 {
    if (!monolithic) {
       NIR_PASS(_, nir, lower_rt_derefs);
@@ -1191,7 +1191,9 @@ radv_nir_lower_rt_io(nir_shader *nir, bool monolithic, uint32_t payload_offset)
           * nir_lower_vars_to_explicit_types later after splitting the payloads.
           */
          uint32_t scratch_size = nir->scratch_size;
+         nir->scratch_size = 0;
          nir_lower_vars_to_explicit_types(nir, nir_var_function_temp, glsl_get_natural_size_align_bytes);
+         *payload_size = MAX2(*payload_size, nir->scratch_size);
          nir->scratch_size = scratch_size;
 
          nir_shader_intrinsics_pass(nir, radv_lower_payload_arg_to_offset,
@@ -1626,7 +1628,7 @@ radv_build_ahit_case(nir_builder *b, nir_def *sbt_idx, struct radv_ray_tracing_g
       radv_pipeline_cache_handle_to_nir(data->device, data->pipeline->stages[group->any_hit_shader].nir);
    assert(nir_stage);
 
-   radv_nir_lower_rt_io(nir_stage, data->vars->monolithic, data->vars->payload_offset);
+   radv_nir_lower_rt_io(nir_stage, data->vars->monolithic, data->vars->payload_offset, NULL);
 
    insert_rt_case(b, nir_stage, data->vars, sbt_idx, group->handle.any_hit_index);
    ralloc_free(nir_stage);
@@ -1650,7 +1652,7 @@ radv_build_isec_case(nir_builder *b, nir_def *sbt_idx, struct radv_ray_tracing_g
       radv_pipeline_cache_handle_to_nir(data->device, data->pipeline->stages[group->intersection_shader].nir);
    assert(nir_stage);
 
-   radv_nir_lower_rt_io(nir_stage, data->vars->monolithic, data->vars->payload_offset);
+   radv_nir_lower_rt_io(nir_stage, data->vars->monolithic, data->vars->payload_offset, NULL);
 
    nir_shader *any_hit_stage = NULL;
    if (group->any_hit_shader != VK_SHADER_UNUSED_KHR) {
@@ -1658,7 +1660,7 @@ radv_build_isec_case(nir_builder *b, nir_def *sbt_idx, struct radv_ray_tracing_g
          radv_pipeline_cache_handle_to_nir(data->device, data->pipeline->stages[group->any_hit_shader].nir);
       assert(any_hit_stage);
 
-      radv_nir_lower_rt_io(any_hit_stage, data->vars->monolithic, data->vars->payload_offset);
+      radv_nir_lower_rt_io(any_hit_stage, data->vars->monolithic, data->vars->payload_offset, NULL);
 
       /* reserve stack size for any_hit before it is inlined */
       data->pipeline->stages[group->any_hit_shader].stack_size = any_hit_stage->scratch_size;
@@ -1702,7 +1704,7 @@ radv_build_recursive_case(nir_builder *b, nir_def *sbt_idx, struct radv_ray_trac
       radv_pipeline_cache_handle_to_nir(data->device, data->pipeline->stages[group->recursive_shader].nir);
    assert(nir_stage);
 
-   radv_nir_lower_rt_io(nir_stage, data->vars->monolithic, data->vars->payload_offset);
+   radv_nir_lower_rt_io(nir_stage, data->vars->monolithic, data->vars->payload_offset, NULL);
 
    insert_rt_case(b, nir_stage, data->vars, sbt_idx, group->handle.general_index);
    ralloc_free(nir_stage);
diff --git a/src/amd/vulkan/radv_shader.h b/src/amd/vulkan/radv_shader.h
index 60489905ad2cb..569521674b5b5 100644
--- a/src/amd/vulkan/radv_shader.h
+++ b/src/amd/vulkan/radv_shader.h
@@ -503,7 +503,7 @@ radv_get_rt_shader_entrypoint(nir_shader *shader)
    return NULL;
 }
 
-void radv_nir_lower_rt_io(nir_shader *shader, bool monolithic, uint32_t payload_offset);
+void radv_nir_lower_rt_io(nir_shader *shader, bool monolithic, uint32_t payload_offset, uint32_t *payload_size);
 
 struct radv_ray_tracing_stage_info;
 
-- 
GitLab


From f5d11ddc38916610b390a2a14025c3303a534b4d Mon Sep 17 00:00:00 2001
From: Friedrich Vock <friedrich.vock@gmx.de>
Date: Wed, 5 Jun 2024 12:10:31 +0200
Subject: [PATCH 19/71] radv/rt: Include inlined shader scratch size in
 traversal scratch

When calls without tail-call optimization happen, the traversal shader
must spill, and spilled vars must be placed after shader scratch.
---
 src/amd/vulkan/nir/radv_nir_rt_shader.c | 3 +++
 1 file changed, 3 insertions(+)

diff --git a/src/amd/vulkan/nir/radv_nir_rt_shader.c b/src/amd/vulkan/nir/radv_nir_rt_shader.c
index 05fb4ce618433..11f33a4feeb58 100644
--- a/src/amd/vulkan/nir/radv_nir_rt_shader.c
+++ b/src/amd/vulkan/nir/radv_nir_rt_shader.c
@@ -1629,6 +1629,7 @@ radv_build_ahit_case(nir_builder *b, nir_def *sbt_idx, struct radv_ray_tracing_g
    assert(nir_stage);
 
    radv_nir_lower_rt_io(nir_stage, data->vars->monolithic, data->vars->payload_offset, NULL);
+   b->shader->scratch_size = MAX2(nir_stage->scratch_size, b->shader->scratch_size);
 
    insert_rt_case(b, nir_stage, data->vars, sbt_idx, group->handle.any_hit_index);
    ralloc_free(nir_stage);
@@ -1664,10 +1665,12 @@ radv_build_isec_case(nir_builder *b, nir_def *sbt_idx, struct radv_ray_tracing_g
 
       /* reserve stack size for any_hit before it is inlined */
       data->pipeline->stages[group->any_hit_shader].stack_size = any_hit_stage->scratch_size;
+      b->shader->scratch_size = MAX2(any_hit_stage->scratch_size, b->shader->scratch_size);
 
       nir_lower_intersection_shader(nir_stage, any_hit_stage);
       ralloc_free(any_hit_stage);
    }
+   b->shader->scratch_size = MAX2(nir_stage->scratch_size, b->shader->scratch_size);
 
    insert_rt_case(b, nir_stage, data->vars, sbt_idx, group->handle.intersection_index);
    ralloc_free(nir_stage);
-- 
GitLab


From 7f7c07b238f5b3f50bc65aa208b6e1e0c2a12780 Mon Sep 17 00:00:00 2001
From: Friedrich Vock <friedrich.vock@gmx.de>
Date: Wed, 5 Jun 2024 12:17:15 +0200
Subject: [PATCH 20/71] radv/rt: Don't store vars->shader_record_ptr directly
 in load_sbt_entry

When calling functions, we don't want the new shader record to stick
beyond the function call, so only store it when not calling functions.
---
 src/amd/vulkan/nir/radv_nir_rt_shader.c | 20 ++++++++++++--------
 1 file changed, 12 insertions(+), 8 deletions(-)

diff --git a/src/amd/vulkan/nir/radv_nir_rt_shader.c b/src/amd/vulkan/nir/radv_nir_rt_shader.c
index 11f33a4feeb58..116fee096f91e 100644
--- a/src/amd/vulkan/nir/radv_nir_rt_shader.c
+++ b/src/amd/vulkan/nir/radv_nir_rt_shader.c
@@ -654,7 +654,7 @@ enum sbt_entry {
    SBT_ANY_HIT_IDX = offsetof(struct radv_pipeline_group_handle, any_hit_index),
 };
 
-static void
+static nir_def *
 load_sbt_entry(nir_builder *b, const struct rt_variables *vars, nir_def *idx, enum sbt_type binding,
                enum sbt_entry offset)
 {
@@ -674,7 +674,7 @@ load_sbt_entry(nir_builder *b, const struct rt_variables *vars, nir_def *idx, en
    }
 
    nir_def *record_addr = nir_iadd_imm(b, addr, RADV_RT_HANDLE_SIZE - offset);
-   nir_store_var(b, vars->shader_record_ptr, record_addr, 1);
+   return record_addr;
 }
 
 struct radv_rt_shader_info {
@@ -951,7 +951,7 @@ radv_lower_rt_instruction(nir_builder *b, nir_instr *instr, void *_data)
       nir_store_var(b, vars->instance_addr, intr->src[3].ssa, 0x1);
       nir_store_var(b, vars->geometry_id_and_flags, intr->src[4].ssa, 0x1);
       nir_store_var(b, vars->hit_kind, intr->src[5].ssa, 0x1);
-      load_sbt_entry(b, vars, intr->src[0].ssa, SBT_HIT, SBT_RECURSIVE_PTR);
+      nir_def *record = load_sbt_entry(b, vars, intr->src[0].ssa, SBT_HIT, SBT_RECURSIVE_PTR);
 
       nir_def *should_return =
          nir_test_mask(b, nir_load_var(b, vars->cull_mask_and_flags), SpvRayFlagsSkipClosestHitShaderKHRMask);
@@ -975,7 +975,7 @@ radv_lower_rt_instruction(nir_builder *b, nir_instr *instr, void *_data)
       nir_store_var(b, vars->geometry_id_and_flags, undef, 0x1);
       nir_store_var(b, vars->hit_kind, undef, 0x1);
       nir_def *miss_index = nir_load_var(b, vars->miss_index);
-      load_sbt_entry(b, vars, miss_index, SBT_MISS, SBT_RECURSIVE_PTR);
+      nir_def *record = load_sbt_entry(b, vars, miss_index, SBT_MISS, SBT_RECURSIVE_PTR);
 
       if (!(vars->flags & VK_PIPELINE_CREATE_2_RAY_TRACING_NO_NULL_MISS_SHADERS_BIT_KHR)) {
          /* In case of a NULL miss shader, do nothing and just return. */
@@ -1744,7 +1744,8 @@ handle_candidate_triangle(nir_builder *b, struct radv_triangle_intersection *int
       nir_store_var(b, inner_vars.instance_addr, nir_load_var(b, data->trav_vars->instance_addr), 0x1);
       nir_store_var(b, inner_vars.hit_kind, hit_kind, 0x1);
 
-      load_sbt_entry(b, &inner_vars, sbt_idx, SBT_HIT, SBT_ANY_HIT_IDX);
+      nir_def *record = load_sbt_entry(b, &inner_vars, sbt_idx, SBT_HIT, SBT_ANY_HIT_IDX);
+      nir_store_var(b, inner_vars.shader_record_ptr, record, 0x1);
 
       struct radv_rt_case_data case_data = {
          .device = data->device,
@@ -1812,7 +1813,8 @@ handle_candidate_aabb(nir_builder *b, struct radv_leaf_intersection *intersectio
    nir_store_var(b, inner_vars.instance_addr, nir_load_var(b, data->trav_vars->instance_addr), 0x1);
    nir_store_var(b, inner_vars.opaque, intersection->opaque, 1);
 
-   load_sbt_entry(b, &inner_vars, sbt_idx, SBT_HIT, SBT_INTERSECTION_IDX);
+   nir_def *record = load_sbt_entry(b, &inner_vars, sbt_idx, SBT_HIT, SBT_INTERSECTION_IDX);
+   nir_store_var(b, inner_vars.shader_record_ptr, record, 0x1);
 
    nir_store_var(b, data->vars->ahit_accept, nir_imm_false(b), 0x1);
    nir_store_var(b, data->vars->ahit_terminate, nir_imm_false(b), 0x1);
@@ -1991,7 +1993,8 @@ radv_build_traversal(struct radv_device *device, struct radv_ray_tracing_pipelin
    nir_push_if(b, nir_load_var(b, trav_vars.hit));
    {
       if (monolithic) {
-         load_sbt_entry(b, vars, nir_load_var(b, vars->idx), SBT_HIT, SBT_CLOSEST_HIT_IDX);
+         nir_def *record = load_sbt_entry(b, vars, nir_load_var(b, vars->idx), SBT_HIT, SBT_CLOSEST_HIT_IDX);
+         nir_store_var(b, vars->shader_record_ptr, record, 0x1);
 
          nir_def *should_return =
             nir_test_mask(b, nir_load_var(b, vars->cull_mask_and_flags), SpvRayFlagsSkipClosestHitShaderKHRMask);
@@ -2023,7 +2026,8 @@ radv_build_traversal(struct radv_device *device, struct radv_ray_tracing_pipelin
    nir_push_else(b, NULL);
    {
       if (monolithic) {
-         load_sbt_entry(b, vars, nir_load_var(b, vars->miss_index), SBT_MISS, SBT_GENERAL_IDX);
+         nir_def *record = load_sbt_entry(b, vars, nir_load_var(b, vars->miss_index), SBT_MISS, SBT_GENERAL_IDX);
+         nir_store_var(b, vars->shader_record_ptr, record, 0x1);
 
          struct radv_rt_case_data case_data = {
             .device = device,
-- 
GitLab


From 231ea4d362d23bc29084d1bfba874d3ce82aa2a1 Mon Sep 17 00:00:00 2001
From: Friedrich Vock <friedrich.vock@gmx.de>
Date: Wed, 5 Jun 2024 12:21:29 +0200
Subject: [PATCH 21/71] radv/rt: Use shader calls directly

Don't call nir_lower_shader_calls anymore, but emit nir_call
instructions for trace_ray and friends. Also, switch from shader args
to parameters for most things, and change lowerings accordingly.
---
 src/amd/common/ac_shader_args.h         |  16 -
 src/amd/vulkan/nir/radv_nir_rt_shader.c | 394 ++++++++++++------------
 src/amd/vulkan/radv_pipeline_rt.c       |  62 +---
 src/amd/vulkan/radv_shader.h            |   7 +-
 src/amd/vulkan/radv_shader_args.c       |  19 +-
 5 files changed, 214 insertions(+), 284 deletions(-)

diff --git a/src/amd/common/ac_shader_args.h b/src/amd/common/ac_shader_args.h
index 62ac708c3d185..030a271e22ff3 100644
--- a/src/amd/common/ac_shader_args.h
+++ b/src/amd/common/ac_shader_args.h
@@ -179,29 +179,13 @@ struct ac_shader_args {
 
    /* RT */
    struct {
-      struct ac_arg uniform_shader_addr;
       struct ac_arg sbt_descriptors;
       struct ac_arg launch_sizes[3];
       struct ac_arg launch_size_addr;
       struct ac_arg launch_ids[3];
       struct ac_arg dynamic_callable_stack_base;
       struct ac_arg traversal_shader_addr;
-      struct ac_arg shader_addr;
-      struct ac_arg shader_record;
       struct ac_arg payload_offset;
-      struct ac_arg ray_origin;
-      struct ac_arg ray_tmin;
-      struct ac_arg ray_direction;
-      struct ac_arg ray_tmax;
-      struct ac_arg cull_mask_and_flags;
-      struct ac_arg sbt_offset;
-      struct ac_arg sbt_stride;
-      struct ac_arg miss_index;
-      struct ac_arg accel_struct;
-      struct ac_arg primitive_id;
-      struct ac_arg instance_addr;
-      struct ac_arg geometry_id_and_flags;
-      struct ac_arg hit_kind;
    } rt;
 };
 
diff --git a/src/amd/vulkan/nir/radv_nir_rt_shader.c b/src/amd/vulkan/nir/radv_nir_rt_shader.c
index 116fee096f91e..a054e61334076 100644
--- a/src/amd/vulkan/nir/radv_nir_rt_shader.c
+++ b/src/amd/vulkan/nir/radv_nir_rt_shader.c
@@ -712,74 +712,48 @@ radv_lower_rt_instruction(nir_builder *b, nir_instr *instr, void *_data)
 
    nir_def *ret = NULL;
    switch (intr->intrinsic) {
-   case nir_intrinsic_rt_execute_callable: {
-      uint32_t size = align(nir_intrinsic_stack_size(intr), 16);
-      nir_def *ret_ptr = nir_load_resume_shader_address_amd(b, nir_intrinsic_call_idx(intr));
-      ret_ptr = nir_ior_imm(b, ret_ptr, radv_get_rt_priority(b->shader->info.stage));
-
-      nir_store_var(b, vars->stack_ptr, nir_iadd_imm_nuw(b, nir_load_var(b, vars->stack_ptr), size), 1);
-      nir_store_scratch(b, ret_ptr, nir_load_var(b, vars->stack_ptr), .align_mul = 16);
-
-      nir_store_var(b, vars->stack_ptr, nir_iadd_imm_nuw(b, nir_load_var(b, vars->stack_ptr), 16), 1);
-      load_sbt_entry(b, vars, intr->src[0].ssa, SBT_CALLABLE, SBT_RECURSIVE_PTR);
-
-      nir_store_var(b, vars->arg, nir_iadd_imm(b, intr->src[1].ssa, -size - 16), 1);
-
-      vars->stack_size = MAX2(vars->stack_size, size + 16);
+   case nir_intrinsic_execute_callable: {
+      nir_def *record = load_sbt_entry(b, vars, intr->src[0].ssa, SBT_CALLABLE, SBT_RECURSIVE_PTR);
+
+      unsigned param_count = RAYGEN_ARG_COUNT + DIV_ROUND_UP(vars->payload_size, 4);
+      nir_def **args = rzalloc_array_size(b->shader, sizeof(nir_def *), param_count);
+      args[RAYGEN_ARG_SHADER_RECORD_PTR] = record;
+      for (unsigned i = 0; i < DIV_ROUND_UP(vars->payload_size, 4); ++i) {
+         args[RAYGEN_ARG_COUNT + i] = nir_instr_def(&nir_build_deref_var(b, vars->out_payload_storage[i])->instr);
+      }
+      nir_build_indirect_call(b, vars->callable_func, nir_load_var(b, vars->shader_addr), nir_call_abi_rt_raygen_amd,
+                              param_count, args);
       break;
    }
-   case nir_intrinsic_rt_trace_ray: {
-      uint32_t size = align(nir_intrinsic_stack_size(intr), 16);
-      nir_def *ret_ptr = nir_load_resume_shader_address_amd(b, nir_intrinsic_call_idx(intr));
-      ret_ptr = nir_ior_imm(b, ret_ptr, radv_get_rt_priority(b->shader->info.stage));
-
-      nir_store_var(b, vars->stack_ptr, nir_iadd_imm_nuw(b, nir_load_var(b, vars->stack_ptr), size), 1);
-      nir_store_scratch(b, ret_ptr, nir_load_var(b, vars->stack_ptr), .align_mul = 16);
-
-      nir_store_var(b, vars->stack_ptr, nir_iadd_imm_nuw(b, nir_load_var(b, vars->stack_ptr), 16), 1);
-
-      nir_store_var(b, vars->shader_addr, nir_load_var(b, vars->traversal_addr), 1);
-      nir_store_var(b, vars->arg, nir_iadd_imm(b, intr->src[10].ssa, -size - 16), 1);
-
-      vars->stack_size = MAX2(vars->stack_size, size + 16);
-
+   case nir_intrinsic_trace_ray: {
+      nir_def *undef = nir_undef(b, 1, 32);
       /* Per the SPIR-V extension spec we have to ignore some bits for some arguments. */
-      nir_store_var(b, vars->accel_struct, intr->src[0].ssa, 0x1);
-      nir_store_var(b, vars->cull_mask_and_flags, nir_ior(b, nir_ishl_imm(b, intr->src[2].ssa, 24), intr->src[1].ssa),
-                    0x1);
-      nir_store_var(b, vars->sbt_offset, nir_iand_imm(b, intr->src[3].ssa, 0xf), 0x1);
-      nir_store_var(b, vars->sbt_stride, nir_iand_imm(b, intr->src[4].ssa, 0xf), 0x1);
-      nir_store_var(b, vars->miss_index, nir_iand_imm(b, intr->src[5].ssa, 0xffff), 0x1);
-      nir_store_var(b, vars->origin, intr->src[6].ssa, 0x7);
-      nir_store_var(b, vars->tmin, intr->src[7].ssa, 0x1);
-      nir_store_var(b, vars->direction, intr->src[8].ssa, 0x7);
-      nir_store_var(b, vars->tmax, intr->src[9].ssa, 0x1);
-      break;
-   }
-   case nir_intrinsic_rt_resume: {
-      uint32_t size = align(nir_intrinsic_stack_size(intr), 16);
-
-      nir_store_var(b, vars->stack_ptr, nir_iadd_imm(b, nir_load_var(b, vars->stack_ptr), -size), 1);
-      break;
-   }
-   case nir_intrinsic_rt_return_amd: {
-      if (b->shader->info.stage == MESA_SHADER_RAYGEN) {
-         nir_terminate(b);
-         break;
+      nir_def *cull_mask_and_flags = nir_ior(b, nir_ishl_imm(b, intr->src[2].ssa, 24), intr->src[1].ssa);
+
+      unsigned param_count = TRAVERSAL_ARG_PAYLOAD_BASE + DIV_ROUND_UP(vars->payload_size, 4);
+      nir_def **args = rzalloc_array_size(b->shader, sizeof(nir_def *), param_count);
+      args[TRAVERSAL_ARG_SHADER_RECORD_PTR] = nir_load_var(b, vars->shader_record_ptr);
+      args[TRAVERSAL_ARG_ACCEL_STRUCT] = intr->src[0].ssa;
+      args[TRAVERSAL_ARG_CULL_MASK_AND_FLAGS] = cull_mask_and_flags;
+      args[TRAVERSAL_ARG_SBT_OFFSET] = nir_iand_imm(b, intr->src[3].ssa, 0xf);
+      args[TRAVERSAL_ARG_SBT_STRIDE] = nir_iand_imm(b, intr->src[4].ssa, 0xf);
+      args[TRAVERSAL_ARG_MISS_INDEX] = nir_iand_imm(b, intr->src[5].ssa, 0xffff);
+      args[TRAVERSAL_ARG_RAY_ORIGIN] = intr->src[6].ssa;
+      args[TRAVERSAL_ARG_RAY_TMIN] = intr->src[7].ssa;
+      args[TRAVERSAL_ARG_RAY_DIRECTION] = intr->src[8].ssa;
+      args[TRAVERSAL_ARG_RAY_TMAX] = intr->src[9].ssa;
+      args[TRAVERSAL_ARG_PRIMITIVE_ID] = undef;
+      args[TRAVERSAL_ARG_INSTANCE_ADDR] = nir_undef(b, 1, 64);
+      args[TRAVERSAL_ARG_GEOMETRY_ID_AND_FLAGS] = undef;
+      args[TRAVERSAL_ARG_HIT_KIND] = undef;
+      for (unsigned i = 0; i < DIV_ROUND_UP(vars->payload_size, 4); ++i) {
+         args[TRAVERSAL_ARG_PAYLOAD_BASE + i] =
+            nir_instr_def(&nir_build_deref_var(b, vars->out_payload_storage[i])->instr);
       }
-      insert_rt_return(b, vars);
+      nir_build_indirect_call(b, vars->trace_ray_func, nir_load_var(b, vars->traversal_addr),
+                              nir_call_abi_rt_traversal_amd, param_count, args);
       break;
    }
-   case nir_intrinsic_load_scratch: {
-      if (data->late_lowering)
-         nir_src_rewrite(&intr->src[0], nir_iadd_nuw(b, nir_load_var(b, vars->stack_ptr), intr->src[0].ssa));
-      return true;
-   }
-   case nir_intrinsic_store_scratch: {
-      if (data->late_lowering)
-         nir_src_rewrite(&intr->src[1], nir_iadd_nuw(b, nir_load_var(b, vars->stack_ptr), intr->src[1].ssa));
-      return true;
-   }
    case nir_intrinsic_load_shader_record_ptr: {
       ret = nir_load_var(b, vars->shader_record_ptr);
       break;
@@ -946,11 +920,6 @@ radv_lower_rt_instruction(nir_builder *b, nir_instr *instr, void *_data)
       break;
    }
    case nir_intrinsic_execute_closest_hit_amd: {
-      nir_store_var(b, vars->tmax, intr->src[1].ssa, 0x1);
-      nir_store_var(b, vars->primitive_id, intr->src[2].ssa, 0x1);
-      nir_store_var(b, vars->instance_addr, intr->src[3].ssa, 0x1);
-      nir_store_var(b, vars->geometry_id_and_flags, intr->src[4].ssa, 0x1);
-      nir_store_var(b, vars->hit_kind, intr->src[5].ssa, 0x1);
       nir_def *record = load_sbt_entry(b, vars, intr->src[0].ssa, SBT_HIT, SBT_RECURSIVE_PTR);
 
       nir_def *should_return =
@@ -962,27 +931,71 @@ radv_lower_rt_instruction(nir_builder *b, nir_instr *instr, void *_data)
 
       /* should_return is set if we had a hit but we won't be calling the closest hit
        * shader and hence need to return immediately to the calling shader. */
-      nir_push_if(b, should_return);
-      insert_rt_return(b, vars);
+      nir_push_if(b, nir_inot(b, should_return));
+      unsigned param_count = CHIT_MISS_ARG_PAYLOAD_BASE + DIV_ROUND_UP(vars->payload_size, 4);
+      nir_def **args = rzalloc_array_size(b->shader, sizeof(nir_def *), param_count);
+      args[CHIT_MISS_ARG_SHADER_RECORD_PTR] = record;
+      args[CHIT_MISS_ARG_ACCEL_STRUCT] = nir_load_var(b, vars->accel_struct);
+      args[CHIT_MISS_ARG_CULL_MASK_AND_FLAGS] = nir_load_var(b, vars->cull_mask_and_flags);
+      args[CHIT_MISS_ARG_SBT_OFFSET] = nir_load_var(b, vars->sbt_offset);
+      args[CHIT_MISS_ARG_SBT_STRIDE] = nir_load_var(b, vars->sbt_stride);
+      args[CHIT_MISS_ARG_MISS_INDEX] = nir_load_var(b, vars->miss_index);
+      args[CHIT_MISS_ARG_RAY_ORIGIN] = nir_load_var(b, vars->origin);
+      args[CHIT_MISS_ARG_RAY_TMIN] = nir_load_var(b, vars->tmin);
+      args[CHIT_MISS_ARG_RAY_DIRECTION] = nir_load_var(b, vars->direction);
+      args[CHIT_MISS_ARG_RAY_TMAX] = intr->src[1].ssa;
+      args[CHIT_MISS_ARG_PRIMITIVE_ID] = intr->src[2].ssa;
+      args[CHIT_MISS_ARG_INSTANCE_ADDR] = intr->src[3].ssa;
+      args[CHIT_MISS_ARG_GEOMETRY_ID_AND_FLAGS] = intr->src[4].ssa;
+      args[CHIT_MISS_ARG_HIT_KIND] = intr->src[5].ssa;
+      for (unsigned i = 0; i < DIV_ROUND_UP(vars->payload_size, 4); ++i) {
+         args[CHIT_MISS_ARG_PAYLOAD_BASE + i] =
+            nir_instr_def(&nir_build_deref_cast(b, nir_load_param(b, TRAVERSAL_ARG_PAYLOAD_BASE + i),
+                                                nir_var_shader_call_data, glsl_uint_type(), 4)
+                              ->instr);
+      }
+      nir_build_indirect_call(b, vars->chit_miss_func, nir_load_var(b, vars->shader_addr), nir_call_abi_rt_raygen_amd,
+                              param_count, args);
       nir_pop_if(b, NULL);
       break;
    }
    case nir_intrinsic_execute_miss_amd: {
-      nir_store_var(b, vars->tmax, intr->src[0].ssa, 0x1);
       nir_def *undef = nir_undef(b, 1, 32);
-      nir_store_var(b, vars->primitive_id, undef, 0x1);
-      nir_store_var(b, vars->instance_addr, nir_undef(b, 1, 64), 0x1);
-      nir_store_var(b, vars->geometry_id_and_flags, undef, 0x1);
-      nir_store_var(b, vars->hit_kind, undef, 0x1);
       nir_def *miss_index = nir_load_var(b, vars->miss_index);
       nir_def *record = load_sbt_entry(b, vars, miss_index, SBT_MISS, SBT_RECURSIVE_PTR);
 
       if (!(vars->flags & VK_PIPELINE_CREATE_2_RAY_TRACING_NO_NULL_MISS_SHADERS_BIT_KHR)) {
          /* In case of a NULL miss shader, do nothing and just return. */
-         nir_push_if(b, nir_ieq_imm(b, nir_load_var(b, vars->shader_addr), 0));
-         insert_rt_return(b, vars);
-         nir_pop_if(b, NULL);
+         nir_push_if(b, nir_ine_imm(b, nir_load_var(b, vars->shader_addr), 0));
+      }
+
+      unsigned param_count = CHIT_MISS_ARG_PAYLOAD_BASE + DIV_ROUND_UP(vars->payload_size, 4);
+      nir_def **args = rzalloc_array_size(b->shader, sizeof(nir_def *), param_count);
+      args[CHIT_MISS_ARG_SHADER_RECORD_PTR] = record;
+      args[CHIT_MISS_ARG_ACCEL_STRUCT] = nir_load_var(b, vars->accel_struct);
+      args[CHIT_MISS_ARG_CULL_MASK_AND_FLAGS] = nir_load_var(b, vars->cull_mask_and_flags);
+      args[CHIT_MISS_ARG_SBT_OFFSET] = nir_load_var(b, vars->sbt_offset);
+      args[CHIT_MISS_ARG_SBT_STRIDE] = nir_load_var(b, vars->sbt_stride);
+      args[CHIT_MISS_ARG_MISS_INDEX] = nir_load_var(b, vars->miss_index);
+      args[CHIT_MISS_ARG_RAY_ORIGIN] = nir_load_var(b, vars->origin);
+      args[CHIT_MISS_ARG_RAY_TMIN] = nir_load_var(b, vars->tmin);
+      args[CHIT_MISS_ARG_RAY_DIRECTION] = nir_load_var(b, vars->direction);
+      args[CHIT_MISS_ARG_RAY_TMAX] = intr->src[0].ssa;
+      args[CHIT_MISS_ARG_PRIMITIVE_ID] = undef;
+      args[CHIT_MISS_ARG_INSTANCE_ADDR] = nir_undef(b, 1, 64);
+      args[CHIT_MISS_ARG_GEOMETRY_ID_AND_FLAGS] = undef;
+      args[CHIT_MISS_ARG_HIT_KIND] = undef;
+      for (unsigned i = 0; i < DIV_ROUND_UP(vars->payload_size, 4); ++i) {
+         args[CHIT_MISS_ARG_PAYLOAD_BASE + i] =
+            nir_instr_def(&nir_build_deref_cast(b, nir_load_param(b, TRAVERSAL_ARG_PAYLOAD_BASE + i),
+                                                nir_var_shader_call_data, glsl_uint_type(), 4)
+                              ->instr);
       }
+      nir_build_indirect_call(b, vars->chit_miss_func, nir_load_var(b, vars->shader_addr), nir_call_abi_rt_raygen_amd,
+                              param_count, args);
+
+      if (!(vars->flags & VK_PIPELINE_CREATE_2_RAY_TRACING_NO_NULL_MISS_SHADERS_BIT_KHR))
+         nir_pop_if(b, NULL);
 
       break;
    }
@@ -992,6 +1005,14 @@ radv_lower_rt_instruction(nir_builder *b, nir_instr *instr, void *_data)
       ret = radv_load_vertex_position(vars->device, b, instance_node_addr, primitive_id, nir_intrinsic_column(intr));
       break;
    }
+   case nir_intrinsic_rt_trace_ray:
+      unreachable("nir_intrinsic_rt_trace_ray");
+   case nir_intrinsic_rt_execute_callable:
+      unreachable("nir_intrinsic_rt_execute_callable");
+   case nir_intrinsic_rt_resume:
+      unreachable("nir_intrinsic_rt_resume");
+   case nir_intrinsic_rt_return_amd:
+      unreachable("nir_intrinsic_rt_return_amd");
    default:
       return false;
    }
@@ -2222,22 +2243,24 @@ lower_rt_instructions_monolithic(nir_shader *shader, struct radv_device *device,
 }
 
 static void
-radv_store_arg(nir_builder *b, const struct radv_shader_args *args, const struct radv_ray_tracing_stage_info *info,
-               struct ac_arg arg, nir_def *value)
+store_param_var(nir_builder *b, nir_variable *var, unsigned param_index, unsigned num_components, unsigned bit_size)
 {
-   /* Do not pass unused data to the next stage. */
-   if (!info || !BITSET_TEST(info->unused_args, arg.arg_index))
-      ac_nir_store_arg(b, &args->ac, arg, value);
+   if (param_index != -1u)
+      nir_store_var(b, var, nir_load_param(b, param_index), (1 << num_components) - 1);
+   else
+      nir_store_var(b, var, nir_undef(b, num_components, bit_size), (1 << num_components) - 1);
 }
 
 void
 radv_nir_lower_rt_abi(nir_shader *shader, const VkRayTracingPipelineCreateInfoKHR *pCreateInfo,
-                      const struct radv_shader_args *args, const struct radv_shader_info *info, uint32_t *stack_size,
-                      bool resume_shader, uint32_t payload_size, struct radv_device *device,
-                      struct radv_ray_tracing_pipeline *pipeline, bool monolithic,
-                      const struct radv_ray_tracing_stage_info *traversal_info)
+                      const struct radv_shader_args *args, const struct radv_shader_info *info, uint32_t payload_size,
+                      uint32_t *stack_size, struct radv_device *device, struct radv_ray_tracing_pipeline *pipeline,
+                      bool monolithic)
 {
    nir_function_impl *impl = nir_shader_get_entrypoint(shader);
+   nir_function *entrypoint_function = impl->function;
+
+   radv_nir_init_function_params(entrypoint_function, shader->info.stage, payload_size);
 
    const VkPipelineCreateFlagBits2KHR create_flags = vk_rt_pipeline_create_flags(pCreateInfo);
 
@@ -2254,33 +2277,77 @@ radv_nir_lower_rt_abi(nir_shader *shader, const VkRayTracingPipelineCreateInfoKH
       vars.stack_size = MAX2(vars.stack_size, shader->scratch_size);
       *stack_size = MAX2(*stack_size, vars.stack_size);
    }
-   shader->scratch_size = 0;
 
    NIR_PASS(_, shader, nir_lower_returns);
 
-   nir_cf_list list;
-   nir_cf_extract(&list, nir_before_impl(impl), nir_after_impl(impl));
+   unsigned shader_record_ptr_arg = -1u;
+   unsigned accel_struct_arg = -1u;
+   unsigned cull_mask_and_flags_arg = -1u;
+   unsigned sbt_offset_arg = -1u;
+   unsigned sbt_stride_arg = -1u;
+   unsigned miss_index_arg = -1u;
+   unsigned ray_origin_arg = -1u;
+   unsigned ray_tmin_arg = -1u;
+   unsigned ray_direction_arg = -1u;
+   unsigned ray_tmax_arg = -1u;
+   unsigned primitive_id_arg = -1u;
+   unsigned instance_addr_arg = -1u;
+   unsigned geometry_id_and_flags_arg = -1u;
+   unsigned hit_kind_arg = -1u;
+   unsigned in_payload_base_arg = -1u;
+
+   switch (shader->info.stage) {
+   case MESA_SHADER_CALLABLE:
+      in_payload_base_arg = RAYGEN_ARG_COUNT;
+      shader_record_ptr_arg = RAYGEN_ARG_SHADER_RECORD_PTR;
+      break;
+   case MESA_SHADER_RAYGEN:
+      shader_record_ptr_arg = RAYGEN_ARG_SHADER_RECORD_PTR;
+      break;
+   case MESA_SHADER_INTERSECTION:
+      shader_record_ptr_arg = TRAVERSAL_ARG_SHADER_RECORD_PTR;
+      accel_struct_arg = TRAVERSAL_ARG_ACCEL_STRUCT;
+      cull_mask_and_flags_arg = TRAVERSAL_ARG_CULL_MASK_AND_FLAGS;
+      sbt_offset_arg = TRAVERSAL_ARG_SBT_OFFSET;
+      sbt_stride_arg = TRAVERSAL_ARG_SBT_STRIDE;
+      miss_index_arg = TRAVERSAL_ARG_MISS_INDEX;
+      ray_origin_arg = TRAVERSAL_ARG_RAY_ORIGIN;
+      ray_tmin_arg = TRAVERSAL_ARG_RAY_TMIN;
+      ray_direction_arg = TRAVERSAL_ARG_RAY_DIRECTION;
+      ray_tmax_arg = TRAVERSAL_ARG_RAY_TMAX;
+      in_payload_base_arg = TRAVERSAL_ARG_PAYLOAD_BASE;
+      break;
+   case MESA_SHADER_CLOSEST_HIT:
+   case MESA_SHADER_MISS:
+      shader_record_ptr_arg = CHIT_MISS_ARG_SHADER_RECORD_PTR;
+      accel_struct_arg = CHIT_MISS_ARG_ACCEL_STRUCT;
+      cull_mask_and_flags_arg = CHIT_MISS_ARG_CULL_MASK_AND_FLAGS;
+      sbt_offset_arg = CHIT_MISS_ARG_SBT_OFFSET;
+      sbt_stride_arg = CHIT_MISS_ARG_SBT_STRIDE;
+      miss_index_arg = CHIT_MISS_ARG_MISS_INDEX;
+      ray_origin_arg = CHIT_MISS_ARG_RAY_ORIGIN;
+      ray_tmin_arg = CHIT_MISS_ARG_RAY_TMIN;
+      ray_direction_arg = CHIT_MISS_ARG_RAY_DIRECTION;
+      ray_tmax_arg = CHIT_MISS_ARG_RAY_TMAX;
+      primitive_id_arg = CHIT_MISS_ARG_PRIMITIVE_ID;
+      instance_addr_arg = CHIT_MISS_ARG_INSTANCE_ADDR;
+      geometry_id_and_flags_arg = CHIT_MISS_ARG_GEOMETRY_ID_AND_FLAGS;
+      hit_kind_arg = CHIT_MISS_ARG_HIT_KIND;
+      in_payload_base_arg = CHIT_MISS_ARG_PAYLOAD_BASE;
+      break;
+   default:
+      break;
+   }
 
    /* initialize variables */
    nir_builder b = nir_builder_at(nir_before_impl(impl));
 
-   nir_def *descriptor_sets = ac_nir_load_arg(&b, &args->ac, args->descriptor_sets[0]);
-   nir_def *push_constants = ac_nir_load_arg(&b, &args->ac, args->ac.push_constants);
-   nir_def *sbt_descriptors = ac_nir_load_arg(&b, &args->ac, args->ac.rt.sbt_descriptors);
-
    nir_def *launch_sizes[3];
    for (uint32_t i = 0; i < ARRAY_SIZE(launch_sizes); i++) {
       launch_sizes[i] = ac_nir_load_arg(&b, &args->ac, args->ac.rt.launch_sizes[i]);
       nir_store_var(&b, vars.launch_sizes[i], launch_sizes[i], 1);
    }
 
-   nir_def *scratch_offset = NULL;
-   if (args->ac.scratch_offset.used)
-      scratch_offset = ac_nir_load_arg(&b, &args->ac, args->ac.scratch_offset);
-   nir_def *ring_offsets = NULL;
-   if (args->ac.ring_offsets.used)
-      ring_offsets = ac_nir_load_arg(&b, &args->ac, args->ac.ring_offsets);
-
    nir_def *launch_ids[3];
    for (uint32_t i = 0; i < ARRAY_SIZE(launch_ids); i++) {
       launch_ids[i] = ac_nir_load_arg(&b, &args->ac, args->ac.rt.launch_ids[i]);
@@ -2290,116 +2357,53 @@ radv_nir_lower_rt_abi(nir_shader *shader, const VkRayTracingPipelineCreateInfoKH
    nir_def *traversal_addr = ac_nir_load_arg(&b, &args->ac, args->ac.rt.traversal_shader_addr);
    nir_store_var(&b, vars.traversal_addr, nir_pack_64_2x32(&b, traversal_addr), 1);
 
-   nir_def *shader_addr = ac_nir_load_arg(&b, &args->ac, args->ac.rt.shader_addr);
-   shader_addr = nir_pack_64_2x32(&b, shader_addr);
-   nir_store_var(&b, vars.shader_addr, shader_addr, 1);
-
    nir_store_var(&b, vars.stack_ptr, ac_nir_load_arg(&b, &args->ac, args->ac.rt.dynamic_callable_stack_base), 1);
-   nir_def *record_ptr = ac_nir_load_arg(&b, &args->ac, args->ac.rt.shader_record);
-   nir_store_var(&b, vars.shader_record_ptr, nir_pack_64_2x32(&b, record_ptr), 1);
-   nir_store_var(&b, vars.arg, ac_nir_load_arg(&b, &args->ac, args->ac.rt.payload_offset), 1);
-
-   nir_def *accel_struct = ac_nir_load_arg(&b, &args->ac, args->ac.rt.accel_struct);
-   nir_store_var(&b, vars.accel_struct, nir_pack_64_2x32(&b, accel_struct), 1);
-   nir_store_var(&b, vars.cull_mask_and_flags, ac_nir_load_arg(&b, &args->ac, args->ac.rt.cull_mask_and_flags), 1);
-   nir_store_var(&b, vars.sbt_offset, ac_nir_load_arg(&b, &args->ac, args->ac.rt.sbt_offset), 1);
-   nir_store_var(&b, vars.sbt_stride, ac_nir_load_arg(&b, &args->ac, args->ac.rt.sbt_stride), 1);
-   nir_store_var(&b, vars.origin, ac_nir_load_arg(&b, &args->ac, args->ac.rt.ray_origin), 0x7);
-   nir_store_var(&b, vars.tmin, ac_nir_load_arg(&b, &args->ac, args->ac.rt.ray_tmin), 1);
-   nir_store_var(&b, vars.direction, ac_nir_load_arg(&b, &args->ac, args->ac.rt.ray_direction), 0x7);
-   nir_store_var(&b, vars.tmax, ac_nir_load_arg(&b, &args->ac, args->ac.rt.ray_tmax), 1);
-
-   if (traversal_info && traversal_info->miss_index.state == RADV_RT_CONST_ARG_STATE_VALID)
-      nir_store_var(&b, vars.miss_index, nir_imm_int(&b, traversal_info->miss_index.value), 0x1);
-   else
-      nir_store_var(&b, vars.miss_index, ac_nir_load_arg(&b, &args->ac, args->ac.rt.miss_index), 0x1);
-
-   nir_store_var(&b, vars.primitive_id, ac_nir_load_arg(&b, &args->ac, args->ac.rt.primitive_id), 1);
-   nir_def *instance_addr = ac_nir_load_arg(&b, &args->ac, args->ac.rt.instance_addr);
-   nir_store_var(&b, vars.instance_addr, nir_pack_64_2x32(&b, instance_addr), 1);
-   nir_store_var(&b, vars.geometry_id_and_flags, ac_nir_load_arg(&b, &args->ac, args->ac.rt.geometry_id_and_flags), 1);
-   nir_store_var(&b, vars.hit_kind, ac_nir_load_arg(&b, &args->ac, args->ac.rt.hit_kind), 1);
-
-   /* guard the shader, so that only the correct invocations execute it */
-   nir_if *shader_guard = NULL;
-   if (shader->info.stage != MESA_SHADER_RAYGEN || resume_shader) {
-      nir_def *uniform_shader_addr = ac_nir_load_arg(&b, &args->ac, args->ac.rt.uniform_shader_addr);
-      uniform_shader_addr = nir_pack_64_2x32(&b, uniform_shader_addr);
-      uniform_shader_addr = nir_ior_imm(&b, uniform_shader_addr, radv_get_rt_priority(shader->info.stage));
 
-      shader_guard = nir_push_if(&b, nir_ieq(&b, uniform_shader_addr, shader_addr));
-      shader_guard->control = nir_selection_control_divergent_always_taken;
-   }
-
-   nir_cf_reinsert(&list, b.cursor);
+   nir_store_var(&b, vars.arg, ac_nir_load_arg(&b, &args->ac, args->ac.rt.payload_offset), 1);
 
-   if (shader_guard)
-      nir_pop_if(&b, shader_guard);
+   store_param_var(&b, vars.shader_record_ptr, shader_record_ptr_arg, 1, 64);
+   store_param_var(&b, vars.accel_struct, accel_struct_arg, 1, 64);
+   store_param_var(&b, vars.cull_mask_and_flags, cull_mask_and_flags_arg, 1, 32);
+   store_param_var(&b, vars.sbt_offset, sbt_offset_arg, 1, 32);
+   store_param_var(&b, vars.sbt_stride, sbt_stride_arg, 1, 32);
+   store_param_var(&b, vars.miss_index, miss_index_arg, 1, 32);
+   store_param_var(&b, vars.origin, ray_origin_arg, 3, 32);
+   store_param_var(&b, vars.tmin, ray_tmin_arg, 1, 32);
+   store_param_var(&b, vars.direction, ray_direction_arg, 3, 32);
+   store_param_var(&b, vars.tmax, ray_tmax_arg, 1, 32);
+   store_param_var(&b, vars.primitive_id, primitive_id_arg, 1, 32);
+   store_param_var(&b, vars.instance_addr, instance_addr_arg, 1, 64);
+   store_param_var(&b, vars.geometry_id_and_flags, geometry_id_and_flags_arg, 1, 32);
+   store_param_var(&b, vars.hit_kind, hit_kind_arg, 1, 32);
 
    b.cursor = nir_after_impl(impl);
 
    if (monolithic) {
       nir_terminate(&b);
-   } else {
-      /* select next shader */
-      shader_addr = nir_load_var(&b, vars.shader_addr);
-      nir_def *next = select_next_shader(&b, shader_addr, info->wave_size);
-      ac_nir_store_arg(&b, &args->ac, args->ac.rt.uniform_shader_addr, next);
-
-      ac_nir_store_arg(&b, &args->ac, args->descriptor_sets[0], descriptor_sets);
-      ac_nir_store_arg(&b, &args->ac, args->ac.push_constants, push_constants);
-      ac_nir_store_arg(&b, &args->ac, args->ac.rt.sbt_descriptors, sbt_descriptors);
-      ac_nir_store_arg(&b, &args->ac, args->ac.rt.traversal_shader_addr, traversal_addr);
-
-      for (uint32_t i = 0; i < ARRAY_SIZE(launch_sizes); i++) {
-         if (rt_info.uses_launch_size)
-            ac_nir_store_arg(&b, &args->ac, args->ac.rt.launch_sizes[i], launch_sizes[i]);
-         else
-            radv_store_arg(&b, args, traversal_info, args->ac.rt.launch_sizes[i], launch_sizes[i]);
-      }
-
-      if (scratch_offset)
-         ac_nir_store_arg(&b, &args->ac, args->ac.scratch_offset, scratch_offset);
-      if (ring_offsets)
-         ac_nir_store_arg(&b, &args->ac, args->ac.ring_offsets, ring_offsets);
-
-      for (uint32_t i = 0; i < ARRAY_SIZE(launch_ids); i++) {
-         if (rt_info.uses_launch_id)
-            ac_nir_store_arg(&b, &args->ac, args->ac.rt.launch_ids[i], launch_ids[i]);
-         else
-            radv_store_arg(&b, args, traversal_info, args->ac.rt.launch_ids[i], launch_ids[i]);
-      }
-
-      /* store back all variables to registers */
-      ac_nir_store_arg(&b, &args->ac, args->ac.rt.dynamic_callable_stack_base, nir_load_var(&b, vars.stack_ptr));
-      ac_nir_store_arg(&b, &args->ac, args->ac.rt.shader_addr, shader_addr);
-      radv_store_arg(&b, args, traversal_info, args->ac.rt.shader_record, nir_load_var(&b, vars.shader_record_ptr));
-      radv_store_arg(&b, args, traversal_info, args->ac.rt.payload_offset, nir_load_var(&b, vars.arg));
-      radv_store_arg(&b, args, traversal_info, args->ac.rt.accel_struct, nir_load_var(&b, vars.accel_struct));
-      radv_store_arg(&b, args, traversal_info, args->ac.rt.cull_mask_and_flags,
-                     nir_load_var(&b, vars.cull_mask_and_flags));
-      radv_store_arg(&b, args, traversal_info, args->ac.rt.sbt_offset, nir_load_var(&b, vars.sbt_offset));
-      radv_store_arg(&b, args, traversal_info, args->ac.rt.sbt_stride, nir_load_var(&b, vars.sbt_stride));
-      radv_store_arg(&b, args, traversal_info, args->ac.rt.miss_index, nir_load_var(&b, vars.miss_index));
-      radv_store_arg(&b, args, traversal_info, args->ac.rt.ray_origin, nir_load_var(&b, vars.origin));
-      radv_store_arg(&b, args, traversal_info, args->ac.rt.ray_tmin, nir_load_var(&b, vars.tmin));
-      radv_store_arg(&b, args, traversal_info, args->ac.rt.ray_direction, nir_load_var(&b, vars.direction));
-      radv_store_arg(&b, args, traversal_info, args->ac.rt.ray_tmax, nir_load_var(&b, vars.tmax));
-
-      radv_store_arg(&b, args, traversal_info, args->ac.rt.primitive_id, nir_load_var(&b, vars.primitive_id));
-      radv_store_arg(&b, args, traversal_info, args->ac.rt.instance_addr, nir_load_var(&b, vars.instance_addr));
-      radv_store_arg(&b, args, traversal_info, args->ac.rt.geometry_id_and_flags,
-                     nir_load_var(&b, vars.geometry_id_and_flags));
-      radv_store_arg(&b, args, traversal_info, args->ac.rt.hit_kind, nir_load_var(&b, vars.hit_kind));
    }
 
    nir_metadata_preserve(impl, nir_metadata_none);
 
    /* cleanup passes */
+   if (!monolithic) {
+      NIR_PASS_V(shader, radv_nir_lower_ray_payload_derefs, 0);
+
+      b.cursor = nir_before_impl(impl);
+      nir_deref_instr **payload_in_storage =
+         rzalloc_array_size(shader, sizeof(nir_deref_instr *), DIV_ROUND_UP(payload_size, 4));
+      if (in_payload_base_arg != -1u) {
+         for (unsigned i = 0; i < DIV_ROUND_UP(payload_size, 4); ++i) {
+            payload_in_storage[i] = nir_build_deref_cast(&b, nir_load_param(&b, in_payload_base_arg + i),
+                                                         nir_var_shader_call_data, glsl_uint_type(), 4);
+         }
+      }
+      NIR_PASS_V(shader, lower_rt_storage, NULL, payload_in_storage, vars.out_payload_storage, info->wave_size);
+
+      nir_remove_dead_derefs(shader);
+      nir_remove_dead_variables(shader, nir_var_function_temp | nir_var_shader_call_data, NULL);
+   }
    NIR_PASS_V(shader, nir_lower_global_vars_to_local);
    NIR_PASS_V(shader, nir_lower_vars_to_ssa);
-   if (shader->info.stage == MESA_SHADER_CLOSEST_HIT || shader->info.stage == MESA_SHADER_INTERSECTION)
-      NIR_PASS_V(shader, lower_hit_attribs, NULL, info->wave_size);
 }
 
 static bool
diff --git a/src/amd/vulkan/radv_pipeline_rt.c b/src/amd/vulkan/radv_pipeline_rt.c
index 6757979b8dc3a..13e5259cbd8a1 100644
--- a/src/amd/vulkan/radv_pipeline_rt.c
+++ b/src/amd/vulkan/radv_pipeline_rt.c
@@ -349,7 +349,6 @@ radv_rt_nir_to_asm(struct radv_device *device, struct vk_pipeline_cache *cache,
                    const struct radv_ray_tracing_stage_info *traversal_stage_info,
                    struct radv_serialized_shader_arena_block *replay_block, struct radv_shader **out_shader)
 {
-   struct radv_physical_device *pdev = radv_device_physical(device);
    struct radv_shader_binary *binary;
    bool keep_executable_info = radv_pipeline_capture_shaders(device, pipeline->base.base.create_flags);
    bool keep_statistic_info = radv_pipeline_capture_shader_stats(device, pipeline->base.base.create_flags);
@@ -368,67 +367,28 @@ radv_rt_nir_to_asm(struct radv_device *device, struct vk_pipeline_cache *cache,
    stage->info.user_sgprs_locs = stage->args.user_sgprs_locs;
    stage->info.inline_push_constant_mask = stage->args.ac.inline_push_const_mask;
 
-   /* Move ray tracing system values to the top that are set by rt_trace_ray
-    * to prevent them from being overwritten by other rt_trace_ray calls.
-    */
-   NIR_PASS_V(stage->nir, move_rt_instructions);
-
-   uint32_t num_resume_shaders = 0;
-   nir_shader **resume_shaders = NULL;
-
-   if (stage->stage != MESA_SHADER_INTERSECTION && !monolithic) {
-      nir_builder b = nir_builder_at(nir_after_impl(nir_shader_get_entrypoint(stage->nir)));
-      nir_rt_return_amd(&b);
-
-      const nir_lower_shader_calls_options opts = {
-         .address_format = nir_address_format_32bit_offset,
-         .stack_alignment = 16,
-         .localized_loads = true,
-         .vectorizer_callback = ac_nir_mem_vectorize_callback,
-         .vectorizer_data = &pdev->info.gfx_level,
-      };
-      nir_lower_shader_calls(stage->nir, &opts, &resume_shaders, &num_resume_shaders, stage->nir);
-   }
-
-   unsigned num_shaders = num_resume_shaders + 1;
-   nir_shader **shaders = ralloc_array(stage->nir, nir_shader *, num_shaders);
-   if (!shaders)
-      return VK_ERROR_OUT_OF_HOST_MEMORY;
-
-   shaders[0] = stage->nir;
-   for (uint32_t i = 0; i < num_resume_shaders; i++)
-      shaders[i + 1] = resume_shaders[i];
-
    if (stage_info)
       memset(stage_info->unused_args, 0xFF, sizeof(stage_info->unused_args));
 
    /* Postprocess shader parts. */
-   for (uint32_t i = 0; i < num_shaders; i++) {
-      struct radv_shader_stage temp_stage = *stage;
-      temp_stage.nir = shaders[i];
-      radv_nir_lower_rt_abi(temp_stage.nir, pCreateInfo, &temp_stage.args, &stage->info, stack_size, i > 0, device,
-                            pipeline, monolithic, traversal_stage_info);
+   radv_nir_lower_rt_abi(stage->nir, pCreateInfo, &stage->args, &stage->info, *payload_size, stack_size, device,
+                         pipeline, monolithic);
 
-      /* Info might be out-of-date after inlining in radv_nir_lower_rt_abi(). */
-      nir_shader_gather_info(temp_stage.nir, radv_get_rt_shader_entrypoint(temp_stage.nir));
+   /* Info might be out-of-date after inlining in radv_nir_lower_rt_abi(). */
+   nir_shader_gather_info(stage->nir, radv_get_rt_shader_entrypoint(stage->nir));
 
-      radv_optimize_nir(temp_stage.nir, stage->key.optimisations_disabled);
-      radv_postprocess_nir(device, NULL, &temp_stage);
+   radv_optimize_nir(stage->nir, stage->key.optimisations_disabled);
+   radv_postprocess_nir(device, NULL, stage);
 
-      if (stage_info)
-         radv_gather_unused_args(stage_info, shaders[i]);
-
-      if (radv_can_dump_shader(device, temp_stage.nir, false))
-         nir_print_shader(temp_stage.nir, stderr);
-   }
+   if (radv_can_dump_shader(device, stage->nir, false))
+      nir_print_shader(stage->nir, stderr);
 
-   bool dump_shader = radv_can_dump_shader(device, shaders[0], false);
+   bool dump_shader = radv_can_dump_shader(device, stage->nir, false);
    bool replayable =
       pipeline->base.base.create_flags & VK_PIPELINE_CREATE_2_RAY_TRACING_SHADER_GROUP_HANDLE_CAPTURE_REPLAY_BIT_KHR;
 
    /* Compile NIR shader to AMD assembly. */
-   binary =
-      radv_shader_nir_to_asm(device, stage, shaders, num_shaders, NULL, keep_executable_info, keep_statistic_info);
+   binary = radv_shader_nir_to_asm(device, stage, &stage->nir, 1, NULL, keep_executable_info, keep_statistic_info);
    struct radv_shader *shader;
    if (replay_block || replayable) {
       VkResult result = radv_shader_create_uncached(device, binary, replayable, replay_block, &shader);
@@ -443,7 +403,7 @@ radv_rt_nir_to_asm(struct radv_device *device, struct vk_pipeline_cache *cache,
       if (stack_size)
          *stack_size += DIV_ROUND_UP(shader->config.scratch_bytes_per_wave, shader->info.wave_size);
 
-      radv_shader_generate_debug_info(device, dump_shader, keep_executable_info, binary, shader, shaders, num_shaders,
+      radv_shader_generate_debug_info(device, dump_shader, keep_executable_info, binary, shader, &stage->nir, 1,
                                       &stage->info);
 
       if (shader && keep_executable_info && stage->spirv.size) {
diff --git a/src/amd/vulkan/radv_shader.h b/src/amd/vulkan/radv_shader.h
index 569521674b5b5..209ff9be8daed 100644
--- a/src/amd/vulkan/radv_shader.h
+++ b/src/amd/vulkan/radv_shader.h
@@ -509,11 +509,10 @@ struct radv_ray_tracing_stage_info;
 
 void radv_nir_lower_rt_abi(nir_shader *shader, const VkRayTracingPipelineCreateInfoKHR *pCreateInfo,
                            const struct radv_shader_args *args, const struct radv_shader_info *info,
-                           uint32_t *stack_size, bool resume_shader, uint32_t payload_size, struct radv_device *device,
-                           struct radv_ray_tracing_pipeline *pipeline, bool monolithic,
-                           const struct radv_ray_tracing_stage_info *traversal_info);
 
-void radv_gather_unused_args(struct radv_ray_tracing_stage_info *info, nir_shader *nir);
+                           void radv_gather_unused_args(struct radv_ray_tracing_stage_info *info, nir_shader *nir);
+                           uint32_t payload_size, uint32_t *stack_size, struct radv_device *device,
+                           struct radv_ray_tracing_pipeline *pipeline, bool monolithic);
 
 struct radv_shader_stage;
 
diff --git a/src/amd/vulkan/radv_shader_args.c b/src/amd/vulkan/radv_shader_args.c
index d5617ceda29e3..8db26e424003e 100644
--- a/src/amd/vulkan/radv_shader_args.c
+++ b/src/amd/vulkan/radv_shader_args.c
@@ -313,7 +313,7 @@ radv_init_shader_args(const struct radv_device *device, gl_shader_stage stage, s
 void
 radv_declare_rt_shader_args(enum amd_gfx_level gfx_level, struct radv_shader_args *args)
 {
-   add_ud_arg(args, 2, AC_ARG_CONST_PTR, &args->ac.rt.uniform_shader_addr, AC_UD_SCRATCH_RING_OFFSETS);
+   add_ud_arg(args, 2, AC_ARG_CONST_PTR, &args->ac.ring_offsets, AC_UD_SCRATCH_RING_OFFSETS);
    add_ud_arg(args, 1, AC_ARG_CONST_PTR_PTR, &args->descriptor_sets[0], AC_UD_INDIRECT_DESCRIPTOR_SETS);
    ac_add_arg(&args->ac, AC_ARG_SGPR, 1, AC_ARG_CONST_PTR, &args->ac.push_constants);
    ac_add_arg(&args->ac, AC_ARG_SGPR, 2, AC_ARG_CONST_DESC_PTR, &args->ac.rt.sbt_descriptors);
@@ -331,25 +331,8 @@ radv_declare_rt_shader_args(enum amd_gfx_level gfx_level, struct radv_shader_arg
       ac_add_arg(&args->ac, AC_ARG_VGPR, 1, AC_ARG_INT, &args->ac.rt.launch_ids[i]);
 
    ac_add_arg(&args->ac, AC_ARG_VGPR, 1, AC_ARG_INT, &args->ac.rt.dynamic_callable_stack_base);
-   ac_add_arg(&args->ac, AC_ARG_VGPR, 2, AC_ARG_CONST_PTR, &args->ac.rt.shader_addr);
-   ac_add_arg(&args->ac, AC_ARG_VGPR, 2, AC_ARG_CONST_PTR, &args->ac.rt.shader_record);
 
    ac_add_arg(&args->ac, AC_ARG_VGPR, 1, AC_ARG_INT, &args->ac.rt.payload_offset);
-   ac_add_arg(&args->ac, AC_ARG_VGPR, 3, AC_ARG_FLOAT, &args->ac.rt.ray_origin);
-   ac_add_arg(&args->ac, AC_ARG_VGPR, 3, AC_ARG_FLOAT, &args->ac.rt.ray_direction);
-   ac_add_arg(&args->ac, AC_ARG_VGPR, 1, AC_ARG_FLOAT, &args->ac.rt.ray_tmin);
-   ac_add_arg(&args->ac, AC_ARG_VGPR, 1, AC_ARG_FLOAT, &args->ac.rt.ray_tmax);
-   ac_add_arg(&args->ac, AC_ARG_VGPR, 1, AC_ARG_INT, &args->ac.rt.cull_mask_and_flags);
-
-   ac_add_arg(&args->ac, AC_ARG_VGPR, 2, AC_ARG_CONST_PTR, &args->ac.rt.accel_struct);
-   ac_add_arg(&args->ac, AC_ARG_VGPR, 1, AC_ARG_INT, &args->ac.rt.sbt_offset);
-   ac_add_arg(&args->ac, AC_ARG_VGPR, 1, AC_ARG_INT, &args->ac.rt.sbt_stride);
-   ac_add_arg(&args->ac, AC_ARG_VGPR, 1, AC_ARG_INT, &args->ac.rt.miss_index);
-
-   ac_add_arg(&args->ac, AC_ARG_VGPR, 2, AC_ARG_CONST_PTR, &args->ac.rt.instance_addr);
-   ac_add_arg(&args->ac, AC_ARG_VGPR, 1, AC_ARG_INT, &args->ac.rt.primitive_id);
-   ac_add_arg(&args->ac, AC_ARG_VGPR, 1, AC_ARG_INT, &args->ac.rt.geometry_id_and_flags);
-   ac_add_arg(&args->ac, AC_ARG_VGPR, 1, AC_ARG_INT, &args->ac.rt.hit_kind);
 }
 
 static bool
-- 
GitLab


From fd58873672c7121dfcd70d3e4bfa7093cf97b2d1 Mon Sep 17 00:00:00 2001
From: Friedrich Vock <friedrich.vock@gmx.de>
Date: Wed, 5 Jun 2024 12:22:46 +0200
Subject: [PATCH 22/71] radv/rt: Remove radv_gather_unused_args

Not needed anymore.
---
 src/amd/vulkan/nir/radv_nir_rt_shader.c | 47 -------------------------
 src/amd/vulkan/radv_shader.h            |  2 --
 2 files changed, 49 deletions(-)

diff --git a/src/amd/vulkan/nir/radv_nir_rt_shader.c b/src/amd/vulkan/nir/radv_nir_rt_shader.c
index a054e61334076..b8072d85f248e 100644
--- a/src/amd/vulkan/nir/radv_nir_rt_shader.c
+++ b/src/amd/vulkan/nir/radv_nir_rt_shader.c
@@ -2405,50 +2405,3 @@ radv_nir_lower_rt_abi(nir_shader *shader, const VkRayTracingPipelineCreateInfoKH
    NIR_PASS_V(shader, nir_lower_global_vars_to_local);
    NIR_PASS_V(shader, nir_lower_vars_to_ssa);
 }
-
-static bool
-radv_arg_def_is_unused(nir_def *def)
-{
-   nir_foreach_use (use, def) {
-      nir_instr *use_instr = nir_src_parent_instr(use);
-      if (use_instr->type == nir_instr_type_intrinsic) {
-         nir_intrinsic_instr *use_intr = nir_instr_as_intrinsic(use_instr);
-         if (use_intr->intrinsic == nir_intrinsic_store_scalar_arg_amd ||
-             use_intr->intrinsic == nir_intrinsic_store_vector_arg_amd)
-            continue;
-      } else if (use_instr->type == nir_instr_type_phi) {
-         nir_cf_node *prev_node = nir_cf_node_prev(&use_instr->block->cf_node);
-         if (!prev_node)
-            return false;
-
-         nir_phi_instr *phi = nir_instr_as_phi(use_instr);
-         if (radv_arg_def_is_unused(&phi->def))
-            continue;
-      }
-
-      return false;
-   }
-
-   return true;
-}
-
-static bool
-radv_gather_unused_args_instr(nir_builder *b, nir_intrinsic_instr *instr, void *data)
-{
-   if (instr->intrinsic != nir_intrinsic_load_scalar_arg_amd && instr->intrinsic != nir_intrinsic_load_vector_arg_amd)
-      return false;
-
-   if (!radv_arg_def_is_unused(&instr->def)) {
-      /* This arg is used for more than passing data to the next stage. */
-      struct radv_ray_tracing_stage_info *info = data;
-      BITSET_CLEAR(info->unused_args, nir_intrinsic_base(instr));
-   }
-
-   return false;
-}
-
-void
-radv_gather_unused_args(struct radv_ray_tracing_stage_info *info, nir_shader *nir)
-{
-   nir_shader_intrinsics_pass(nir, radv_gather_unused_args_instr, nir_metadata_all, info);
-}
diff --git a/src/amd/vulkan/radv_shader.h b/src/amd/vulkan/radv_shader.h
index 209ff9be8daed..b2e09d6e86a0f 100644
--- a/src/amd/vulkan/radv_shader.h
+++ b/src/amd/vulkan/radv_shader.h
@@ -509,8 +509,6 @@ struct radv_ray_tracing_stage_info;
 
 void radv_nir_lower_rt_abi(nir_shader *shader, const VkRayTracingPipelineCreateInfoKHR *pCreateInfo,
                            const struct radv_shader_args *args, const struct radv_shader_info *info,
-
-                           void radv_gather_unused_args(struct radv_ray_tracing_stage_info *info, nir_shader *nir);
                            uint32_t payload_size, uint32_t *stack_size, struct radv_device *device,
                            struct radv_ray_tracing_pipeline *pipeline, bool monolithic);
 
-- 
GitLab


From 365247b5fcd543cbdcedac1cdb24317cf31f5186 Mon Sep 17 00:00:00 2001
From: Friedrich Vock <friedrich.vock@gmx.de>
Date: Sat, 4 May 2024 17:51:17 +0200
Subject: [PATCH 23/71] radv/rt: make radv_nir_init_rt_function_params public

---
 src/amd/vulkan/nir/radv_nir_rt_shader.c | 12 ++++++------
 src/amd/vulkan/radv_shader.h            |  1 +
 2 files changed, 7 insertions(+), 6 deletions(-)

diff --git a/src/amd/vulkan/nir/radv_nir_rt_shader.c b/src/amd/vulkan/nir/radv_nir_rt_shader.c
index b8072d85f248e..a8ba33946c171 100644
--- a/src/amd/vulkan/nir/radv_nir_rt_shader.c
+++ b/src/amd/vulkan/nir/radv_nir_rt_shader.c
@@ -260,8 +260,8 @@ enum radv_nir_chit_miss_function_arg {
    CHIT_MISS_ARG_PAYLOAD_BASE,
 };
 
-static void
-radv_nir_init_function_params(nir_function *function, gl_shader_stage stage, unsigned payload_size)
+void
+radv_nir_init_rt_function_params(nir_function *function, gl_shader_stage stage, unsigned payload_size)
 {
    unsigned payload_base = -1u;
 
@@ -554,13 +554,13 @@ create_rt_variables(nir_shader *shader, struct radv_device *device, const VkPipe
    }
 
    nir_function *trace_ray_func = nir_function_create(shader, "trace_ray_func");
-   radv_nir_init_function_params(trace_ray_func, MESA_SHADER_INTERSECTION, max_payload_size);
+   radv_nir_init_rt_function_params(trace_ray_func, MESA_SHADER_INTERSECTION, max_payload_size);
    vars.trace_ray_func = trace_ray_func;
    nir_function *chit_miss_func = nir_function_create(shader, "chit_miss_func");
-   radv_nir_init_function_params(chit_miss_func, MESA_SHADER_CLOSEST_HIT, max_payload_size);
+   radv_nir_init_rt_function_params(chit_miss_func, MESA_SHADER_CLOSEST_HIT, max_payload_size);
    vars.chit_miss_func = chit_miss_func;
    nir_function *callable_func = nir_function_create(shader, "callable_func");
-   radv_nir_init_function_params(callable_func, MESA_SHADER_CALLABLE, max_payload_size);
+   radv_nir_init_rt_function_params(callable_func, MESA_SHADER_CALLABLE, max_payload_size);
    vars.callable_func = callable_func;
    return vars;
 }
@@ -2260,7 +2260,7 @@ radv_nir_lower_rt_abi(nir_shader *shader, const VkRayTracingPipelineCreateInfoKH
    nir_function_impl *impl = nir_shader_get_entrypoint(shader);
    nir_function *entrypoint_function = impl->function;
 
-   radv_nir_init_function_params(entrypoint_function, shader->info.stage, payload_size);
+   radv_nir_init_rt_function_params(entrypoint_function, shader->info.stage, payload_size);
 
    const VkPipelineCreateFlagBits2KHR create_flags = vk_rt_pipeline_create_flags(pCreateInfo);
 
diff --git a/src/amd/vulkan/radv_shader.h b/src/amd/vulkan/radv_shader.h
index b2e09d6e86a0f..1db253e0764a3 100644
--- a/src/amd/vulkan/radv_shader.h
+++ b/src/amd/vulkan/radv_shader.h
@@ -507,6 +507,7 @@ void radv_nir_lower_rt_io(nir_shader *shader, bool monolithic, uint32_t payload_
 
 struct radv_ray_tracing_stage_info;
 
+void radv_nir_init_rt_function_params(nir_function *function, gl_shader_stage stage, unsigned payload_size);
 void radv_nir_lower_rt_abi(nir_shader *shader, const VkRayTracingPipelineCreateInfoKHR *pCreateInfo,
                            const struct radv_shader_args *args, const struct radv_shader_info *info,
                            uint32_t payload_size, uint32_t *stack_size, struct radv_device *device,
-- 
GitLab


From db076fbd5b23e73eb60ed886f123135446036c0e Mon Sep 17 00:00:00 2001
From: Friedrich Vock <friedrich.vock@gmx.de>
Date: Fri, 3 May 2024 17:36:43 +0200
Subject: [PATCH 24/71] radv: Use call optimization

---
 src/amd/vulkan/radv_pipeline.c | 24 ++++++++++++++++++++++++
 1 file changed, 24 insertions(+)

diff --git a/src/amd/vulkan/radv_pipeline.c b/src/amd/vulkan/radv_pipeline.c
index a4de95225f959..e951fd39cbd98 100644
--- a/src/amd/vulkan/radv_pipeline.c
+++ b/src/amd/vulkan/radv_pipeline.c
@@ -365,6 +365,27 @@ non_uniform_access_callback(const nir_src *src, void *_)
    return nir_chase_binding(*src).success ? 0x2 : 0x3;
 }
 
+static bool
+radv_rt_can_remat_intrinsic(nir_instr *instr)
+{
+   nir_intrinsic_instr *intrin = nir_instr_as_intrinsic(instr);
+   switch (intrin->intrinsic) {
+   case nir_intrinsic_load_ray_launch_id:
+   case nir_intrinsic_load_ray_launch_size:
+   case nir_intrinsic_vulkan_resource_index:
+   case nir_intrinsic_vulkan_resource_reindex:
+   case nir_intrinsic_load_vulkan_descriptor:
+   case nir_intrinsic_load_push_constant:
+   case nir_intrinsic_load_global_constant:
+   case nir_intrinsic_load_smem_amd:
+   case nir_intrinsic_load_scalar_arg_amd:
+   case nir_intrinsic_load_vector_arg_amd:
+      return true;
+   default:
+      return false;
+   }
+}
+
 void
 radv_postprocess_nir(struct radv_device *device, const struct radv_graphics_state_key *gfx_state,
                      struct radv_shader_stage *stage)
@@ -656,6 +677,9 @@ radv_postprocess_nir(struct radv_device *device, const struct radv_graphics_stat
        * spilling.
        */
       NIR_PASS(_, stage->nir, nir_opt_move, nir_move_comparisons);
+
+      struct nir_minimize_call_live_states_options live_opts = {.can_remat = radv_rt_can_remat_intrinsic};
+      NIR_PASS(_, stage->nir, nir_minimize_call_live_states, &live_opts);
    }
 }
 
-- 
GitLab


From 0ca2d27b34b7cdd95c99266becd9fb5f84d46702 Mon Sep 17 00:00:00 2001
From: Friedrich Vock <friedrich.vock@gmx.de>
Date: Tue, 27 Feb 2024 11:31:02 +0100
Subject: [PATCH 25/71] aco: add and use get_hwreg_imm helper

---
 src/amd/compiler/aco_ir.h                  | 20 ++++++++++++++++++++
 src/amd/compiler/aco_lower_to_hw_instr.cpp |  8 ++++----
 2 files changed, 24 insertions(+), 4 deletions(-)

diff --git a/src/amd/compiler/aco_ir.h b/src/amd/compiler/aco_ir.h
index 2b68f47a8b993..71089410b6220 100644
--- a/src/amd/compiler/aco_ir.h
+++ b/src/amd/compiler/aco_ir.h
@@ -269,6 +269,26 @@ withoutVOP3(Format format)
    return (Format)((uint32_t)format & ~((uint32_t)Format::VOP3));
 }
 
+enum class HwReg {
+   MODE = 1,
+   STATUS = 2,
+   TRAPSTS = 3,
+   FLUSH_IB = 14,
+   SH_MEM_BASES = 15,
+   FLAT_SCRATCH_LO = 20,
+   FLAT_SCRATCH_HI = 21,
+   HW_ID1 = 23,
+   HW_ID2 = 24,
+   SHADER_CYCLES = 29,
+};
+
+/* Construct imm16 for s_getreg/s_setreg. */
+constexpr uint16_t
+get_hwreg_imm(HwReg reg, uint8_t bit_offset = 0, uint8_t bit_size = 32)
+{
+   return ((uint16_t)reg & 0x3f) | (bit_offset & 0x1f) << 6 | ((bit_size - 1) & 0x1f) << 11;
+}
+
 enum class RegType {
    sgpr,
    vgpr,
diff --git a/src/amd/compiler/aco_lower_to_hw_instr.cpp b/src/amd/compiler/aco_lower_to_hw_instr.cpp
index 966846aedd837..5bc8508b40ab7 100644
--- a/src/amd/compiler/aco_lower_to_hw_instr.cpp
+++ b/src/amd/compiler/aco_lower_to_hw_instr.cpp
@@ -2107,10 +2107,10 @@ hw_init_scratch(Builder& bld, Definition def, Operand scratch_addr, Operand scra
       bld.sop2(aco_opcode::s_addc_u32, Definition(scratch_hi, s1), Definition(scc, s1),
                scratch_addr_hi, hi_add, Operand(scc, s1));
 
-      /* "((size - 1) << 11) | register" (FLAT_SCRATCH_LO/HI is encoded as register
-       * 20/21) */
-      bld.sopk(aco_opcode::s_setreg_b32, Operand(scratch_lo, s1), (31 << 11) | 20);
-      bld.sopk(aco_opcode::s_setreg_b32, Operand(scratch_hi, s1), (31 << 11) | 21);
+      bld.sopk(aco_opcode::s_setreg_b32, Operand(scratch_lo, s1),
+               get_hwreg_imm(HwReg::FLAT_SCRATCH_LO));
+      bld.sopk(aco_opcode::s_setreg_b32, Operand(scratch_hi, s1),
+               get_hwreg_imm(HwReg::FLAT_SCRATCH_HI));
    } else {
       bld.sop2(aco_opcode::s_add_u32, Definition(flat_scr_lo, s1), Definition(scc, s1),
                scratch_addr_lo, scratch_offset);
-- 
GitLab


From a909d98ba889f6ee580164353030bfb81e7b9cca Mon Sep 17 00:00:00 2001
From: Friedrich Vock <friedrich.vock@gmx.de>
Date: Thu, 28 Dec 2023 20:03:05 +0100
Subject: [PATCH 26/71] aco: Add ABI and Pseudo CALL format

---
 src/amd/compiler/aco_builder_h.py             |  29 +++
 .../compiler/aco_instruction_selection.cpp    |  30 +++
 src/amd/compiler/aco_ir.cpp                   |   1 +
 src/amd/compiler/aco_ir.h                     | 230 +++++++++++++++++-
 src/amd/compiler/aco_opcodes.py               |   7 +-
 src/amd/compiler/aco_register_allocation.cpp  |  71 ------
 6 files changed, 294 insertions(+), 74 deletions(-)

diff --git a/src/amd/compiler/aco_builder_h.py b/src/amd/compiler/aco_builder_h.py
index 608949811b8b0..0c930037c36f9 100644
--- a/src/amd/compiler/aco_builder_h.py
+++ b/src/amd/compiler/aco_builder_h.py
@@ -564,6 +564,7 @@ formats = [("pseudo", [Format.PSEUDO], list(itertools.product(range(5), range(6)
            ("branch", [Format.PSEUDO_BRANCH], itertools.product([1], [0, 1])),
            ("barrier", [Format.PSEUDO_BARRIER], [(0, 0)]),
            ("reduction", [Format.PSEUDO_REDUCTION], [(3, 3)]),
+           ("call", [Format.PSEUDO_CALL], [(0, 0)]),
            ("vop1", [Format.VOP1], [(0, 0), (1, 1), (2, 2)]),
            ("vop1_sdwa", [Format.VOP1, Format.SDWA], [(1, 1)]),
            ("vop2", [Format.VOP2], itertools.product([1, 2], [2, 3])),
@@ -600,6 +601,7 @@ formats = [("pseudo", [Format.PSEUDO], list(itertools.product(range(5), range(6)
 formats = [(f if len(f) == 5 else f + ('',)) for f in formats]
 %>\\
 % for name, formats, shapes, extra_field_setup in formats:
+    % if shapes:
     % for num_definitions, num_operands in shapes:
         <%
         args = ['aco_opcode opcode']
@@ -649,6 +651,33 @@ formats = [(f if len(f) == 5 else f + ('',)) for f in formats]
 
     % endif
     % endfor
+% else:
+        <%
+        args = ['aco_opcode opcode', 'aco::span<Definition> definitions', 'aco::span<Operand> operands' ]
+        for f in formats:
+            args += f.get_builder_field_decls()
+        %>\\
+
+   Result ${name}(${', '.join(args)})
+   {
+      ${struct} *instr = create_instruction<${struct}>(opcode, (Format)(${'|'.join('(int)Format::%s' % f.name for f in formats)}), operands.size(), definitions.size());
+      for (unsigned i = 0; i < definitions.size(); ++i) {
+         instr->definitions[i] = definitions[i];
+         instr->definitions[i].setPrecise(is_precise);
+         instr->definitions[i].setNUW(is_nuw);
+      }
+      for (unsigned i = 0; i < operands.size(); ++i)
+         instr->operands[i] = operands[i];
+        % for f in formats:
+            % for dest, field_name in zip(f.get_builder_field_dests(), f.get_builder_field_names()):
+      instr->${dest} = ${field_name};
+            % endfor
+            ${f.get_builder_initialization(num_operands)}
+        % endfor
+       ${extra_field_setup}
+      return insert(instr);
+   }
+% endif
 % endfor
 };
 
diff --git a/src/amd/compiler/aco_instruction_selection.cpp b/src/amd/compiler/aco_instruction_selection.cpp
index 6fbb6f08c6bf4..5850b6b032973 100644
--- a/src/amd/compiler/aco_instruction_selection.cpp
+++ b/src/amd/compiler/aco_instruction_selection.cpp
@@ -10234,6 +10234,36 @@ visit_jump(isel_context* ctx, nir_jump_instr* instr)
    }
 }
 
+ABI
+make_abi(const ABI& base, const ac_shader_args* args, Program* program)
+{
+   ABI abi = base;
+
+   unsigned sgpr_limit = program->dev.sgpr_limit;
+   /* GFX8- needs a scratch_rsrc that we need to keep around somewhere */
+   if (program->gfx_level < GFX9)
+      sgpr_limit -= (align(sgpr_limit, 4) - sgpr_limit) + 4;
+   unsigned vgpr_limit = program->dev.vgpr_limit;
+
+   abi.parameterSpace.vgpr.lo_ =
+      abi.parameterSpace.vgpr.lo_.advance(align(args->num_vgprs_used, 2) * 4);
+   abi.parameterSpace.sgpr.lo_ =
+      abi.parameterSpace.sgpr.lo_.advance(align(args->num_sgprs_used, 2) * 4);
+   abi.clobberedRegs.vgpr.lo_ = abi.clobberedRegs.vgpr.lo_.advance(args->num_vgprs_used * 4);
+   abi.clobberedRegs.sgpr.lo_ = abi.clobberedRegs.sgpr.lo_.advance(args->num_sgprs_used * 4);
+
+   abi.parameterSpace.sgpr.size =
+      std::min(abi.parameterSpace.sgpr.size, sgpr_limit - abi.parameterSpace.sgpr.lo());
+   abi.parameterSpace.vgpr.size =
+      std::min(abi.parameterSpace.vgpr.size, vgpr_limit - (abi.parameterSpace.vgpr.lo() - 256));
+   abi.clobberedRegs.sgpr.size =
+      std::min(abi.clobberedRegs.sgpr.size, sgpr_limit - abi.clobberedRegs.sgpr.lo());
+   abi.clobberedRegs.vgpr.size =
+      std::min(abi.clobberedRegs.vgpr.size, vgpr_limit - (abi.clobberedRegs.vgpr.lo() - 256));
+
+   return abi;
+}
+
 void
 visit_block(isel_context* ctx, nir_block* block)
 {
diff --git a/src/amd/compiler/aco_ir.cpp b/src/amd/compiler/aco_ir.cpp
index 05f0745e6a793..ba7113131b8ec 100644
--- a/src/amd/compiler/aco_ir.cpp
+++ b/src/amd/compiler/aco_ir.cpp
@@ -1485,6 +1485,7 @@ get_instr_data_size(Format format)
    case Format::PSEUDO_BARRIER: return sizeof(Pseudo_barrier_instruction);
    case Format::PSEUDO_REDUCTION: return sizeof(Pseudo_reduction_instruction);
    case Format::PSEUDO_BRANCH: return sizeof(Pseudo_branch_instruction);
+   case Format::PSEUDO_CALL: return sizeof(Pseudo_call_instruction);
    case Format::DS: return sizeof(DS_instruction);
    case Format::FLAT:
    case Format::GLOBAL:
diff --git a/src/amd/compiler/aco_ir.h b/src/amd/compiler/aco_ir.h
index 71089410b6220..7051d4af59e47 100644
--- a/src/amd/compiler/aco_ir.h
+++ b/src/amd/compiler/aco_ir.h
@@ -465,6 +465,215 @@ static constexpr PhysReg exec_hi{127};
 static constexpr PhysReg pops_exiting_wave_id{239}; /* GFX9-GFX10.3 */
 static constexpr PhysReg scc{253};
 
+/* Iterator type for making PhysRegInterval compatible with range-based for */
+struct PhysRegIterator {
+   using difference_type = int;
+   using value_type = unsigned;
+   using reference = const unsigned&;
+   using pointer = const unsigned*;
+   using iterator_category = std::bidirectional_iterator_tag;
+
+   PhysReg reg;
+
+   PhysReg operator*() const { return reg; }
+
+   PhysRegIterator& operator++()
+   {
+      reg.reg_b += 4;
+      return *this;
+   }
+
+   PhysRegIterator& operator--()
+   {
+      reg.reg_b -= 4;
+      return *this;
+   }
+
+   bool operator==(PhysRegIterator oth) const { return reg == oth.reg; }
+
+   bool operator!=(PhysRegIterator oth) const { return reg != oth.reg; }
+
+   bool operator<(PhysRegIterator oth) const { return reg < oth.reg; }
+};
+
+/* Half-open register interval used in "sliding window"-style for-loops */
+struct PhysRegInterval {
+   PhysReg lo_;
+   unsigned size;
+
+   /* Inclusive lower bound */
+   PhysReg lo() const { return lo_; }
+
+   /* Exclusive upper bound */
+   PhysReg hi() const { return PhysReg{lo() + size}; }
+
+   PhysRegInterval& operator+=(uint32_t stride)
+   {
+      lo_ = PhysReg{lo_.reg() + stride};
+      return *this;
+   }
+
+   bool operator!=(const PhysRegInterval& oth) const { return lo_ != oth.lo_ || size != oth.size; }
+
+   /* Construct a half-open interval, excluding the end register */
+   static PhysRegInterval from_until(PhysReg first, PhysReg end) { return {first, end - first}; }
+
+   bool contains(PhysReg reg) const { return lo() <= reg && reg < hi(); }
+
+   bool contains(const PhysRegInterval& needle) const
+   {
+      return needle.lo() >= lo() && needle.hi() <= hi();
+   }
+
+   PhysRegIterator begin() const { return {lo_}; }
+
+   PhysRegIterator end() const { return {PhysReg{lo_ + size}}; }
+};
+
+inline bool
+intersects(const PhysRegInterval& a, const PhysRegInterval& b)
+{
+   return a.hi() > b.lo() && b.hi() > a.lo();
+}
+
+struct GPRInterval {
+   PhysRegInterval sgpr;
+   PhysRegInterval vgpr;
+};
+
+struct ABI {
+   GPRInterval parameterSpace;
+   GPRInterval clobberedRegs;
+
+   bool clobbersVCC;
+   bool clobbersSCC;
+};
+
+static constexpr ABI rtRaygenABI = {
+   .parameterSpace =
+      {
+         .sgpr =
+            {
+               .lo_ = PhysReg(0),
+               .size = 32,
+            },
+         .vgpr =
+            {
+               .lo_ = PhysReg(256),
+               .size = 32,
+            },
+      },
+   .clobberedRegs =
+      {
+         .sgpr =
+            {
+               .lo_ = PhysReg(0),
+               .size = 108,
+            },
+         .vgpr =
+            {
+               .lo_ = PhysReg(256),
+               .size = 128,
+            },
+      },
+   .clobbersVCC = true,
+   .clobbersSCC = true,
+};
+
+static constexpr ABI rtTraversalABI = {
+   .parameterSpace =
+      {
+         .sgpr =
+            {
+               .lo_ = PhysReg(0),
+               .size = 32,
+            },
+         .vgpr =
+            {
+               .lo_ = PhysReg(256),
+               .size = 32,
+            },
+      },
+   .clobberedRegs =
+      {
+         /* TODO: maybe find better values */
+         .sgpr =
+            {
+               .lo_ = PhysReg(0),
+               .size = 108,
+            },
+         .vgpr =
+            {
+               .lo_ = PhysReg(256),
+               .size = 128,
+            },
+      },
+   .clobbersVCC = true,
+   .clobbersSCC = true,
+};
+
+static constexpr ABI rtAnyHitABI = {
+   .parameterSpace =
+      {
+         .sgpr =
+            {
+               .lo_ = PhysReg(0),
+               .size = 32,
+            },
+         .vgpr =
+            {
+               .lo_ = PhysReg(256),
+               .size = 32,
+            },
+      },
+   .clobberedRegs =
+      {
+         .sgpr =
+            {
+               .lo_ = PhysReg(80),
+               .size = 16,
+            },
+         .vgpr =
+            {
+               .lo_ = PhysReg(256 + 80),
+               .size = 32,
+            },
+      },
+   .clobbersVCC = true,
+   .clobbersSCC = true,
+};
+
+static constexpr ABI rtClosestHitMissABI = {
+   .parameterSpace =
+      {
+         .sgpr =
+            {
+               .lo_ = PhysReg(0),
+               .size = 32,
+            },
+         .vgpr =
+            {
+               .lo_ = PhysReg(256),
+               .size = 32,
+            },
+      },
+   .clobberedRegs =
+      {
+         .sgpr =
+            {
+               .lo_ = PhysReg(0),
+               .size = 108,
+            },
+         .vgpr =
+            {
+               .lo_ = PhysReg(256),
+               .size = 128,
+            },
+      },
+   .clobbersVCC = true,
+   .clobbersSCC = true,
+};
+
 /**
  * Operand Class
  * Initially, each Operand refers to either
@@ -1000,6 +1209,7 @@ struct FLAT_instruction;
 struct Pseudo_branch_instruction;
 struct Pseudo_barrier_instruction;
 struct Pseudo_reduction_instruction;
+struct Pseudo_call_instruction;
 struct VALU_instruction;
 struct VINTERP_inreg_instruction;
 struct VINTRP_instruction;
@@ -1197,6 +1407,17 @@ struct Instruction {
       return *(Pseudo_reduction_instruction*)this;
    }
    constexpr bool isReduction() const noexcept { return format == Format::PSEUDO_REDUCTION; }
+   Pseudo_call_instruction& call() noexcept
+   {
+      assert(isCall());
+      return *(Pseudo_call_instruction*)this;
+   }
+   const Pseudo_call_instruction& call() const noexcept
+   {
+      assert(isCall());
+      return *(Pseudo_call_instruction*)this;
+   }
+   constexpr bool isCall() const noexcept { return format == Format::PSEUDO_CALL; }
    constexpr bool isVOP3P() const noexcept { return (uint16_t)format & (uint16_t)Format::VOP3P; }
    VINTERP_inreg_instruction& vinterp_inreg() noexcept
    {
@@ -1688,6 +1909,11 @@ struct Pseudo_reduction_instruction : public Instruction {
 static_assert(sizeof(Pseudo_reduction_instruction) == sizeof(Instruction) + 4,
               "Unexpected padding");
 
+struct Pseudo_call_instruction : public Instruction {
+   ABI abi;
+};
+static_assert(sizeof(Pseudo_call_instruction) == sizeof(Instruction) + 36, "Unexpected padding");
+
 inline bool
 Instruction::accessesLDS() const noexcept
 {
@@ -1760,8 +1986,8 @@ memory_sync_info get_sync_info(const Instruction* instr);
 inline bool
 is_dead(const std::vector<uint16_t>& uses, const Instruction* instr)
 {
-   if (instr->definitions.empty() || instr->isBranch() || instr->opcode == aco_opcode::p_startpgm ||
-       instr->opcode == aco_opcode::p_init_scratch ||
+   if (instr->definitions.empty() || instr->isBranch() || instr->isCall() ||
+       instr->opcode == aco_opcode::p_startpgm || instr->opcode == aco_opcode::p_init_scratch ||
        instr->opcode == aco_opcode::p_dual_src_export_gfx11)
       return false;
 
diff --git a/src/amd/compiler/aco_opcodes.py b/src/amd/compiler/aco_opcodes.py
index 9f23d595d789b..767474ecafc79 100644
--- a/src/amd/compiler/aco_opcodes.py
+++ b/src/amd/compiler/aco_opcodes.py
@@ -50,6 +50,7 @@ class Format(IntEnum):
    PSEUDO_BRANCH = auto()
    PSEUDO_BARRIER = auto()
    PSEUDO_REDUCTION = auto()
+   PSEUDO_CALL = auto()
    # Scalar ALU & Control Formats
    SOP1 = auto()
    SOP2 = auto()
@@ -93,7 +94,7 @@ class Format(IntEnum):
          return "salu"
       elif self in [Format.FLAT, Format.GLOBAL, Format.SCRATCH]:
          return "flatlike"
-      elif self in [Format.PSEUDO_BRANCH, Format.PSEUDO_REDUCTION, Format.PSEUDO_BARRIER]:
+      elif self in [Format.PSEUDO_BRANCH, Format.PSEUDO_REDUCTION, Format.PSEUDO_BARRIER, Format.PSEUDO_CALL]:
          return self.name.split("_")[-1].lower()
       else:
          return self.name.lower()
@@ -171,6 +172,8 @@ class Format(IntEnum):
       elif self == Format.PSEUDO_BARRIER:
          return [('memory_sync_info', 'sync', None),
                  ('sync_scope', 'exec_scope', 'scope_invocation')]
+      elif self == Format.PSEUDO_CALL:
+         return [('ABI', 'abi', None)]
       elif self == Format.VINTRP:
          return [('unsigned', 'attribute', None),
                  ('unsigned', 'component', None),
@@ -359,6 +362,8 @@ insn("p_cbranch_nz", format=Format.PSEUDO_BRANCH)
 
 insn("p_barrier", format=Format.PSEUDO_BARRIER)
 
+insn("p_call", format=Format.PSEUDO_CALL)
+
 # Primitive Ordered Pixel Shading pseudo-instructions.
 
 # For querying whether the current wave can enter the ordered section on GFX9-10.3, doing
diff --git a/src/amd/compiler/aco_register_allocation.cpp b/src/amd/compiler/aco_register_allocation.cpp
index 99f01d22dd504..ee707f70496e0 100644
--- a/src/amd/compiler/aco_register_allocation.cpp
+++ b/src/amd/compiler/aco_register_allocation.cpp
@@ -67,37 +67,6 @@ struct assignment {
    }
 };
 
-/* Iterator type for making PhysRegInterval compatible with range-based for */
-struct PhysRegIterator {
-   using difference_type = int;
-   using value_type = unsigned;
-   using reference = const unsigned&;
-   using pointer = const unsigned*;
-   using iterator_category = std::bidirectional_iterator_tag;
-
-   PhysReg reg;
-
-   PhysReg operator*() const { return reg; }
-
-   PhysRegIterator& operator++()
-   {
-      reg.reg_b += 4;
-      return *this;
-   }
-
-   PhysRegIterator& operator--()
-   {
-      reg.reg_b -= 4;
-      return *this;
-   }
-
-   bool operator==(PhysRegIterator oth) const { return reg == oth.reg; }
-
-   bool operator!=(PhysRegIterator oth) const { return reg != oth.reg; }
-
-   bool operator<(PhysRegIterator oth) const { return reg < oth.reg; }
-};
-
 struct ra_ctx {
 
    Program* program;
@@ -139,46 +108,6 @@ struct ra_ctx {
    }
 };
 
-/* Half-open register interval used in "sliding window"-style for-loops */
-struct PhysRegInterval {
-   PhysReg lo_;
-   unsigned size;
-
-   /* Inclusive lower bound */
-   PhysReg lo() const { return lo_; }
-
-   /* Exclusive upper bound */
-   PhysReg hi() const { return PhysReg{lo() + size}; }
-
-   PhysRegInterval& operator+=(uint32_t stride)
-   {
-      lo_ = PhysReg{lo_.reg() + stride};
-      return *this;
-   }
-
-   bool operator!=(const PhysRegInterval& oth) const { return lo_ != oth.lo_ || size != oth.size; }
-
-   /* Construct a half-open interval, excluding the end register */
-   static PhysRegInterval from_until(PhysReg first, PhysReg end) { return {first, end - first}; }
-
-   bool contains(PhysReg reg) const { return lo() <= reg && reg < hi(); }
-
-   bool contains(const PhysRegInterval& needle) const
-   {
-      return needle.lo() >= lo() && needle.hi() <= hi();
-   }
-
-   PhysRegIterator begin() const { return {lo_}; }
-
-   PhysRegIterator end() const { return {PhysReg{lo_ + size}}; }
-};
-
-bool
-intersects(const PhysRegInterval& a, const PhysRegInterval& b)
-{
-   return a.hi() > b.lo() && b.hi() > a.lo();
-}
-
 /* Gets the stride for full (non-subdword) registers */
 uint32_t
 get_stride(RegClass rc)
-- 
GitLab


From 6836677616e9b7f0f44de42dbbfd2ae799b29df9 Mon Sep 17 00:00:00 2001
From: Friedrich Vock <friedrich.vock@gmx.de>
Date: Tue, 9 Apr 2024 08:08:07 +0200
Subject: [PATCH 27/71] aco: Add pseudo instr to calculate a function callee's
 stack pointer

---
 src/amd/compiler/aco_lower_to_hw_instr.cpp | 14 ++++++++++++++
 src/amd/compiler/aco_opcodes.py            |  2 ++
 2 files changed, 16 insertions(+)

diff --git a/src/amd/compiler/aco_lower_to_hw_instr.cpp b/src/amd/compiler/aco_lower_to_hw_instr.cpp
index 5bc8508b40ab7..968c2ae80bacf 100644
--- a/src/amd/compiler/aco_lower_to_hw_instr.cpp
+++ b/src/amd/compiler/aco_lower_to_hw_instr.cpp
@@ -2781,6 +2781,20 @@ lower_to_hw_instr(Program* program)
                         ((32 - 1) << 11) | shader_cycles_hi);
                break;
             }
+            case aco_opcode::p_callee_stack_ptr: {
+               unsigned caller_stack_size =
+                  ctx.program->config->scratch_bytes_per_wave / ctx.program->wave_size;
+               unsigned scratch_param_size = instr->operands[0].constantValue();
+               unsigned callee_stack_start = caller_stack_size + scratch_param_size;
+               if (ctx.program->gfx_level < GFX9)
+                  callee_stack_start *= ctx.program->wave_size;
+               if (instr->operands.size() < 2)
+                  bld.sop1(aco_opcode::s_mov_b32, instr->definitions[0],
+                           Operand::c32(callee_stack_start));
+               else
+                  bld.sop2(aco_opcode::s_add_u32, instr->definitions[0], Definition(scc, s1),
+                           instr->operands[1], Operand::c32(callee_stack_start));
+            }
             default: break;
             }
          } else if (instr->isBranch()) {
diff --git a/src/amd/compiler/aco_opcodes.py b/src/amd/compiler/aco_opcodes.py
index 767474ecafc79..5a11ea8e8267b 100644
--- a/src/amd/compiler/aco_opcodes.py
+++ b/src/amd/compiler/aco_opcodes.py
@@ -339,6 +339,8 @@ insn("p_boolean_phi")
 insn("p_as_uniform")
 insn("p_unit_test")
 
+insn("p_callee_stack_ptr")
+
 insn("p_create_vector")
 insn("p_extract_vector")
 insn("p_split_vector")
-- 
GitLab


From 918e0a903b54c0f774a1c38da2640f09ea943b20 Mon Sep 17 00:00:00 2001
From: Friedrich Vock <friedrich.vock@gmx.de>
Date: Sat, 1 Jun 2024 11:57:04 +0200
Subject: [PATCH 28/71] aco: Limit rt stages to 128 vgprs

---
 src/amd/compiler/aco_ir.cpp | 2 +-
 1 file changed, 1 insertion(+), 1 deletion(-)

diff --git a/src/amd/compiler/aco_ir.cpp b/src/amd/compiler/aco_ir.cpp
index ba7113131b8ec..20a154875fd28 100644
--- a/src/amd/compiler/aco_ir.cpp
+++ b/src/amd/compiler/aco_ir.cpp
@@ -94,7 +94,7 @@ init_program(Program* program, Stage stage, const struct aco_shader_info* info,
    /* apparently gfx702 also has 16-bank LDS but I can't find a family for that */
    program->dev.has_16bank_lds = family == CHIP_KABINI || family == CHIP_STONEY;
 
-   program->dev.vgpr_limit = 256;
+   program->dev.vgpr_limit = stage == raytracing_cs ? 128 : 256;
    program->dev.physical_vgprs = 256;
    program->dev.vgpr_alloc_granule = 4;
 
-- 
GitLab


From dc5cc28db889ea2e5d74b3a728e825ef24b5f71c Mon Sep 17 00:00:00 2001
From: Friedrich Vock <friedrich.vock@gmx.de>
Date: Mon, 22 Apr 2024 06:50:54 +0200
Subject: [PATCH 29/71] aco: Add scratch stack pointer

Function callees shouldn't overwrite caller's stacks.
Track where to write scratch data with a stack pointer.
---
 src/amd/compiler/aco_ir.h            | 1 +
 src/amd/compiler/aco_reindex_ssa.cpp | 1 +
 2 files changed, 2 insertions(+)

diff --git a/src/amd/compiler/aco_ir.h b/src/amd/compiler/aco_ir.h
index 7051d4af59e47..cc7bec8d766b2 100644
--- a/src/amd/compiler/aco_ir.h
+++ b/src/amd/compiler/aco_ir.h
@@ -2305,6 +2305,7 @@ public:
    std::vector<uint8_t> constant_data;
    Temp private_segment_buffer;
    Temp scratch_offset;
+   Temp stack_ptr = {};
 
    uint16_t num_waves = 0;
    uint16_t min_waves = 0;
diff --git a/src/amd/compiler/aco_reindex_ssa.cpp b/src/amd/compiler/aco_reindex_ssa.cpp
index 15b9687349431..eabfbd792d161 100644
--- a/src/amd/compiler/aco_reindex_ssa.cpp
+++ b/src/amd/compiler/aco_reindex_ssa.cpp
@@ -73,6 +73,7 @@ reindex_program(idx_ctx& ctx, Program* program)
                                           program->private_segment_buffer.regClass());
    program->scratch_offset =
       Temp(ctx.renames[program->scratch_offset.id()], program->scratch_offset.regClass());
+   program->stack_ptr = Temp(ctx.renames[program->stack_ptr.id()], program->stack_ptr.regClass());
    program->temp_rc = ctx.temp_rc;
 }
 
-- 
GitLab


From 30b303d0e2b0ae1cfd84225997d07dc6a73ca383 Mon Sep 17 00:00:00 2001
From: Friedrich Vock <friedrich.vock@gmx.de>
Date: Mon, 22 Apr 2024 06:51:10 +0200
Subject: [PATCH 30/71] aco/spill: Use scratch stack pointer

---
 src/amd/compiler/aco_spill.cpp | 7 ++++++-
 1 file changed, 6 insertions(+), 1 deletion(-)

diff --git a/src/amd/compiler/aco_spill.cpp b/src/amd/compiler/aco_spill.cpp
index 172addd3c22af..d6f57c33c70c0 100644
--- a/src/amd/compiler/aco_spill.cpp
+++ b/src/amd/compiler/aco_spill.cpp
@@ -1321,7 +1321,12 @@ setup_vgpr_spill_reload(spill_ctx& ctx, Block& block,
          }
 
          /* GFX9+ uses scratch_* instructions, which don't use a resource. */
-         ctx.scratch_rsrc = offset_bld.copy(offset_bld.def(s1), Operand::c32(saddr));
+         if (ctx.program->stack_ptr.id())
+            ctx.scratch_rsrc =
+               offset_bld.sop2(aco_opcode::s_add_u32, offset_bld.def(s1), Definition(scc, s1),
+                               Operand(ctx.program->stack_ptr), Operand::c32(saddr));
+         else
+            ctx.scratch_rsrc = offset_bld.copy(offset_bld.def(s1), Operand::c32(saddr));
       }
    } else {
       if (ctx.scratch_rsrc == Temp())
-- 
GitLab


From f714642506fb4637130b5f759a3e0f28dcbf58a3 Mon Sep 17 00:00:00 2001
From: Friedrich Vock <friedrich.vock@gmx.de>
Date: Sat, 4 May 2024 16:01:59 +0200
Subject: [PATCH 31/71] nir: Allow forward-declaring nir_parameter

---
 src/compiler/nir/nir.h | 2 +-
 1 file changed, 1 insertion(+), 1 deletion(-)

diff --git a/src/compiler/nir/nir.h b/src/compiler/nir/nir.h
index b3120ad1a8355..09547f265532d 100644
--- a/src/compiler/nir/nir.h
+++ b/src/compiler/nir/nir.h
@@ -3536,7 +3536,7 @@ nir_cf_list_is_empty_block(struct exec_list *cf_list)
    return false;
 }
 
-typedef struct {
+typedef struct nir_parameter {
    uint8_t num_components;
    uint8_t bit_size;
    /* True if this parameter is a deref used for returning values */
-- 
GitLab


From cb325848d07b87cbce5934949907522dc7ff7ac5 Mon Sep 17 00:00:00 2001
From: Friedrich Vock <friedrich.vock@gmx.de>
Date: Wed, 6 Mar 2024 13:27:17 +0100
Subject: [PATCH 32/71] aco: Add call info

---
 .../compiler/aco_instruction_selection.cpp    | 53 +++++++++++++++++++
 src/amd/compiler/aco_instruction_selection.h  | 31 +++++++++++
 .../aco_instruction_selection_setup.cpp       |  8 +++
 src/amd/compiler/aco_ir.h                     |  4 ++
 4 files changed, 96 insertions(+)

diff --git a/src/amd/compiler/aco_instruction_selection.cpp b/src/amd/compiler/aco_instruction_selection.cpp
index 5850b6b032973..60452013846f0 100644
--- a/src/amd/compiler/aco_instruction_selection.cpp
+++ b/src/amd/compiler/aco_instruction_selection.cpp
@@ -11151,6 +11151,59 @@ create_fs_end_for_epilog(isel_context* ctx)
    ctx->program->needs_exact = true;
 }
 
+struct callee_info
+get_callee_info(const ABI& abi, unsigned param_count, const nir_parameter* parameters,
+                Program* program)
+{
+   struct callee_info info = {};
+
+   unsigned reg_param_count = 0;
+   int sgpr_reg_byte_offset = 16;
+   int vgpr_reg_byte_offset = 0;
+   unsigned scratch_param_byte_offset = 0;
+
+   for (unsigned i = 0; i < param_count; ++i) {
+      int* reg_byte_offset;
+      PhysRegInterval interval;
+      if (parameters[i].is_divergent) {
+         reg_byte_offset = &vgpr_reg_byte_offset;
+         interval = abi.parameterSpace.vgpr;
+      } else {
+         reg_byte_offset = &sgpr_reg_byte_offset;
+         interval = abi.parameterSpace.sgpr;
+      }
+
+      unsigned byte_size = align(parameters[i].bit_size, 32) / 8 * parameters[i].num_components;
+      RegClass type =
+         RegClass(parameters[i].is_divergent ? RegType::vgpr : RegType::sgpr, byte_size / 4);
+      PhysReg param_reg = interval.lo().advance(*reg_byte_offset);
+
+      if (param_reg < interval.hi()) {
+         ++reg_param_count;
+         Temp dst = program ? program->allocateTmp(type) : Temp();
+         Definition def = Definition(dst);
+         def.setFixed(param_reg);
+         *reg_byte_offset += byte_size;
+         info.param_infos.emplace(i, parameter_info{
+                                        .discardable = parameters[i].discardable,
+                                        .is_reg = true,
+                                        .def = def,
+                                     });
+      } else {
+         info.param_infos.emplace(i, parameter_info{
+                                        .discardable = parameters[i].discardable,
+                                        .is_reg = false,
+                                        .scratch_offset = scratch_param_byte_offset,
+                                     });
+         scratch_param_byte_offset += byte_size;
+      }
+   }
+
+   info.reg_param_count = reg_param_count;
+   info.scratch_param_size = scratch_param_byte_offset;
+   return info;
+}
+
 Instruction*
 add_startpgm(struct isel_context* ctx)
 {
diff --git a/src/amd/compiler/aco_instruction_selection.h b/src/amd/compiler/aco_instruction_selection.h
index c44849de6cc00..e240b2fd2fcd1 100644
--- a/src/amd/compiler/aco_instruction_selection.h
+++ b/src/amd/compiler/aco_instruction_selection.h
@@ -35,6 +35,30 @@ struct shader_io_state {
    }
 };
 
+struct parameter_info {
+   bool discardable;
+   bool is_reg;
+   union {
+      Definition def;
+      unsigned scratch_offset;
+   };
+};
+
+struct call_info {
+   nir_call_instr* nir_instr;
+   Instruction* aco_instr;
+   std::vector<parameter_info> return_info;
+   unsigned scratch_param_size;
+};
+
+struct callee_info {
+   std::unordered_map<unsigned, parameter_info> param_infos;
+   parameter_info return_address;
+   parameter_info stack_ptr;
+   unsigned reg_param_count = 0;
+   unsigned scratch_param_size = 0;
+};
+
 struct isel_context {
    const struct aco_compiler_options* options;
    const struct ac_shader_args* args;
@@ -91,6 +115,13 @@ struct isel_context {
    uint32_t wqm_instruction_idx;
 
    BITSET_DECLARE(output_args, AC_MAX_ARGS);
+
+   /* Function information */
+   ABI callee_abi;
+   struct callee_info callee_info;
+   std::vector<call_info> call_infos;
+   Temp next_divergent_pc;
+   Temp next_pc;
 };
 
 inline Temp
diff --git a/src/amd/compiler/aco_instruction_selection_setup.cpp b/src/amd/compiler/aco_instruction_selection_setup.cpp
index 5fe46c2e93cc0..68bd04ad91977 100644
--- a/src/amd/compiler/aco_instruction_selection_setup.cpp
+++ b/src/amd/compiler/aco_instruction_selection_setup.cpp
@@ -300,6 +300,8 @@ init_context(isel_context* ctx, nir_shader* shader)
    ctx->program->allocateRange(impl->ssa_alloc);
    RegClass* regclasses = ctx->program->temp_rc.data() + ctx->first_temp_id;
 
+   unsigned call_count = 0;
+
    /* TODO: make this recursive to improve compile times */
    bool done = false;
    while (!done) {
@@ -590,12 +592,18 @@ init_context(isel_context* ctx, nir_shader* shader)
                regclasses[phi->def.index] = rc;
                break;
             }
+            case nir_instr_type_call: {
+               ++call_count;
+               break;
+            }
             default: break;
             }
          }
       }
    }
 
+   ctx->call_infos.reserve(call_count);
+
    ctx->program->config->spi_ps_input_ena = ctx->program->info.ps.spi_ps_input_ena;
    ctx->program->config->spi_ps_input_addr = ctx->program->info.ps.spi_ps_input_addr;
 
diff --git a/src/amd/compiler/aco_ir.h b/src/amd/compiler/aco_ir.h
index cc7bec8d766b2..c749ea085ff77 100644
--- a/src/amd/compiler/aco_ir.h
+++ b/src/amd/compiler/aco_ir.h
@@ -2329,6 +2329,10 @@ public:
    /* For shader part with previous shader part that has lds access. */
    bool pending_lds_access = false;
 
+   ABI callee_abi = {};
+   unsigned short arg_sgpr_count;
+   unsigned short arg_vgpr_count;
+
    struct {
       FILE* output = stderr;
       bool shorten_messages = false;
-- 
GitLab


From 2958046f8355b650a4dd785f958668e3bbf5e4bc Mon Sep 17 00:00:00 2001
From: Friedrich Vock <friedrich.vock@gmx.de>
Date: Sun, 21 Apr 2024 17:52:58 +0200
Subject: [PATCH 33/71] aco/isel: Use stack pointer parameter in
 load/store_scratch

---
 .../compiler/aco_instruction_selection.cpp    | 32 +++++++++++++++++--
 1 file changed, 29 insertions(+), 3 deletions(-)

diff --git a/src/amd/compiler/aco_instruction_selection.cpp b/src/amd/compiler/aco_instruction_selection.cpp
index 60452013846f0..863b8c9a15388 100644
--- a/src/amd/compiler/aco_instruction_selection.cpp
+++ b/src/amd/compiler/aco_instruction_selection.cpp
@@ -7600,11 +7600,28 @@ visit_load_scratch(isel_context* ctx, nir_intrinsic_instr* instr)
    if (ctx->program->gfx_level >= GFX9) {
       if (nir_src_is_const(instr->src[0])) {
          uint32_t max = ctx->program->dev.scratch_global_offset_max + 1;
-         info.offset =
-            bld.copy(bld.def(s1), Operand::c32(ROUND_DOWN_TO(nir_src_as_uint(instr->src[0]), max)));
+         if (ctx->callee_info.stack_ptr.is_reg)
+            info.offset =
+               bld.sop2(aco_opcode::s_add_u32, bld.def(s1), bld.def(s1, scc),
+                        Operand(ctx->callee_info.stack_ptr.def.getTemp()),
+                        Operand::c32(ROUND_DOWN_TO(nir_src_as_uint(instr->src[0]), max)));
+         else
+            info.offset = bld.copy(
+               bld.def(s1), Operand::c32(ROUND_DOWN_TO(nir_src_as_uint(instr->src[0]), max)));
          info.const_offset = nir_src_as_uint(instr->src[0]) % max;
       } else {
-         info.offset = Operand(get_ssa_temp(ctx, instr->src[0].ssa));
+         if (ctx->callee_info.stack_ptr.is_reg) {
+            Temp store_offset = get_ssa_temp(ctx, instr->src[0].ssa);
+            if (store_offset.type() == RegType::sgpr)
+               info.offset = bld.sop2(aco_opcode::s_add_u32, bld.def(s1), bld.def(s1, scc),
+                                      Operand(ctx->callee_info.stack_ptr.def.getTemp()),
+                                      Operand(store_offset));
+            else
+               info.offset = bld.vop2(aco_opcode::v_add_u32, bld.def(v1),
+                                      Operand(ctx->callee_info.stack_ptr.def.getTemp()),
+                                      Operand(store_offset));
+         } else
+            info.offset = Operand(get_ssa_temp(ctx, instr->src[0].ssa));
       }
       EmitLoadParameters params = scratch_flat_load_params;
       params.max_const_offset_plus_one = ctx->program->dev.scratch_global_offset_max + 1;
@@ -7624,6 +7641,15 @@ visit_store_scratch(isel_context* ctx, nir_intrinsic_instr* instr)
    Temp data = as_vgpr(ctx, get_ssa_temp(ctx, instr->src[0].ssa));
    Temp offset = get_ssa_temp(ctx, instr->src[1].ssa);
 
+   if (ctx->callee_info.stack_ptr.is_reg && ctx->program->gfx_level >= GFX9) {
+      if (offset.type() == RegType::sgpr)
+         offset = bld.sop2(aco_opcode::s_add_u32, bld.def(s1), bld.def(s1, scc),
+                           Operand(ctx->callee_info.stack_ptr.def.getTemp()), Operand(offset));
+      else
+         offset = bld.vop2(aco_opcode::v_add_u32, bld.def(v1),
+                           Operand(ctx->callee_info.stack_ptr.def.getTemp()), Operand(offset));
+   }
+
    unsigned elem_size_bytes = instr->src[0].ssa->bit_size / 8;
    unsigned writemask = util_widen_mask(nir_intrinsic_write_mask(instr), elem_size_bytes);
 
-- 
GitLab


From 4c0e615a38807af54e1bba457f0db4e9ff783706 Mon Sep 17 00:00:00 2001
From: Friedrich Vock <friedrich.vock@gmx.de>
Date: Sat, 9 Mar 2024 11:15:43 +0100
Subject: [PATCH 34/71] nir,aco: Add set_next_call_pc_amd intrinsic

Used for lowering function calls
---
 src/amd/compiler/aco_instruction_selection.cpp | 5 +++++
 src/compiler/nir/nir_intrinsics.py             | 2 ++
 2 files changed, 7 insertions(+)

diff --git a/src/amd/compiler/aco_instruction_selection.cpp b/src/amd/compiler/aco_instruction_selection.cpp
index 863b8c9a15388..0b76bc477ba87 100644
--- a/src/amd/compiler/aco_instruction_selection.cpp
+++ b/src/amd/compiler/aco_instruction_selection.cpp
@@ -9372,6 +9372,11 @@ visit_intrinsic(isel_context* ctx, nir_intrinsic_instr* instr)
       bld.pseudo(aco_opcode::p_unit_test, Definition(get_ssa_temp(ctx, &instr->def)),
                  Operand::c32(nir_intrinsic_base(instr)));
       break;
+   case nir_intrinsic_set_next_call_pc_amd: {
+      ctx->next_divergent_pc = as_vgpr(ctx, get_ssa_temp(ctx, instr->src[0].ssa));
+      ctx->next_pc = get_ssa_temp(ctx, instr->src[1].ssa);
+      break;
+   }
    default:
       isel_err(&instr->instr, "Unimplemented intrinsic instr");
       abort();
diff --git a/src/compiler/nir/nir_intrinsics.py b/src/compiler/nir/nir_intrinsics.py
index d4847c4487912..be9e1d9c9eadd 100644
--- a/src/compiler/nir/nir_intrinsics.py
+++ b/src/compiler/nir/nir_intrinsics.py
@@ -2308,3 +2308,5 @@ intrinsic("enqueue_node_payloads", src_comp=[-1])
 
 # Returns true if it has been called for every payload.
 intrinsic("finalize_incoming_node_payload", src_comp=[-1], dest_comp=1)
+
+intrinsic("set_next_call_pc_amd", src_comp=[1, 1], bit_sizes=[64])
-- 
GitLab


From 94c2b57845fd3afd8804c1032de05ba5980aa672 Mon Sep 17 00:00:00 2001
From: Friedrich Vock <friedrich.vock@gmx.de>
Date: Mon, 26 Feb 2024 12:20:26 +0100
Subject: [PATCH 35/71] nir,aco: add call_return_adress sysval

---
 src/amd/compiler/aco_instruction_selection.cpp | 5 +++++
 src/compiler/nir/nir_divergence_analysis.c     | 1 +
 src/compiler/nir/nir_intrinsics.py             | 1 +
 3 files changed, 7 insertions(+)

diff --git a/src/amd/compiler/aco_instruction_selection.cpp b/src/amd/compiler/aco_instruction_selection.cpp
index 0b76bc477ba87..e500698714151 100644
--- a/src/amd/compiler/aco_instruction_selection.cpp
+++ b/src/amd/compiler/aco_instruction_selection.cpp
@@ -9372,6 +9372,11 @@ visit_intrinsic(isel_context* ctx, nir_intrinsic_instr* instr)
       bld.pseudo(aco_opcode::p_unit_test, Definition(get_ssa_temp(ctx, &instr->def)),
                  Operand::c32(nir_intrinsic_base(instr)));
       break;
+   case nir_intrinsic_load_call_return_address_amd: {
+      bld.copy(Definition(get_ssa_temp(ctx, &instr->def)),
+               Operand(ctx->callee_info.return_address.def.getTemp()));
+      break;
+   }
    case nir_intrinsic_set_next_call_pc_amd: {
       ctx->next_divergent_pc = as_vgpr(ctx, get_ssa_temp(ctx, instr->src[0].ssa));
       ctx->next_pc = get_ssa_temp(ctx, instr->src[1].ssa);
diff --git a/src/compiler/nir/nir_divergence_analysis.c b/src/compiler/nir/nir_divergence_analysis.c
index f56ad90118a32..61375ec339cb3 100644
--- a/src/compiler/nir/nir_divergence_analysis.c
+++ b/src/compiler/nir/nir_divergence_analysis.c
@@ -267,6 +267,7 @@ visit_intrinsic(nir_intrinsic_instr *instr, struct divergence_state *state)
    case nir_intrinsic_optimization_barrier_sgpr_amd:
    case nir_intrinsic_load_printf_buffer_address:
    case nir_intrinsic_load_printf_base_identifier:
+   case nir_intrinsic_load_call_return_address_amd:
       is_divergent = false;
       break;
 
diff --git a/src/compiler/nir/nir_intrinsics.py b/src/compiler/nir/nir_intrinsics.py
index be9e1d9c9eadd..21deaa227a669 100644
--- a/src/compiler/nir/nir_intrinsics.py
+++ b/src/compiler/nir/nir_intrinsics.py
@@ -2309,4 +2309,5 @@ intrinsic("enqueue_node_payloads", src_comp=[-1])
 # Returns true if it has been called for every payload.
 intrinsic("finalize_incoming_node_payload", src_comp=[-1], dest_comp=1)
 
+system_value("call_return_address_amd", 1, bit_sizes=[64])
 intrinsic("set_next_call_pc_amd", src_comp=[1, 1], bit_sizes=[64])
-- 
GitLab


From d8ed972542dde1d07929b4ce8bfdfe367aba10df Mon Sep 17 00:00:00 2001
From: Friedrich Vock <friedrich.vock@gmx.de>
Date: Sun, 7 Jan 2024 22:15:13 +0100
Subject: [PATCH 36/71] radv/nir: Lower NIR function call ABI

---
 src/amd/vulkan/meson.build                   |   1 +
 src/amd/vulkan/nir/radv_nir.h                |   2 +
 src/amd/vulkan/nir/radv_nir_lower_call_abi.c | 375 +++++++++++++++++++
 src/amd/vulkan/radv_pipeline.c               |   4 +
 src/compiler/nir/nir_divergence_analysis.c   |   1 +
 src/compiler/nir/nir_intrinsics.py           |   3 +
 6 files changed, 386 insertions(+)
 create mode 100644 src/amd/vulkan/nir/radv_nir_lower_call_abi.c

diff --git a/src/amd/vulkan/meson.build b/src/amd/vulkan/meson.build
index a9412c76625a6..73514d4eeea98 100644
--- a/src/amd/vulkan/meson.build
+++ b/src/amd/vulkan/meson.build
@@ -67,6 +67,7 @@ libradv_files = files(
   'nir/radv_nir_apply_pipeline_layout.c',
   'nir/radv_nir_export_multiview.c',
   'nir/radv_nir_lower_abi.c',
+  'nir/radv_nir_lower_call_abi.c',
   'nir/radv_nir_lower_cooperative_matrix.c',
   'nir/radv_nir_lower_fs_barycentric.c',
   'nir/radv_nir_lower_fs_intrinsics.c',
diff --git a/src/amd/vulkan/nir/radv_nir.h b/src/amd/vulkan/nir/radv_nir.h
index 03b1da8753ca6..c3fdea99883f0 100644
--- a/src/amd/vulkan/nir/radv_nir.h
+++ b/src/amd/vulkan/nir/radv_nir.h
@@ -73,6 +73,8 @@ bool radv_nir_lower_draw_id_to_zero(nir_shader *shader);
 
 bool radv_nir_remap_color_attachment(nir_shader *shader, const struct radv_graphics_state_key *gfx_state);
 
+bool radv_nir_lower_call_abi(nir_shader *shader, unsigned wave_size);
+
 #ifdef __cplusplus
 }
 #endif
diff --git a/src/amd/vulkan/nir/radv_nir_lower_call_abi.c b/src/amd/vulkan/nir/radv_nir_lower_call_abi.c
new file mode 100644
index 0000000000000..c8db7ffc3e6a7
--- /dev/null
+++ b/src/amd/vulkan/nir/radv_nir_lower_call_abi.c
@@ -0,0 +1,375 @@
+/*
+ * Copyright Â© 2023 Valve Corporation
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
+ * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS
+ * IN THE SOFTWARE.
+ */
+
+#include "nir_builder.h"
+#include "radv_nir.h"
+
+static void
+adjust_callee_parameters(nir_function *function, struct set *visited_funcs)
+{
+   if (_mesa_set_search(visited_funcs, function))
+      return;
+   _mesa_set_add(visited_funcs, function);
+
+   nir_parameter *old_params = function->params;
+   unsigned old_num_params = function->num_params;
+
+   function->num_params += 2;
+   function->params = rzalloc_array_size(function->shader, function->num_params, sizeof(nir_parameter));
+
+   memcpy(function->params + 2, old_params, old_num_params * sizeof(nir_parameter));
+
+   function->params[0].num_components = 1;
+   function->params[0].bit_size = 64;
+   function->params[0].is_divergent = true;
+   function->params[1].num_components = 1;
+   function->params[1].bit_size = 64;
+
+   nir_function_impl *impl = function->impl;
+
+   if (!impl)
+      return;
+
+   nir_foreach_block (block, impl) {
+      nir_foreach_instr_safe (instr, block) {
+         if (instr->type != nir_instr_type_intrinsic)
+            continue;
+
+         nir_intrinsic_instr *intr = nir_instr_as_intrinsic(instr);
+
+         if (intr->intrinsic == nir_intrinsic_load_param)
+            nir_intrinsic_set_param_idx(intr, nir_intrinsic_param_idx(intr) + 2);
+      }
+   }
+}
+
+static void
+gather_tail_call_instrs_block(nir_function *caller, const struct nir_block *block, struct set *tail_calls)
+{
+   nir_foreach_instr_reverse (instr, block) {
+      switch (instr->type) {
+      case nir_instr_type_phi:
+      case nir_instr_type_undef:
+      case nir_instr_type_load_const:
+         continue;
+      case nir_instr_type_alu:
+         if (!nir_op_is_vec_or_mov(nir_instr_as_alu(instr)->op))
+            return;
+         continue;
+      case nir_instr_type_call: {
+         nir_call_instr *call = nir_instr_as_call(instr);
+
+         /* TODO: */
+         if (call->abi != nir_call_abi_rt_raygen_amd)
+            return;
+
+         for (unsigned i = 0; i < call->num_params; ++i) {
+            if (call->callee->params[i].is_return != caller->params[i].is_return)
+               return;
+            if (!call->callee->params[i].is_divergent)
+               return;
+            if (call->callee->params[i].bit_size != caller->params[i].bit_size)
+               return;
+            if (call->callee->params[i].num_components != caller->params[i].num_components)
+               return;
+         }
+
+         _mesa_set_add(tail_calls, instr);
+         continue;
+      }
+      default:
+         return;
+      }
+   }
+
+   set_foreach (block->predecessors, pred) {
+      gather_tail_call_instrs_block(caller, pred->key, tail_calls);
+   }
+}
+
+struct lower_param_info {
+   /*  */
+   nir_def *load_param_def;
+
+   nir_def *return_deref;
+};
+
+static void
+rewrite_return_param_uses(nir_intrinsic_instr *intr, unsigned param_idx, struct lower_param_info *param_defs)
+{
+   nir_foreach_use_safe (use, &intr->def) {
+      nir_instr *use_instr = nir_src_parent_instr(use);
+      assert(use_instr && use_instr->type == nir_instr_type_deref &&
+             nir_instr_as_deref(use_instr)->deref_type == nir_deref_type_cast);
+      nir_def_rewrite_uses(&nir_instr_as_deref(use_instr)->def, param_defs[param_idx].return_deref);
+
+      nir_instr_remove(use_instr);
+   }
+}
+
+static void
+lower_call_abi_for_callee(nir_function *function, unsigned wave_size, struct set *visited_funcs)
+{
+   nir_function_impl *impl = function->impl;
+
+   nir_builder b = nir_builder_create(impl);
+   b.cursor = nir_before_impl(impl);
+
+   nir_variable *tail_call_pc =
+      nir_variable_create(b.shader, nir_var_shader_temp, glsl_uint64_t_type(), "_tail_call_pc");
+   nir_store_var(&b, tail_call_pc, nir_imm_int64(&b, 0), 0x1);
+
+   struct set *tail_call_instrs = _mesa_set_create(b.shader, _mesa_hash_pointer, _mesa_key_pointer_equal);
+   gather_tail_call_instrs_block(function, nir_impl_last_block(impl), tail_call_instrs);
+
+   adjust_callee_parameters(function, visited_funcs);
+
+   /* guard the shader, so that only the correct invocations execute it */
+
+   nir_def *guard_condition = NULL;
+   nir_def *shader_addr;
+   nir_def *uniform_shader_addr;
+   if (!function->uniform_call) {
+      nir_cf_list list;
+      nir_cf_extract(&list, nir_before_impl(impl), nir_after_impl(impl));
+
+      b.cursor = nir_before_impl(impl);
+
+      shader_addr = nir_load_param(&b, 0);
+      uniform_shader_addr = nir_load_param(&b, 1);
+
+      guard_condition = nir_ieq(&b, uniform_shader_addr, shader_addr);
+      nir_if *shader_guard = nir_push_if(&b, guard_condition);
+      shader_guard->control = nir_selection_control_divergent_always_taken;
+      nir_cf_reinsert(&list, b.cursor);
+      nir_pop_if(&b, shader_guard);
+   } else {
+      shader_addr = nir_load_param(&b, 0);
+   }
+
+   b.cursor = nir_before_impl(impl);
+   struct lower_param_info *param_infos = ralloc_size(b.shader, function->num_params * sizeof(struct lower_param_info));
+   nir_variable **param_vars = ralloc_size(b.shader, function->num_params * sizeof(nir_variable *));
+
+   for (unsigned i = 2; i < function->num_params; ++i) {
+      param_vars[i] = nir_local_variable_create(impl, function->params[i].type, "_param");
+      unsigned num_components = glsl_get_vector_elements(function->params[i].type);
+
+      if (function->params[i].is_return) {
+         assert(!glsl_type_is_array(function->params[i].type) && !glsl_type_is_struct(function->params[i].type));
+
+         function->params[i].bit_size = glsl_get_bit_size(function->params[i].type);
+         function->params[i].num_components = num_components;
+
+         param_infos[i].return_deref = &nir_build_deref_var(&b, param_vars[i])->def;
+      } else {
+         param_infos[i].return_deref = NULL;
+      }
+
+      param_infos[i].load_param_def = nir_load_param(&b, i);
+      nir_store_var(&b, param_vars[i], param_infos[i].load_param_def, (0x1 << num_components) - 1);
+   }
+
+   unsigned max_tail_call_param = 0;
+
+   nir_foreach_block (block, impl) {
+      bool progress;
+      do {
+         progress = false;
+         nir_foreach_instr_safe (instr, block) {
+            if (instr->type == nir_instr_type_call && _mesa_set_search(tail_call_instrs, instr)) {
+               nir_call_instr *call = nir_instr_as_call(instr);
+               b.cursor = nir_before_instr(instr);
+
+               for (unsigned i = 0; i < call->num_params; ++i) {
+                  if (call->callee->params[i].is_return)
+                     nir_store_var(&b, param_vars[i + 2],
+                                   nir_load_deref(&b, nir_instr_as_deref(call->params[i].ssa->parent_instr)),
+                                   (0x1 << glsl_get_vector_elements(call->callee->params[i].type)) - 1);
+                  else
+                     nir_store_var(&b, param_vars[i + 2], call->params[i].ssa,
+                                   (0x1 << call->params[i].ssa->num_components) - 1);
+               }
+
+               nir_store_var(&b, tail_call_pc, call->indirect_callee.ssa, 0x1);
+               max_tail_call_param = MAX2(max_tail_call_param, call->num_params + 2);
+
+               nir_instr_remove(instr);
+
+               progress = true;
+               break;
+            }
+
+            if (instr->type != nir_instr_type_intrinsic)
+               continue;
+            nir_intrinsic_instr *intr = nir_instr_as_intrinsic(instr);
+            if (nir_instr_as_intrinsic(instr)->intrinsic == nir_intrinsic_load_param) {
+               unsigned param_idx = nir_intrinsic_param_idx(intr);
+
+               if (param_idx >= 2 && &intr->def != param_infos[param_idx].load_param_def) {
+                  if (function->params[param_idx].is_return)
+                     rewrite_return_param_uses(intr, param_idx, param_infos);
+                  else
+                     nir_def_rewrite_uses(&intr->def, param_infos[param_idx].load_param_def);
+                  nir_instr_remove(instr);
+                  progress = true;
+                  break;
+               }
+            }
+         }
+      } while (progress);
+   }
+
+   b.cursor = nir_after_impl(impl);
+
+   for (unsigned i = 2; i < function->num_params; ++i) {
+      if (function->params[i].is_return || i < max_tail_call_param) {
+         nir_store_param_amd(&b, nir_load_var(&b, param_vars[i]), .param_idx = i);
+      }
+   }
+
+   if (guard_condition)
+      shader_addr = nir_bcsel(&b, guard_condition, nir_load_var(&b, tail_call_pc), shader_addr);
+   else
+      shader_addr = nir_load_var(&b, tail_call_pc);
+   nir_def *ballot = nir_ballot(&b, 1, wave_size, nir_ine_imm(&b, shader_addr, 0));
+   nir_def *ballot_addr = nir_read_invocation(&b, shader_addr, nir_find_lsb(&b, ballot));
+   uniform_shader_addr = nir_bcsel(&b, nir_ieq_imm(&b, ballot, 0), nir_load_call_return_address_amd(&b), ballot_addr);
+
+   if (!function->noreturn) {
+      nir_push_if(&b, nir_ieq_imm(&b, uniform_shader_addr, 0));
+      nir_terminate(&b);
+      nir_pop_if(&b, NULL);
+
+      nir_set_next_call_pc_amd(&b, shader_addr, uniform_shader_addr);
+   }
+}
+
+static void
+lower_call_abi_for_call(nir_builder *b, nir_call_instr *call, unsigned *cur_call_idx, struct set *visited_funcs,
+                        struct set *visited_calls)
+{
+   unsigned call_idx = (*cur_call_idx)++;
+
+   for (unsigned i = 0; i < call->num_params; ++i) {
+      unsigned callee_param_idx = i;
+      if (_mesa_set_search(visited_funcs, call->callee))
+         callee_param_idx += 2;
+
+      if (!call->callee->params[callee_param_idx].is_return)
+         continue;
+
+      b->cursor = nir_before_instr(&call->instr);
+
+      nir_src *old_src = &call->params[i];
+
+      assert(old_src->ssa->parent_instr->type == nir_instr_type_deref);
+      nir_deref_instr *param_deref = nir_instr_as_deref(old_src->ssa->parent_instr);
+      assert(param_deref->deref_type == nir_deref_type_var);
+
+      nir_src_rewrite(old_src, nir_load_deref(b, param_deref));
+
+      b->cursor = nir_after_instr(&call->instr);
+
+      unsigned num_components = glsl_get_vector_elements(param_deref->type);
+
+      nir_store_deref(
+         b, param_deref,
+         nir_load_return_param_amd(b, num_components, glsl_base_type_get_bit_size(param_deref->type->base_type),
+                                   .call_idx = call_idx, .param_idx = i + 2),
+         (1u << num_components) - 1);
+
+      assert(call->callee->params[callee_param_idx].bit_size == glsl_get_bit_size(param_deref->type));
+      assert(call->callee->params[callee_param_idx].num_components == num_components);
+   }
+
+   adjust_callee_parameters(call->callee, visited_funcs);
+
+   b->cursor = nir_after_instr(&call->instr);
+
+   nir_call_instr *new_call = nir_call_instr_create(b->shader, call->callee);
+   new_call->indirect_callee = nir_src_for_ssa(call->indirect_callee.ssa);
+   new_call->params[0] = nir_src_for_ssa(call->indirect_callee.ssa);
+   new_call->params[1] = nir_src_for_ssa(nir_read_first_invocation(b, call->indirect_callee.ssa));
+   for (unsigned i = 2; i < new_call->num_params; ++i)
+      new_call->params[i] = nir_src_for_ssa(call->params[i - 2].ssa);
+   new_call->abi = call->abi;
+
+   nir_builder_instr_insert(b, &new_call->instr);
+   b->cursor = nir_after_instr(&new_call->instr);
+   _mesa_set_add(visited_calls, new_call);
+
+   nir_instr_remove(&call->instr);
+}
+
+static bool
+lower_call_abi_for_caller(nir_function_impl *impl, struct set *visited_funcs)
+{
+   bool progress = false;
+   unsigned cur_call_idx = 0;
+   struct set *visited_calls = _mesa_set_create(NULL, _mesa_hash_pointer, _mesa_key_pointer_equal);
+
+   nir_foreach_block (block, impl) {
+      nir_foreach_instr_safe (instr, block) {
+         if (instr->type != nir_instr_type_call)
+            continue;
+         nir_call_instr *call = nir_instr_as_call(instr);
+         if (call->callee->impl)
+            continue;
+         if (_mesa_set_search(visited_calls, call))
+            continue;
+
+         nir_builder b = nir_builder_create(impl);
+         lower_call_abi_for_call(&b, call, &cur_call_idx, visited_funcs, visited_calls);
+         progress = true;
+      }
+   }
+
+   _mesa_set_destroy(visited_calls, NULL);
+
+   return progress;
+}
+
+bool
+radv_nir_lower_call_abi(nir_shader *shader, unsigned wave_size)
+{
+   struct set *visited_funcs = _mesa_set_create(NULL, _mesa_hash_pointer, _mesa_key_pointer_equal);
+
+   bool progress = false;
+   nir_foreach_function_with_impl (function, impl, shader) {
+      bool func_progress = false;
+      if (function->is_exported) {
+         lower_call_abi_for_callee(function, wave_size, visited_funcs);
+         func_progress = true;
+      }
+      func_progress |= lower_call_abi_for_caller(impl, visited_funcs);
+
+      if (func_progress)
+         nir_metadata_preserve(impl, nir_metadata_block_index | nir_metadata_dominance);
+      progress |= func_progress;
+   }
+
+   _mesa_set_destroy(visited_funcs, NULL);
+
+   return progress;
+}
diff --git a/src/amd/vulkan/radv_pipeline.c b/src/amd/vulkan/radv_pipeline.c
index e951fd39cbd98..cb4afc9f07448 100644
--- a/src/amd/vulkan/radv_pipeline.c
+++ b/src/amd/vulkan/radv_pipeline.c
@@ -604,6 +604,10 @@ radv_postprocess_nir(struct radv_device *device, const struct radv_graphics_stat
       stage->nir, io_to_mem || lowered_ngg || stage->stage == MESA_SHADER_COMPUTE || stage->stage == MESA_SHADER_TASK,
       gfx_level >= GFX7);
 
+   NIR_PASS(_, stage->nir, radv_nir_lower_call_abi, stage->info.wave_size);
+   NIR_PASS(_, stage->nir, nir_lower_global_vars_to_local);
+   NIR_PASS(_, stage->nir, nir_lower_vars_to_ssa);
+
    NIR_PASS(_, stage->nir, nir_lower_fp16_casts, nir_lower_fp16_split_fp64);
 
    if (stage->nir->info.bit_sizes_int & (8 | 16)) {
diff --git a/src/compiler/nir/nir_divergence_analysis.c b/src/compiler/nir/nir_divergence_analysis.c
index 61375ec339cb3..e1ce5fe6c2349 100644
--- a/src/compiler/nir/nir_divergence_analysis.c
+++ b/src/compiler/nir/nir_divergence_analysis.c
@@ -756,6 +756,7 @@ visit_intrinsic(nir_intrinsic_instr *instr, struct divergence_state *state)
    case nir_intrinsic_load_tcs_header_ir3:
    case nir_intrinsic_load_rel_patch_id_ir3:
    case nir_intrinsic_brcst_active_ir3:
+   case nir_intrinsic_load_return_param_amd:
       is_divergent = true;
       break;
 
diff --git a/src/compiler/nir/nir_intrinsics.py b/src/compiler/nir/nir_intrinsics.py
index 21deaa227a669..6e1ebd11c06e5 100644
--- a/src/compiler/nir/nir_intrinsics.py
+++ b/src/compiler/nir/nir_intrinsics.py
@@ -2309,5 +2309,8 @@ intrinsic("enqueue_node_payloads", src_comp=[-1])
 # Returns true if it has been called for every payload.
 intrinsic("finalize_incoming_node_payload", src_comp=[-1], dest_comp=1)
 
+intrinsic("store_param_amd", src_comp=[-1], indices=[PARAM_IDX])
+intrinsic("load_return_param_amd", dest_comp=0, indices=[CALL_IDX, PARAM_IDX])
+
 system_value("call_return_address_amd", 1, bit_sizes=[64])
 intrinsic("set_next_call_pc_amd", src_comp=[1, 1], bit_sizes=[64])
-- 
GitLab


From 7ec58309dbdf6716502cf96d5de8517c45f92646 Mon Sep 17 00:00:00 2001
From: Friedrich Vock <friedrich.vock@gmx.de>
Date: Sun, 7 Jan 2024 22:42:03 +0100
Subject: [PATCH 37/71] aco: Compile all functions in RT shaders

---
 .../compiler/aco_instruction_selection.cpp    |  43 +-
 .../aco_instruction_selection_setup.cpp       | 614 +++++++++---------
 2 files changed, 333 insertions(+), 324 deletions(-)

diff --git a/src/amd/compiler/aco_instruction_selection.cpp b/src/amd/compiler/aco_instruction_selection.cpp
index e500698714151..a12b9ec5c9b53 100644
--- a/src/amd/compiler/aco_instruction_selection.cpp
+++ b/src/amd/compiler/aco_instruction_selection.cpp
@@ -11568,30 +11568,35 @@ void
 select_program_rt(isel_context& ctx, unsigned shader_count, struct nir_shader* const* shaders,
                   const struct ac_shader_args* args)
 {
+   bool first_block = true;
    for (unsigned i = 0; i < shader_count; i++) {
-      if (i) {
-         ctx.block = ctx.program->create_and_insert_block();
-         ctx.block->kind = block_kind_top_level | block_kind_resume;
-      }
+      nir_foreach_function_impl (impl, shaders[i]) {
+         if (!first_block) {
+            ctx.block = ctx.program->create_and_insert_block();
+            ctx.block->kind = block_kind_top_level | block_kind_resume;
+         }
+         nir_shader* nir = shaders[i];
 
-      nir_shader* nir = shaders[i];
-      init_context(&ctx, nir);
-      setup_fp_mode(&ctx, nir);
+         init_context(&ctx, nir);
+         setup_fp_mode(&ctx, nir);
 
-      Instruction* startpgm = add_startpgm(&ctx);
-      append_logical_start(ctx.block);
-      split_arguments(&ctx, startpgm);
-      visit_cf_list(&ctx, &nir_shader_get_entrypoint(nir)->body);
-      append_logical_end(ctx.block);
-      ctx.block->kind |= block_kind_uniform;
+         Instruction* startpgm = add_startpgm(&ctx);
+         append_logical_start(ctx.block);
+         split_arguments(&ctx, startpgm);
+         visit_cf_list(&ctx, &impl->body);
+         append_logical_end(ctx.block);
+         ctx.block->kind |= block_kind_uniform;
 
-      /* Fix output registers and jump to next shader. We can skip this when dealing with a raygen
-       * shader without shader calls.
-       */
-      if (shader_count > 1 || shaders[i]->info.stage != MESA_SHADER_RAYGEN)
-         insert_rt_jump_next(ctx, args);
+         /* Fix output registers and jump to next shader. We can skip this when dealing with a
+          * raygen shader without shader calls.
+          */
+         if ((shader_count > 1 || shaders[i]->info.stage != MESA_SHADER_RAYGEN) &&
+             impl == nir_shader_get_entrypoint(nir))
+            insert_rt_jump_next(ctx, args);
 
-      cleanup_context(&ctx);
+         cleanup_context(&ctx);
+         first_block = false;
+      }
    }
 
    ctx.program->config->float_mode = ctx.program->blocks[0].fp_mode.val;
diff --git a/src/amd/compiler/aco_instruction_selection_setup.cpp b/src/amd/compiler/aco_instruction_selection_setup.cpp
index 68bd04ad91977..a953a5621fc8b 100644
--- a/src/amd/compiler/aco_instruction_selection_setup.cpp
+++ b/src/amd/compiler/aco_instruction_selection_setup.cpp
@@ -254,8 +254,8 @@ setup_nir(isel_context* ctx, nir_shader* nir)
    nir_convert_to_lcssa(nir, true, false);
    nir_lower_phis_to_scalar(nir, true);
 
-   nir_function_impl* func = nir_shader_get_entrypoint(nir);
-   nir_index_ssa_defs(func);
+   nir_foreach_function_impl (impl, nir)
+      nir_index_ssa_defs(impl);
 }
 
 } /* end namespace */
@@ -263,7 +263,6 @@ setup_nir(isel_context* ctx, nir_shader* nir)
 void
 init_context(isel_context* ctx, nir_shader* shader)
 {
-   nir_function_impl* impl = nir_shader_get_entrypoint(shader);
    ctx->shader = shader;
 
    /* Init NIR range analysis. */
@@ -282,331 +281,335 @@ init_context(isel_context* ctx, nir_shader* shader)
    if (nir_opt_uniform_atomics(shader) && nir_lower_int64(shader))
       nir_divergence_analysis(shader);
 
-   apply_nuw_to_offsets(ctx, impl);
+   nir_foreach_function_impl (impl, shader) {
+      apply_nuw_to_offsets(ctx, impl);
 
-   /* sanitize control flow */
-   sanitize_cf_list(impl, &impl->body);
-   nir_metadata_preserve(impl, nir_metadata_none);
+      /* sanitize control flow */
+      sanitize_cf_list(impl, &impl->body);
+      nir_metadata_preserve(impl, nir_metadata_none);
 
-   /* we'll need these for isel */
-   nir_metadata_require(impl, nir_metadata_block_index);
+      /* we'll need these for isel */
+      nir_metadata_require(impl, nir_metadata_block_index);
 
-   if (ctx->options->dump_preoptir) {
-      fprintf(stderr, "NIR shader before instruction selection:\n");
-      nir_print_shader(shader, stderr);
-   }
+      if (ctx->options->dump_preoptir) {
+         fprintf(stderr, "NIR shader before instruction selection:\n");
+         nir_print_shader(shader, stderr);
+      }
 
-   ctx->first_temp_id = ctx->program->peekAllocationId();
-   ctx->program->allocateRange(impl->ssa_alloc);
-   RegClass* regclasses = ctx->program->temp_rc.data() + ctx->first_temp_id;
-
-   unsigned call_count = 0;
-
-   /* TODO: make this recursive to improve compile times */
-   bool done = false;
-   while (!done) {
-      done = true;
-      nir_foreach_block (block, impl) {
-         nir_foreach_instr (instr, block) {
-            switch (instr->type) {
-            case nir_instr_type_alu: {
-               nir_alu_instr* alu_instr = nir_instr_as_alu(instr);
-               RegType type = alu_instr->def.divergent ? RegType::vgpr : RegType::sgpr;
-               switch (alu_instr->op) {
-               case nir_op_fmul:
-               case nir_op_fmulz:
-               case nir_op_fadd:
-               case nir_op_fsub:
-               case nir_op_ffma:
-               case nir_op_ffmaz:
-               case nir_op_fmax:
-               case nir_op_fmin:
-               case nir_op_fneg:
-               case nir_op_fabs:
-               case nir_op_fsat:
-               case nir_op_fsign:
-               case nir_op_frcp:
-               case nir_op_frsq:
-               case nir_op_fsqrt:
-               case nir_op_fexp2:
-               case nir_op_flog2:
-               case nir_op_ffract:
-               case nir_op_ffloor:
-               case nir_op_fceil:
-               case nir_op_ftrunc:
-               case nir_op_fround_even:
-               case nir_op_fsin_amd:
-               case nir_op_fcos_amd:
-               case nir_op_f2f16:
-               case nir_op_f2f16_rtz:
-               case nir_op_f2f16_rtne:
-               case nir_op_f2f32:
-               case nir_op_f2f64:
-               case nir_op_u2f16:
-               case nir_op_u2f32:
-               case nir_op_u2f64:
-               case nir_op_i2f16:
-               case nir_op_i2f32:
-               case nir_op_i2f64:
-               case nir_op_pack_half_2x16_rtz_split:
-               case nir_op_pack_half_2x16_split:
-               case nir_op_pack_unorm_2x16:
-               case nir_op_pack_snorm_2x16:
-               case nir_op_pack_uint_2x16:
-               case nir_op_pack_sint_2x16:
-               case nir_op_unpack_half_2x16_split_x:
-               case nir_op_unpack_half_2x16_split_y:
-               case nir_op_fddx:
-               case nir_op_fddy:
-               case nir_op_fddx_fine:
-               case nir_op_fddy_fine:
-               case nir_op_fddx_coarse:
-               case nir_op_fddy_coarse:
-               case nir_op_fquantize2f16:
-               case nir_op_ldexp:
-               case nir_op_frexp_sig:
-               case nir_op_frexp_exp:
-               case nir_op_cube_amd:
-               case nir_op_msad_4x8:
-               case nir_op_mqsad_4x8:
-               case nir_op_udot_4x8_uadd:
-               case nir_op_sdot_4x8_iadd:
-               case nir_op_sudot_4x8_iadd:
-               case nir_op_udot_4x8_uadd_sat:
-               case nir_op_sdot_4x8_iadd_sat:
-               case nir_op_sudot_4x8_iadd_sat:
-               case nir_op_udot_2x16_uadd:
-               case nir_op_sdot_2x16_iadd:
-               case nir_op_udot_2x16_uadd_sat:
-               case nir_op_sdot_2x16_iadd_sat: type = RegType::vgpr; break;
-               case nir_op_f2i16:
-               case nir_op_f2u16:
-               case nir_op_f2i32:
-               case nir_op_f2u32:
-               case nir_op_b2i8:
-               case nir_op_b2i16:
-               case nir_op_b2i32:
-               case nir_op_b2b32:
-               case nir_op_b2f16:
-               case nir_op_b2f32:
-               case nir_op_mov: break;
-               case nir_op_iabs:
-               case nir_op_iadd:
-               case nir_op_iadd_sat:
-               case nir_op_uadd_sat:
-               case nir_op_isub:
-               case nir_op_isub_sat:
-               case nir_op_usub_sat:
-               case nir_op_imul:
-               case nir_op_imin:
-               case nir_op_imax:
-               case nir_op_umin:
-               case nir_op_umax:
-               case nir_op_ishl:
-               case nir_op_ishr:
-               case nir_op_ushr:
-                  /* packed 16bit instructions have to be VGPR */
-                  type = alu_instr->def.num_components == 2 ? RegType::vgpr : type;
-                  FALLTHROUGH;
-               default:
-                  for (unsigned i = 0; i < nir_op_infos[alu_instr->op].num_inputs; i++) {
-                     if (regclasses[alu_instr->src[i].src.ssa->index].type() == RegType::vgpr)
-                        type = RegType::vgpr;
+      ctx->first_temp_id = ctx->program->peekAllocationId();
+      ctx->program->allocateRange(impl->ssa_alloc);
+      RegClass* regclasses = ctx->program->temp_rc.data() + ctx->first_temp_id;
+
+      unsigned call_count = 0;
+
+      /* TODO: make this recursive to improve compile times */
+      bool done = false;
+      while (!done) {
+         done = true;
+         nir_foreach_block (block, impl) {
+            nir_foreach_instr (instr, block) {
+               switch (instr->type) {
+               case nir_instr_type_alu: {
+                  nir_alu_instr* alu_instr = nir_instr_as_alu(instr);
+                  RegType type = alu_instr->def.divergent ? RegType::vgpr : RegType::sgpr;
+                  switch (alu_instr->op) {
+                  case nir_op_fmul:
+                  case nir_op_fmulz:
+                  case nir_op_fadd:
+                  case nir_op_fsub:
+                  case nir_op_ffma:
+                  case nir_op_ffmaz:
+                  case nir_op_fmax:
+                  case nir_op_fmin:
+                  case nir_op_fneg:
+                  case nir_op_fabs:
+                  case nir_op_fsat:
+                  case nir_op_fsign:
+                  case nir_op_frcp:
+                  case nir_op_frsq:
+                  case nir_op_fsqrt:
+                  case nir_op_fexp2:
+                  case nir_op_flog2:
+                  case nir_op_ffract:
+                  case nir_op_ffloor:
+                  case nir_op_fceil:
+                  case nir_op_ftrunc:
+                  case nir_op_fround_even:
+                  case nir_op_fsin_amd:
+                  case nir_op_fcos_amd:
+                  case nir_op_f2f16:
+                  case nir_op_f2f16_rtz:
+                  case nir_op_f2f16_rtne:
+                  case nir_op_f2f32:
+                  case nir_op_f2f64:
+                  case nir_op_u2f16:
+                  case nir_op_u2f32:
+                  case nir_op_u2f64:
+                  case nir_op_i2f16:
+                  case nir_op_i2f32:
+                  case nir_op_i2f64:
+                  case nir_op_pack_half_2x16_rtz_split:
+                  case nir_op_pack_half_2x16_split:
+                  case nir_op_pack_unorm_2x16:
+                  case nir_op_pack_snorm_2x16:
+                  case nir_op_pack_uint_2x16:
+                  case nir_op_pack_sint_2x16:
+                  case nir_op_unpack_half_2x16_split_x:
+                  case nir_op_unpack_half_2x16_split_y:
+                  case nir_op_fddx:
+                  case nir_op_fddy:
+                  case nir_op_fddx_fine:
+                  case nir_op_fddy_fine:
+                  case nir_op_fddx_coarse:
+                  case nir_op_fddy_coarse:
+                  case nir_op_fquantize2f16:
+                  case nir_op_ldexp:
+                  case nir_op_frexp_sig:
+                  case nir_op_frexp_exp:
+                  case nir_op_cube_amd:
+                  case nir_op_msad_4x8:
+                  case nir_op_mqsad_4x8:
+                  case nir_op_udot_4x8_uadd:
+                  case nir_op_sdot_4x8_iadd:
+                  case nir_op_sudot_4x8_iadd:
+                  case nir_op_udot_4x8_uadd_sat:
+                  case nir_op_sdot_4x8_iadd_sat:
+                  case nir_op_sudot_4x8_iadd_sat:
+                  case nir_op_udot_2x16_uadd:
+                  case nir_op_sdot_2x16_iadd:
+                  case nir_op_udot_2x16_uadd_sat:
+                  case nir_op_sdot_2x16_iadd_sat: type = RegType::vgpr; break;
+                  case nir_op_f2i16:
+                  case nir_op_f2u16:
+                  case nir_op_f2i32:
+                  case nir_op_f2u32:
+                  case nir_op_b2i8:
+                  case nir_op_b2i16:
+                  case nir_op_b2i32:
+                  case nir_op_b2b32:
+                  case nir_op_b2f16:
+                  case nir_op_b2f32:
+                  case nir_op_mov: break;
+                  case nir_op_iabs:
+                  case nir_op_iadd:
+                  case nir_op_iadd_sat:
+                  case nir_op_uadd_sat:
+                  case nir_op_isub:
+                  case nir_op_isub_sat:
+                  case nir_op_usub_sat:
+                  case nir_op_imul:
+                  case nir_op_imin:
+                  case nir_op_imax:
+                  case nir_op_umin:
+                  case nir_op_umax:
+                  case nir_op_ishl:
+                  case nir_op_ishr:
+                  case nir_op_ushr:
+                     /* packed 16bit instructions have to be VGPR */
+                     type = alu_instr->def.num_components == 2 ? RegType::vgpr : type;
+                     FALLTHROUGH;
+                  default:
+                     for (unsigned i = 0; i < nir_op_infos[alu_instr->op].num_inputs; i++) {
+                        if (regclasses[alu_instr->src[i].src.ssa->index].type() == RegType::vgpr)
+                           type = RegType::vgpr;
+                     }
+                     break;
                   }
-                  break;
-               }
 
-               RegClass rc =
-                  get_reg_class(ctx, type, alu_instr->def.num_components, alu_instr->def.bit_size);
-               regclasses[alu_instr->def.index] = rc;
-               break;
-            }
-            case nir_instr_type_load_const: {
-               unsigned num_components = nir_instr_as_load_const(instr)->def.num_components;
-               unsigned bit_size = nir_instr_as_load_const(instr)->def.bit_size;
-               RegClass rc = get_reg_class(ctx, RegType::sgpr, num_components, bit_size);
-               regclasses[nir_instr_as_load_const(instr)->def.index] = rc;
-               break;
-            }
-            case nir_instr_type_intrinsic: {
-               nir_intrinsic_instr* intrinsic = nir_instr_as_intrinsic(instr);
-               if (!nir_intrinsic_infos[intrinsic->intrinsic].has_dest)
+                  RegClass rc = get_reg_class(ctx, type, alu_instr->def.num_components,
+                                              alu_instr->def.bit_size);
+                  regclasses[alu_instr->def.index] = rc;
                   break;
-               if (intrinsic->intrinsic == nir_intrinsic_strict_wqm_coord_amd) {
-                  regclasses[intrinsic->def.index] =
-                     RegClass::get(RegType::vgpr, intrinsic->def.num_components * 4 +
-                                                     nir_intrinsic_base(intrinsic))
-                        .as_linear();
+               }
+               case nir_instr_type_load_const: {
+                  unsigned num_components = nir_instr_as_load_const(instr)->def.num_components;
+                  unsigned bit_size = nir_instr_as_load_const(instr)->def.bit_size;
+                  RegClass rc = get_reg_class(ctx, RegType::sgpr, num_components, bit_size);
+                  regclasses[nir_instr_as_load_const(instr)->def.index] = rc;
                   break;
                }
-               RegType type = RegType::sgpr;
-               switch (intrinsic->intrinsic) {
-               case nir_intrinsic_load_push_constant:
-               case nir_intrinsic_load_workgroup_id:
-               case nir_intrinsic_load_num_workgroups:
-               case nir_intrinsic_load_sbt_base_amd:
-               case nir_intrinsic_load_subgroup_id:
-               case nir_intrinsic_load_num_subgroups:
-               case nir_intrinsic_load_first_vertex:
-               case nir_intrinsic_load_base_instance:
-               case nir_intrinsic_vote_all:
-               case nir_intrinsic_vote_any:
-               case nir_intrinsic_read_first_invocation:
-               case nir_intrinsic_as_uniform:
-               case nir_intrinsic_read_invocation:
-               case nir_intrinsic_first_invocation:
-               case nir_intrinsic_ballot:
-               case nir_intrinsic_ballot_relaxed:
-               case nir_intrinsic_bindless_image_samples:
-               case nir_intrinsic_load_scalar_arg_amd:
-               case nir_intrinsic_load_lds_ngg_scratch_base_amd:
-               case nir_intrinsic_load_lds_ngg_gs_out_vertex_base_amd:
-               case nir_intrinsic_load_smem_amd:
-               case nir_intrinsic_unit_test_uniform_amd: type = RegType::sgpr; break;
-               case nir_intrinsic_load_sample_id:
-               case nir_intrinsic_load_input:
-               case nir_intrinsic_load_output:
-               case nir_intrinsic_load_input_vertex:
-               case nir_intrinsic_load_per_vertex_input:
-               case nir_intrinsic_load_per_vertex_output:
-               case nir_intrinsic_load_vertex_id_zero_base:
-               case nir_intrinsic_load_barycentric_sample:
-               case nir_intrinsic_load_barycentric_pixel:
-               case nir_intrinsic_load_barycentric_model:
-               case nir_intrinsic_load_barycentric_centroid:
-               case nir_intrinsic_load_barycentric_at_offset:
-               case nir_intrinsic_load_interpolated_input:
-               case nir_intrinsic_load_frag_coord:
-               case nir_intrinsic_load_frag_shading_rate:
-               case nir_intrinsic_load_sample_pos:
-               case nir_intrinsic_load_local_invocation_id:
-               case nir_intrinsic_load_local_invocation_index:
-               case nir_intrinsic_load_subgroup_invocation:
-               case nir_intrinsic_load_tess_coord:
-               case nir_intrinsic_write_invocation_amd:
-               case nir_intrinsic_mbcnt_amd:
-               case nir_intrinsic_lane_permute_16_amd:
-               case nir_intrinsic_load_instance_id:
-               case nir_intrinsic_ssbo_atomic:
-               case nir_intrinsic_ssbo_atomic_swap:
-               case nir_intrinsic_global_atomic_amd:
-               case nir_intrinsic_global_atomic_swap_amd:
-               case nir_intrinsic_bindless_image_atomic:
-               case nir_intrinsic_bindless_image_atomic_swap:
-               case nir_intrinsic_bindless_image_size:
-               case nir_intrinsic_shared_atomic:
-               case nir_intrinsic_shared_atomic_swap:
-               case nir_intrinsic_load_scratch:
-               case nir_intrinsic_load_invocation_id:
-               case nir_intrinsic_load_primitive_id:
-               case nir_intrinsic_load_typed_buffer_amd:
-               case nir_intrinsic_load_buffer_amd:
-               case nir_intrinsic_load_initial_edgeflags_amd:
-               case nir_intrinsic_gds_atomic_add_amd:
-               case nir_intrinsic_bvh64_intersect_ray_amd:
-               case nir_intrinsic_load_vector_arg_amd:
-               case nir_intrinsic_ordered_xfb_counter_add_gfx11_amd:
-               case nir_intrinsic_cmat_muladd_amd:
-               case nir_intrinsic_unit_test_divergent_amd: type = RegType::vgpr; break;
-               case nir_intrinsic_load_shared:
-               case nir_intrinsic_load_shared2_amd:
-                  /* When the result of these loads is only used by cross-lane instructions,
-                   * it is beneficial to use a VGPR destination. This is because this allows
-                   * to put the s_waitcnt further down, which decreases latency.
-                   */
-                  if (only_used_by_cross_lane_instrs(&intrinsic->def)) {
-                     type = RegType::vgpr;
+               case nir_instr_type_intrinsic: {
+                  nir_intrinsic_instr* intrinsic = nir_instr_as_intrinsic(instr);
+                  if (!nir_intrinsic_infos[intrinsic->intrinsic].has_dest)
+                     break;
+                  if (intrinsic->intrinsic == nir_intrinsic_strict_wqm_coord_amd) {
+                     regclasses[intrinsic->def.index] =
+                        RegClass::get(RegType::vgpr, intrinsic->def.num_components * 4 +
+                                                        nir_intrinsic_base(intrinsic))
+                           .as_linear();
                      break;
                   }
-                  FALLTHROUGH;
-               case nir_intrinsic_shuffle:
-               case nir_intrinsic_quad_broadcast:
-               case nir_intrinsic_quad_swap_horizontal:
-               case nir_intrinsic_quad_swap_vertical:
-               case nir_intrinsic_quad_swap_diagonal:
-               case nir_intrinsic_quad_swizzle_amd:
-               case nir_intrinsic_masked_swizzle_amd:
-               case nir_intrinsic_rotate:
-               case nir_intrinsic_inclusive_scan:
-               case nir_intrinsic_exclusive_scan:
-               case nir_intrinsic_reduce:
-               case nir_intrinsic_load_ubo:
-               case nir_intrinsic_load_ssbo:
-               case nir_intrinsic_load_global_amd:
-                  type = intrinsic->def.divergent ? RegType::vgpr : RegType::sgpr;
-                  break;
-               case nir_intrinsic_load_view_index:
-                  type = ctx->stage == fragment_fs ? RegType::vgpr : RegType::sgpr;
-                  break;
-               default:
-                  for (unsigned i = 0; i < nir_intrinsic_infos[intrinsic->intrinsic].num_srcs;
-                       i++) {
-                     if (regclasses[intrinsic->src[i].ssa->index].type() == RegType::vgpr)
+                  RegType type = RegType::sgpr;
+                  switch (intrinsic->intrinsic) {
+                  case nir_intrinsic_load_push_constant:
+                  case nir_intrinsic_load_workgroup_id:
+                  case nir_intrinsic_load_num_workgroups:
+                  case nir_intrinsic_load_ray_launch_size:
+                  case nir_intrinsic_load_sbt_base_amd:
+                  case nir_intrinsic_load_subgroup_id:
+                  case nir_intrinsic_load_num_subgroups:
+                  case nir_intrinsic_load_first_vertex:
+                  case nir_intrinsic_load_base_instance:
+                  case nir_intrinsic_vote_all:
+                  case nir_intrinsic_vote_any:
+                  case nir_intrinsic_read_first_invocation:
+                  case nir_intrinsic_as_uniform:
+                  case nir_intrinsic_read_invocation:
+                  case nir_intrinsic_first_invocation:
+                  case nir_intrinsic_ballot:
+                  case nir_intrinsic_ballot_relaxed:
+                  case nir_intrinsic_bindless_image_samples:
+                  case nir_intrinsic_load_scalar_arg_amd:
+                  case nir_intrinsic_load_lds_ngg_scratch_base_amd:
+                  case nir_intrinsic_load_lds_ngg_gs_out_vertex_base_amd:
+                  case nir_intrinsic_load_smem_amd:
+                  case nir_intrinsic_unit_test_uniform_amd: type = RegType::sgpr; break;
+                  case nir_intrinsic_load_sample_id:
+                  case nir_intrinsic_load_input:
+                  case nir_intrinsic_load_output:
+                  case nir_intrinsic_load_input_vertex:
+                  case nir_intrinsic_load_per_vertex_input:
+                  case nir_intrinsic_load_per_vertex_output:
+                  case nir_intrinsic_load_vertex_id_zero_base:
+                  case nir_intrinsic_load_barycentric_sample:
+                  case nir_intrinsic_load_barycentric_pixel:
+                  case nir_intrinsic_load_barycentric_model:
+                  case nir_intrinsic_load_barycentric_centroid:
+                  case nir_intrinsic_load_barycentric_at_offset:
+                  case nir_intrinsic_load_interpolated_input:
+                  case nir_intrinsic_load_frag_coord:
+                  case nir_intrinsic_load_frag_shading_rate:
+                  case nir_intrinsic_load_sample_pos:
+                  case nir_intrinsic_load_local_invocation_id:
+                  case nir_intrinsic_load_local_invocation_index:
+                  case nir_intrinsic_load_subgroup_invocation:
+                  case nir_intrinsic_load_ray_launch_id:
+                  case nir_intrinsic_load_tess_coord:
+                  case nir_intrinsic_write_invocation_amd:
+                  case nir_intrinsic_mbcnt_amd:
+                  case nir_intrinsic_lane_permute_16_amd:
+                  case nir_intrinsic_load_instance_id:
+                  case nir_intrinsic_ssbo_atomic:
+                  case nir_intrinsic_ssbo_atomic_swap:
+                  case nir_intrinsic_global_atomic_amd:
+                  case nir_intrinsic_global_atomic_swap_amd:
+                  case nir_intrinsic_bindless_image_atomic:
+                  case nir_intrinsic_bindless_image_atomic_swap:
+                  case nir_intrinsic_bindless_image_size:
+                  case nir_intrinsic_shared_atomic:
+                  case nir_intrinsic_shared_atomic_swap:
+                  case nir_intrinsic_load_scratch:
+                  case nir_intrinsic_load_invocation_id:
+                  case nir_intrinsic_load_primitive_id:
+                  case nir_intrinsic_load_typed_buffer_amd:
+                  case nir_intrinsic_load_buffer_amd:
+                  case nir_intrinsic_load_initial_edgeflags_amd:
+                  case nir_intrinsic_gds_atomic_add_amd:
+                  case nir_intrinsic_bvh64_intersect_ray_amd:
+                  case nir_intrinsic_load_vector_arg_amd:
+                  case nir_intrinsic_ordered_xfb_counter_add_gfx11_amd:
+                  case nir_intrinsic_cmat_muladd_amd:
+                  case nir_intrinsic_unit_test_divergent_amd: type = RegType::vgpr; break;
+                  case nir_intrinsic_load_shared:
+                  case nir_intrinsic_load_shared2_amd:
+                     /* When the result of these loads is only used by cross-lane instructions,
+                      * it is beneficial to use a VGPR destination. This is because this allows
+                      * to put the s_waitcnt further down, which decreases latency.
+                      */
+                     if (only_used_by_cross_lane_instrs(&intrinsic->def)) {
                         type = RegType::vgpr;
+                        break;
+                     }
+                     FALLTHROUGH;
+                  case nir_intrinsic_shuffle:
+                  case nir_intrinsic_quad_broadcast:
+                  case nir_intrinsic_quad_swap_horizontal:
+                  case nir_intrinsic_quad_swap_vertical:
+                  case nir_intrinsic_quad_swap_diagonal:
+                  case nir_intrinsic_quad_swizzle_amd:
+                  case nir_intrinsic_masked_swizzle_amd:
+                  case nir_intrinsic_rotate:
+                  case nir_intrinsic_inclusive_scan:
+                  case nir_intrinsic_exclusive_scan:
+                  case nir_intrinsic_reduce:
+                  case nir_intrinsic_load_ubo:
+                  case nir_intrinsic_load_ssbo:
+                  case nir_intrinsic_load_global_amd:
+                     type = intrinsic->def.divergent ? RegType::vgpr : RegType::sgpr;
+                     break;
+                  case nir_intrinsic_load_view_index:
+                     type = ctx->stage == fragment_fs ? RegType::vgpr : RegType::sgpr;
+                     break;
+                  default:
+                     for (unsigned i = 0; i < nir_intrinsic_infos[intrinsic->intrinsic].num_srcs;
+                          i++) {
+                        if (regclasses[intrinsic->src[i].ssa->index].type() == RegType::vgpr)
+                           type = RegType::vgpr;
+                     }
+                     break;
                   }
+                  RegClass rc = get_reg_class(ctx, type, intrinsic->def.num_components,
+                                              intrinsic->def.bit_size);
+                  regclasses[intrinsic->def.index] = rc;
                   break;
                }
-               RegClass rc =
-                  get_reg_class(ctx, type, intrinsic->def.num_components, intrinsic->def.bit_size);
-               regclasses[intrinsic->def.index] = rc;
-               break;
-            }
-            case nir_instr_type_tex: {
-               nir_tex_instr* tex = nir_instr_as_tex(instr);
-               RegType type = tex->def.divergent ? RegType::vgpr : RegType::sgpr;
-
-               if (tex->op == nir_texop_texture_samples) {
-                  assert(!tex->def.divergent);
-               }
+               case nir_instr_type_tex: {
+                  nir_tex_instr* tex = nir_instr_as_tex(instr);
+                  RegType type = tex->def.divergent ? RegType::vgpr : RegType::sgpr;
 
-               RegClass rc = get_reg_class(ctx, type, tex->def.num_components, tex->def.bit_size);
-               regclasses[tex->def.index] = rc;
-               break;
-            }
-            case nir_instr_type_undef: {
-               unsigned num_components = nir_instr_as_undef(instr)->def.num_components;
-               unsigned bit_size = nir_instr_as_undef(instr)->def.bit_size;
-               RegClass rc = get_reg_class(ctx, RegType::sgpr, num_components, bit_size);
-               regclasses[nir_instr_as_undef(instr)->def.index] = rc;
-               break;
-            }
-            case nir_instr_type_phi: {
-               nir_phi_instr* phi = nir_instr_as_phi(instr);
-               RegType type = RegType::sgpr;
-               unsigned num_components = phi->def.num_components;
-               assert((phi->def.bit_size != 1 || num_components == 1) &&
-                      "Multiple components not supported on boolean phis.");
-
-               if (phi->def.divergent) {
-                  type = RegType::vgpr;
-               } else {
-                  nir_foreach_phi_src (src, phi) {
-                     if (regclasses[src->src.ssa->index].type() == RegType::vgpr)
-                        type = RegType::vgpr;
+                  if (tex->op == nir_texop_texture_samples) {
+                     assert(!tex->def.divergent);
                   }
+
+                  RegClass rc =
+                     get_reg_class(ctx, type, tex->def.num_components, tex->def.bit_size);
+                  regclasses[tex->def.index] = rc;
+                  break;
                }
+               case nir_instr_type_undef: {
+                  unsigned num_components = nir_instr_as_undef(instr)->def.num_components;
+                  unsigned bit_size = nir_instr_as_undef(instr)->def.bit_size;
+                  RegClass rc = get_reg_class(ctx, RegType::sgpr, num_components, bit_size);
+                  regclasses[nir_instr_as_undef(instr)->def.index] = rc;
+                  break;
+               }
+               case nir_instr_type_phi: {
+                  nir_phi_instr* phi = nir_instr_as_phi(instr);
+                  RegType type = RegType::sgpr;
+                  unsigned num_components = phi->def.num_components;
+                  assert((phi->def.bit_size != 1 || num_components == 1) &&
+                         "Multiple components not supported on boolean phis.");
+
+                  if (phi->def.divergent) {
+                     type = RegType::vgpr;
+                  } else {
+                     nir_foreach_phi_src (src, phi) {
+                        if (regclasses[src->src.ssa->index].type() == RegType::vgpr)
+                           type = RegType::vgpr;
+                     }
+                  }
 
-               RegClass rc = get_reg_class(ctx, type, num_components, phi->def.bit_size);
-               if (rc != regclasses[phi->def.index])
-                  done = false;
-               regclasses[phi->def.index] = rc;
-               break;
-            }
-            case nir_instr_type_call: {
-               ++call_count;
-               break;
-            }
-            default: break;
+                  RegClass rc = get_reg_class(ctx, type, num_components, phi->def.bit_size);
+                  if (rc != regclasses[phi->def.index])
+                     done = false;
+                  regclasses[phi->def.index] = rc;
+                  break;
+               }
+               case nir_instr_type_call: {
+                  ++call_count;
+                  break;
+               }
+               default: break;
+               }
             }
          }
       }
-   }
 
-   ctx->call_infos.reserve(call_count);
-
-   ctx->program->config->spi_ps_input_ena = ctx->program->info.ps.spi_ps_input_ena;
-   ctx->program->config->spi_ps_input_addr = ctx->program->info.ps.spi_ps_input_addr;
+      ctx->call_infos.reserve(call_count);
 
+      ctx->program->config->spi_ps_input_ena = ctx->program->info.ps.spi_ps_input_ena;
+      ctx->program->config->spi_ps_input_addr = ctx->program->info.ps.spi_ps_input_addr;
+   }
    /* align and copy constant data */
    while (ctx->program->constant_data.size() % 4u)
       ctx->program->constant_data.push_back(0);
@@ -685,7 +688,8 @@ setup_isel_context(Program* program, unsigned shader_count, struct nir_shader* c
 
    unsigned nir_num_blocks = 0;
    for (unsigned i = 0; i < shader_count; i++)
-      nir_num_blocks += nir_shader_get_entrypoint(shaders[i])->num_blocks;
+      nir_foreach_function_impl (impl, shaders[i])
+         nir_num_blocks += impl->num_blocks;
    ctx.program->blocks.reserve(nir_num_blocks * 2);
    ctx.block = ctx.program->create_and_insert_block();
    ctx.block->kind = block_kind_top_level;
-- 
GitLab


From 6dc63eceb7649554c9a67ea1649fd14dd0e6e4a4 Mon Sep 17 00:00:00 2001
From: Friedrich Vock <friedrich.vock@gmx.de>
Date: Sat, 23 Mar 2024 11:20:58 +0100
Subject: [PATCH 38/71] aco: Add param temps in startpgm

---
 src/amd/compiler/aco_assembler.cpp            |  3 +-
 .../compiler/aco_instruction_selection.cpp    | 35 ++++++++++++++++++-
 src/amd/compiler/aco_ir.h                     |  1 +
 3 files changed, 37 insertions(+), 2 deletions(-)

diff --git a/src/amd/compiler/aco_assembler.cpp b/src/amd/compiler/aco_assembler.cpp
index 203e871959593..935245e3c5971 100644
--- a/src/amd/compiler/aco_assembler.cpp
+++ b/src/amd/compiler/aco_assembler.cpp
@@ -1733,7 +1733,8 @@ emit_program(Program* program, std::vector<uint32_t>& code, std::vector<struct a
                (uint32_t*)(program->constant_data.data() + program->constant_data.size()));
 
    program->config->scratch_bytes_per_wave =
-      align(program->config->scratch_bytes_per_wave, program->dev.scratch_alloc_granule);
+      align(program->config->scratch_bytes_per_wave + program->scratch_arg_size,
+            program->dev.scratch_alloc_granule);
 
    return exec_size;
 }
diff --git a/src/amd/compiler/aco_instruction_selection.cpp b/src/amd/compiler/aco_instruction_selection.cpp
index a12b9ec5c9b53..e9755e1eeb75b 100644
--- a/src/amd/compiler/aco_instruction_selection.cpp
+++ b/src/amd/compiler/aco_instruction_selection.cpp
@@ -11241,8 +11241,12 @@ get_callee_info(const ABI& abi, unsigned param_count, const nir_parameter* param
 }
 
 Instruction*
-add_startpgm(struct isel_context* ctx)
+add_startpgm(struct isel_context* ctx, bool is_callee = false)
 {
+   ctx->program->arg_sgpr_count = ctx->args->num_sgprs_used;
+   ctx->program->arg_vgpr_count = ctx->args->num_vgprs_used;
+   ctx->program->scratch_arg_size += ctx->callee_info.scratch_param_size;
+
    unsigned def_count = 0;
    for (unsigned i = 0; i < ctx->args->arg_count; i++) {
       if (ctx->args->args[i].skip)
@@ -11253,6 +11257,9 @@ add_startpgm(struct isel_context* ctx)
       else
          def_count++;
    }
+   unsigned used_arg_count = def_count;
+   def_count +=
+      ctx->callee_info.reg_param_count + (is_callee ? 2 : 0); /* parameters + return address */
 
    if (ctx->stage.hw == AC_HW_COMPUTE_SHADER && ctx->program->gfx_level >= GFX12)
       def_count += 3;
@@ -11318,6 +11325,32 @@ add_startpgm(struct isel_context* ctx)
          ctx->workgroup_id[i] = ids[i].used ? Operand(get_arg(ctx, ids[i])) : Operand::zero();
    }
 
+   if (is_callee) {
+      unsigned param_sgpr = align(ctx->args->num_sgprs_used, 2);
+      unsigned def_idx = used_arg_count;
+
+      Temp stack_tmp = ctx->program->allocateTmp(s1);
+      ctx->callee_info.stack_ptr.is_reg = true;
+      ctx->callee_info.stack_ptr.def = Definition(stack_tmp);
+      ctx->callee_info.stack_ptr.def.setFixed(PhysReg{param_sgpr});
+      startpgm->definitions[def_idx++] = ctx->callee_info.stack_ptr.def;
+      param_sgpr += 2; // only need 1 but 64byte alignment gets screwed up otherwise
+      ctx->program->stack_ptr = stack_tmp;
+
+      Temp return_tmp = ctx->program->allocateTmp(s2);
+      ctx->callee_info.return_address.is_reg = true;
+      ctx->callee_info.return_address.def = Definition(return_tmp);
+      ctx->callee_info.return_address.def.setFixed(PhysReg{param_sgpr});
+      startpgm->definitions[def_idx++] = ctx->callee_info.return_address.def;
+      param_sgpr += 2;
+
+      for (auto& def : ctx->callee_info.param_infos) {
+         if (!def.second.is_reg)
+            continue;
+         startpgm->definitions[def_idx++] = def.second.def;
+      }
+   }
+
    /* epilog has no scratch */
    if (ctx->args->scratch_offset.used) {
       if (ctx->program->gfx_level < GFX9) {
diff --git a/src/amd/compiler/aco_ir.h b/src/amd/compiler/aco_ir.h
index c749ea085ff77..9fe81e5a17d09 100644
--- a/src/amd/compiler/aco_ir.h
+++ b/src/amd/compiler/aco_ir.h
@@ -2332,6 +2332,7 @@ public:
    ABI callee_abi = {};
    unsigned short arg_sgpr_count;
    unsigned short arg_vgpr_count;
+   unsigned scratch_arg_size = 0;
 
    struct {
       FILE* output = stderr;
-- 
GitLab


From 4014a40b349451c25a11cb9b982fadafa932fdbf Mon Sep 17 00:00:00 2001
From: Friedrich Vock <friedrich.vock@gmx.de>
Date: Wed, 6 Mar 2024 13:27:56 +0100
Subject: [PATCH 39/71] aco: Implement call parameter intrinsics

---
 .../compiler/aco_instruction_selection.cpp    | 156 ++++++++++++++++++
 .../aco_instruction_selection_setup.cpp       |  10 ++
 2 files changed, 166 insertions(+)

diff --git a/src/amd/compiler/aco_instruction_selection.cpp b/src/amd/compiler/aco_instruction_selection.cpp
index e9755e1eeb75b..ad92695d9111d 100644
--- a/src/amd/compiler/aco_instruction_selection.cpp
+++ b/src/amd/compiler/aco_instruction_selection.cpp
@@ -8192,6 +8192,105 @@ visit_cmat_muladd(isel_context* ctx, nir_intrinsic_instr* instr)
    emit_split_vector(ctx, dst, instr->def.num_components);
 }
 
+void
+load_scratch_param(isel_context* ctx, Builder& bld, const parameter_info& param, Temp stack_ptr,
+                   unsigned scratch_param_size, Temp dst)
+{
+   int32_t const_offset = param.scratch_offset - scratch_param_size;
+   unsigned byte_size = dst.bytes();
+   if (ctx->program->gfx_level < GFX9) {
+      Temp scratch_rsrc = load_scratch_resource(ctx->program, bld, true, false);
+
+      Temp soffset = bld.sop2(aco_opcode::s_sub_i32, bld.def(s1), bld.def(s1, scc),
+                              stack_ptr == Temp() ? Operand::c32(0) : Operand(stack_ptr),
+                              Operand::c32(-const_offset * ctx->program->wave_size));
+
+      aco_opcode op;
+      switch (byte_size) {
+      case 4: op = aco_opcode::buffer_load_dword; break;
+      case 8: op = aco_opcode::buffer_load_dwordx2; break;
+      case 12: op = aco_opcode::buffer_load_dwordx3; break;
+      case 16: op = aco_opcode::buffer_load_dwordx4; break;
+      default: unreachable("Unexpected param size");
+      }
+
+      Instruction* instr =
+         bld.mubuf(op, Definition(dst), scratch_rsrc, Operand(v1), soffset, 0, false, true);
+      instr->mubuf().sync = memory_sync_info(storage_scratch);
+      return;
+   }
+
+   if (const_offset < ctx->program->dev.scratch_global_offset_min) {
+      stack_ptr = bld.sop2(aco_opcode::s_add_u32, bld.def(s1), bld.def(s1, scc),
+                           stack_ptr == Temp() ? Operand::c32(0) : Operand(stack_ptr),
+                           Operand::c32(const_offset));
+      const_offset = 0;
+   }
+
+   aco_opcode op;
+   switch (byte_size) {
+   case 4: op = aco_opcode::scratch_load_dword; break;
+   case 8: op = aco_opcode::scratch_load_dwordx2; break;
+   case 12: op = aco_opcode::scratch_load_dwordx3; break;
+   case 16: op = aco_opcode::scratch_load_dwordx4; break;
+   default: unreachable("Unexpected param size");
+   }
+
+   bld.scratch(op, Definition(dst), Operand(v1),
+               stack_ptr == Temp() ? Operand(s1) : Operand(stack_ptr), (int16_t)const_offset,
+               memory_sync_info(storage_scratch));
+}
+
+void
+store_scratch_param(isel_context* ctx, Builder& bld, const parameter_info& param, Temp stack_ptr,
+                    unsigned scratch_param_size, Temp data)
+{
+   int32_t const_offset = param.scratch_offset - scratch_param_size;
+   unsigned byte_size = data.bytes();
+   if (ctx->program->gfx_level < GFX9) {
+      Temp scratch_rsrc = load_scratch_resource(ctx->program, bld, true, false);
+
+      Temp soffset = bld.sop2(aco_opcode::s_sub_i32, bld.def(s1), bld.def(s1, scc),
+                              stack_ptr == Temp() ? Operand::c32(0) : Operand(stack_ptr),
+                              Operand::c32(-const_offset * ctx->program->wave_size));
+
+      assert(-const_offset * ctx->program->wave_size < 0x1ff00);
+
+      aco_opcode op;
+      switch (byte_size) {
+      case 4: op = aco_opcode::buffer_store_dword; break;
+      case 8: op = aco_opcode::buffer_store_dwordx2; break;
+      case 12: op = aco_opcode::buffer_store_dwordx3; break;
+      case 16: op = aco_opcode::buffer_store_dwordx4; break;
+      default: unreachable("Unexpected param size");
+      }
+
+      Instruction* instr = bld.mubuf(op, scratch_rsrc, Operand(v1), Operand(soffset),
+                                     as_vgpr(bld, data), 0, false, true);
+      instr->mubuf().sync = memory_sync_info(storage_scratch);
+      return;
+   }
+
+   if (const_offset < ctx->program->dev.scratch_global_offset_min) {
+      stack_ptr = bld.sop2(aco_opcode::s_add_u32, bld.def(s1), bld.def(s1, scc),
+                           stack_ptr == Temp() ? Operand::c32(0) : Operand(stack_ptr),
+                           Operand::c32(const_offset));
+      const_offset = 0;
+   }
+
+   aco_opcode op;
+   switch (byte_size) {
+   case 4: op = aco_opcode::scratch_store_dword; break;
+   case 8: op = aco_opcode::scratch_store_dwordx2; break;
+   case 12: op = aco_opcode::scratch_store_dwordx3; break;
+   case 16: op = aco_opcode::scratch_store_dwordx4; break;
+   default: unreachable("Unexpected param size");
+   }
+
+   bld.scratch(op, Operand(v1), stack_ptr == Temp() ? Operand(s1) : Operand(stack_ptr),
+               as_vgpr(bld, data), (int16_t)const_offset, memory_sync_info(storage_scratch));
+}
+
 void
 visit_intrinsic(isel_context* ctx, nir_intrinsic_instr* instr)
 {
@@ -9372,6 +9471,63 @@ visit_intrinsic(isel_context* ctx, nir_intrinsic_instr* instr)
       bld.pseudo(aco_opcode::p_unit_test, Definition(get_ssa_temp(ctx, &instr->def)),
                  Operand::c32(nir_intrinsic_base(instr)));
       break;
+   case nir_intrinsic_load_return_param_amd: {
+      call_info& info = ctx->call_infos[nir_intrinsic_call_idx(instr)];
+
+      assert(nir_intrinsic_param_idx(instr) < info.nir_instr->callee->num_params);
+
+      unsigned index_in_return_params = 0u;
+      for (unsigned i = 0; i < info.nir_instr->callee->num_params; ++i) {
+         if (nir_intrinsic_param_idx(instr) == i) {
+            assert(info.nir_instr->callee->params[i].is_return);
+            break;
+         }
+         if (info.nir_instr->callee->params[i].is_return) {
+            ++index_in_return_params;
+         }
+      }
+
+      if (info.return_info[index_in_return_params].is_reg) {
+         bld.copy(Definition(get_ssa_temp(ctx, &instr->def)),
+                  Operand(info.return_info[index_in_return_params].def.getTemp()));
+      } else {
+         Temp stack_ptr;
+         if (ctx->callee_info.stack_ptr.is_reg)
+            stack_ptr = bld.pseudo(aco_opcode::p_callee_stack_ptr, bld.def(s1),
+                                   Operand::c32(info.scratch_param_size),
+                                   Operand(ctx->callee_info.stack_ptr.def.getTemp()));
+         else
+            stack_ptr = bld.pseudo(aco_opcode::p_callee_stack_ptr, bld.def(s1),
+                                   Operand::c32(info.scratch_param_size));
+         load_scratch_param(ctx, bld, info.return_info[index_in_return_params], stack_ptr,
+                            info.scratch_param_size, get_ssa_temp(ctx, &instr->def));
+      }
+      break;
+   }
+   case nir_intrinsic_load_param: {
+      auto param = ctx->callee_info.param_infos.find(nir_intrinsic_param_idx(instr));
+      assert(param != ctx->callee_info.param_infos.end());
+      if (param->second.is_reg)
+         bld.copy(Definition(get_ssa_temp(ctx, &instr->def)), Operand(param->second.def.getTemp()));
+      else
+         load_scratch_param(
+            ctx, bld, param->second,
+            ctx->callee_info.stack_ptr.is_reg ? ctx->callee_info.stack_ptr.def.getTemp() : Temp(),
+            ctx->callee_info.scratch_param_size, get_ssa_temp(ctx, &instr->def));
+      break;
+   }
+   case nir_intrinsic_store_param_amd: {
+      auto param = ctx->callee_info.param_infos.find(nir_intrinsic_param_idx(instr));
+      assert(param != ctx->callee_info.param_infos.end());
+      if (param->second.is_reg)
+         param->second.def.setTemp(as_vgpr(ctx, get_ssa_temp(ctx, instr->src[0].ssa)));
+      else
+         store_scratch_param(
+            ctx, bld, param->second,
+            ctx->callee_info.stack_ptr.is_reg ? ctx->callee_info.stack_ptr.def.getTemp() : Temp(),
+            ctx->callee_info.scratch_param_size, get_ssa_temp(ctx, instr->src[0].ssa));
+      break;
+   }
    case nir_intrinsic_load_call_return_address_amd: {
       bld.copy(Definition(get_ssa_temp(ctx, &instr->def)),
                Operand(ctx->callee_info.return_address.def.getTemp()));
diff --git a/src/amd/compiler/aco_instruction_selection_setup.cpp b/src/amd/compiler/aco_instruction_selection_setup.cpp
index a953a5621fc8b..c19ed81775a46 100644
--- a/src/amd/compiler/aco_instruction_selection_setup.cpp
+++ b/src/amd/compiler/aco_instruction_selection_setup.cpp
@@ -540,6 +540,16 @@ init_context(isel_context* ctx, nir_shader* shader)
                   case nir_intrinsic_load_view_index:
                      type = ctx->stage == fragment_fs ? RegType::vgpr : RegType::sgpr;
                      break;
+                  case nir_intrinsic_load_return_param_amd: {
+                     type = RegType::vgpr;
+                     break;
+                  }
+                  case nir_intrinsic_load_param: {
+                     type = impl->function->params[nir_intrinsic_param_idx(intrinsic)].is_divergent
+                               ? RegType::vgpr
+                               : RegType::sgpr;
+                     break;
+                  }
                   default:
                      for (unsigned i = 0; i < nir_intrinsic_infos[intrinsic->intrinsic].num_srcs;
                           i++) {
-- 
GitLab


From e9c0c6be68a58be8a8cfbb5f7c97a8b60d34fb52 Mon Sep 17 00:00:00 2001
From: Friedrich Vock <friedrich.vock@gmx.de>
Date: Thu, 6 Jun 2024 07:17:15 +0200
Subject: [PATCH 40/71] aco: Add common utility to load scratch descriptor

Also modifies the scratch descriptor to take the stack pointer into
account.
---
 .../compiler/aco_instruction_selection.cpp    | 40 +--------
 src/amd/compiler/aco_scratch_rsrc.h           | 82 +++++++++++++++++++
 src/amd/compiler/aco_spill.cpp                | 54 +-----------
 3 files changed, 87 insertions(+), 89 deletions(-)
 create mode 100644 src/amd/compiler/aco_scratch_rsrc.h

diff --git a/src/amd/compiler/aco_instruction_selection.cpp b/src/amd/compiler/aco_instruction_selection.cpp
index ad92695d9111d..2c6c03e19a3dc 100644
--- a/src/amd/compiler/aco_instruction_selection.cpp
+++ b/src/amd/compiler/aco_instruction_selection.cpp
@@ -10,6 +10,7 @@
 #include "aco_builder.h"
 #include "aco_interface.h"
 #include "aco_ir.h"
+#include "aco_scratch_rsrc.h"
 
 #include "common/ac_descriptors.h"
 #include "common/ac_nir.h"
@@ -7551,41 +7552,6 @@ visit_access_shared2_amd(isel_context* ctx, nir_intrinsic_instr* instr)
    }
 }
 
-Temp
-get_scratch_resource(isel_context* ctx)
-{
-   Builder bld(ctx->program, ctx->block);
-   Temp scratch_addr = ctx->program->private_segment_buffer;
-   if (!scratch_addr.bytes()) {
-      Temp addr_lo =
-         bld.sop1(aco_opcode::p_load_symbol, bld.def(s1), Operand::c32(aco_symbol_scratch_addr_lo));
-      Temp addr_hi =
-         bld.sop1(aco_opcode::p_load_symbol, bld.def(s1), Operand::c32(aco_symbol_scratch_addr_hi));
-      scratch_addr = bld.pseudo(aco_opcode::p_create_vector, bld.def(s2), addr_lo, addr_hi);
-   } else if (ctx->stage.hw != AC_HW_COMPUTE_SHADER) {
-      scratch_addr =
-         bld.smem(aco_opcode::s_load_dwordx2, bld.def(s2), scratch_addr, Operand::zero());
-   }
-
-   struct ac_buffer_state ac_state = {0};
-   uint32_t desc[4];
-
-   ac_state.size = 0xffffffff;
-   ac_state.format = PIPE_FORMAT_R32_FLOAT;
-   for (int i = 0; i < 4; i++)
-      ac_state.swizzle[i] = PIPE_SWIZZLE_0;
-   /* older generations need element size = 4 bytes. element size removed in GFX9 */
-   ac_state.element_size = ctx->program->gfx_level <= GFX8 ? 1u : 0u;
-   ac_state.index_stride = ctx->program->wave_size == 64 ? 3u : 2u;
-   ac_state.add_tid = true;
-   ac_state.gfx10_oob_select = V_008F0C_OOB_SELECT_RAW;
-
-   ac_build_buffer_descriptor(ctx->program->gfx_level, &ac_state, desc);
-
-   return bld.pseudo(aco_opcode::p_create_vector, bld.def(s4), scratch_addr, Operand::c32(desc[2]),
-                     Operand::c32(desc[3]));
-}
-
 void
 visit_load_scratch(isel_context* ctx, nir_intrinsic_instr* instr)
 {
@@ -7627,7 +7593,7 @@ visit_load_scratch(isel_context* ctx, nir_intrinsic_instr* instr)
       params.max_const_offset_plus_one = ctx->program->dev.scratch_global_offset_max + 1;
       emit_load(ctx, bld, info, params);
    } else {
-      info.resource = get_scratch_resource(ctx);
+      info.resource = load_scratch_resource(ctx->program, bld, false, true);
       info.offset = Operand(as_vgpr(ctx, get_ssa_temp(ctx, instr->src[0].ssa)));
       info.soffset = ctx->program->scratch_offset;
       emit_load(ctx, bld, info, scratch_mubuf_load_params);
@@ -7690,7 +7656,7 @@ visit_store_scratch(isel_context* ctx, nir_intrinsic_instr* instr)
                      memory_sync_info(storage_scratch, semantic_private));
       }
    } else {
-      Temp rsrc = get_scratch_resource(ctx);
+      Temp rsrc = load_scratch_resource(ctx->program, bld, false, true);
       offset = as_vgpr(ctx, offset);
       for (unsigned i = 0; i < write_count; i++) {
          aco_opcode op = get_buffer_store_op(write_datas[i].bytes());
diff --git a/src/amd/compiler/aco_scratch_rsrc.h b/src/amd/compiler/aco_scratch_rsrc.h
new file mode 100644
index 0000000000000..5b0af2bca46f0
--- /dev/null
+++ b/src/amd/compiler/aco_scratch_rsrc.h
@@ -0,0 +1,82 @@
+/*
+ * Copyright Â© 2024 Valve Corporation.
+ *
+ * SPDX-License-Identifier: MIT
+ */
+
+#include "aco_builder.h"
+#include "aco_ir.h"
+
+#include "ac_descriptors.h"
+#include "amdgfxregs.h"
+
+#ifndef ACO_SCRATCH_RSRC_H
+#define ACO_SCRATCH_RSRC_H
+
+namespace aco {
+
+inline Temp
+load_scratch_resource(Program* program, Builder& bld, bool apply_scratch_offset,
+                      bool apply_stack_ptr)
+{
+   Temp private_segment_buffer = program->private_segment_buffer;
+   if (!private_segment_buffer.bytes()) {
+      Temp addr_lo =
+         bld.sop1(aco_opcode::p_load_symbol, bld.def(s1), Operand::c32(aco_symbol_scratch_addr_lo));
+      Temp addr_hi =
+         bld.sop1(aco_opcode::p_load_symbol, bld.def(s1), Operand::c32(aco_symbol_scratch_addr_hi));
+      private_segment_buffer =
+         bld.pseudo(aco_opcode::p_create_vector, bld.def(s2), addr_lo, addr_hi);
+   } else if (program->stage.hw != AC_HW_COMPUTE_SHADER) {
+      private_segment_buffer =
+         bld.smem(aco_opcode::s_load_dwordx2, bld.def(s2), private_segment_buffer, Operand::zero());
+   }
+
+   if ((apply_stack_ptr && program->stack_ptr != Temp()) || apply_scratch_offset) {
+      Temp addr_lo = bld.tmp(s1);
+      Temp addr_hi = bld.tmp(s1);
+      bld.pseudo(aco_opcode::p_split_vector, Definition(addr_lo), Definition(addr_hi),
+                 private_segment_buffer);
+
+      if (apply_stack_ptr && program->stack_ptr != Temp()) {
+         Temp carry = bld.tmp(s1);
+         addr_lo = bld.sop2(aco_opcode::s_add_u32, bld.def(s1), bld.scc(Definition(carry)), addr_lo,
+                            program->stack_ptr);
+         addr_hi = bld.sop2(aco_opcode::s_addc_u32, bld.def(s1), bld.def(s1, scc), addr_hi,
+                            Operand::c32(0), bld.scc(carry));
+      }
+
+      if (apply_scratch_offset) {
+         Temp carry = bld.tmp(s1);
+         addr_lo = bld.sop2(aco_opcode::s_add_u32, bld.def(s1), bld.scc(Definition(carry)), addr_lo,
+                            program->scratch_offset);
+         addr_hi = bld.sop2(aco_opcode::s_addc_u32, bld.def(s1), bld.def(s1, scc), addr_hi,
+                            Operand::c32(0), bld.scc(carry));
+      }
+
+      private_segment_buffer =
+         bld.pseudo(aco_opcode::p_create_vector, bld.def(s2), addr_lo, addr_hi);
+   }
+
+   struct ac_buffer_state ac_state = {0};
+   uint32_t desc[4];
+
+   ac_state.size = 0xffffffff;
+   ac_state.format = PIPE_FORMAT_R32_FLOAT;
+   for (int i = 0; i < 4; i++)
+      ac_state.swizzle[i] = PIPE_SWIZZLE_0;
+   /* older generations need element size = 4 bytes. element size removed in GFX9 */
+   ac_state.element_size = program->gfx_level <= GFX8 ? 1u : 0u;
+   ac_state.index_stride = program->wave_size == 64 ? 3u : 2u;
+   ac_state.add_tid = true;
+   ac_state.gfx10_oob_select = V_008F0C_OOB_SELECT_RAW;
+
+   ac_build_buffer_descriptor(program->gfx_level, &ac_state, desc);
+
+   return bld.pseudo(aco_opcode::p_create_vector, bld.def(s4), private_segment_buffer,
+                     Operand::c32(desc[2]), Operand::c32(desc[3]));
+}
+
+} // namespace aco
+
+#endif // ACO_SCRATCH_RSRC_H
diff --git a/src/amd/compiler/aco_spill.cpp b/src/amd/compiler/aco_spill.cpp
index d6f57c33c70c0..f38c2b497fcc6 100644
--- a/src/amd/compiler/aco_spill.cpp
+++ b/src/amd/compiler/aco_spill.cpp
@@ -7,6 +7,7 @@
 
 #include "aco_builder.h"
 #include "aco_ir.h"
+#include "aco_scratch_rsrc.h"
 #include "aco_util.h"
 
 #include "common/ac_descriptors.h"
@@ -1215,57 +1216,6 @@ spill_block(spill_ctx& ctx, unsigned block_idx)
    }
 }
 
-Temp
-load_scratch_resource(spill_ctx& ctx, Builder& bld, bool apply_scratch_offset)
-{
-   Temp private_segment_buffer = ctx.program->private_segment_buffer;
-   if (!private_segment_buffer.bytes()) {
-      Temp addr_lo =
-         bld.sop1(aco_opcode::p_load_symbol, bld.def(s1), Operand::c32(aco_symbol_scratch_addr_lo));
-      Temp addr_hi =
-         bld.sop1(aco_opcode::p_load_symbol, bld.def(s1), Operand::c32(aco_symbol_scratch_addr_hi));
-      private_segment_buffer =
-         bld.pseudo(aco_opcode::p_create_vector, bld.def(s2), addr_lo, addr_hi);
-   } else if (ctx.program->stage.hw != AC_HW_COMPUTE_SHADER) {
-      private_segment_buffer =
-         bld.smem(aco_opcode::s_load_dwordx2, bld.def(s2), private_segment_buffer, Operand::zero());
-   }
-
-   if (apply_scratch_offset) {
-      Temp addr_lo = bld.tmp(s1);
-      Temp addr_hi = bld.tmp(s1);
-      bld.pseudo(aco_opcode::p_split_vector, Definition(addr_lo), Definition(addr_hi),
-                 private_segment_buffer);
-
-      Temp carry = bld.tmp(s1);
-      addr_lo = bld.sop2(aco_opcode::s_add_u32, bld.def(s1), bld.scc(Definition(carry)), addr_lo,
-                         ctx.program->scratch_offset);
-      addr_hi = bld.sop2(aco_opcode::s_addc_u32, bld.def(s1), bld.def(s1, scc), addr_hi,
-                         Operand::c32(0), bld.scc(carry));
-
-      private_segment_buffer =
-         bld.pseudo(aco_opcode::p_create_vector, bld.def(s2), addr_lo, addr_hi);
-   }
-
-   struct ac_buffer_state ac_state = {0};
-   uint32_t desc[4];
-
-   ac_state.size = 0xffffffff;
-   ac_state.format = PIPE_FORMAT_R32_FLOAT;
-   for (int i = 0; i < 4; i++)
-      ac_state.swizzle[i] = PIPE_SWIZZLE_0;
-   /* older generations need element size = 4 bytes. element size removed in GFX9 */
-   ac_state.element_size = ctx.program->gfx_level <= GFX8 ? 1u : 0u;
-   ac_state.index_stride = ctx.program->wave_size == 64 ? 3u : 2u;
-   ac_state.add_tid = true;
-   ac_state.gfx10_oob_select = V_008F0C_OOB_SELECT_RAW;
-
-   ac_build_buffer_descriptor(ctx.program->gfx_level, &ac_state, desc);
-
-   return bld.pseudo(aco_opcode::p_create_vector, bld.def(s4), private_segment_buffer,
-                     Operand::c32(desc[2]), Operand::c32(desc[3]));
-}
-
 void
 setup_vgpr_spill_reload(spill_ctx& ctx, Block& block,
                         std::vector<aco_ptr<Instruction>>& instructions, uint32_t spill_slot,
@@ -1330,7 +1280,7 @@ setup_vgpr_spill_reload(spill_ctx& ctx, Block& block,
       }
    } else {
       if (ctx.scratch_rsrc == Temp())
-         ctx.scratch_rsrc = load_scratch_resource(ctx, rsrc_bld, overflow);
+         ctx.scratch_rsrc = load_scratch_resource(ctx.program, rsrc_bld, overflow, true);
 
       if (overflow) {
          uint32_t soffset =
-- 
GitLab


From c35da5c6c5e49be19f6d56c76292dab9ac508eb6 Mon Sep 17 00:00:00 2001
From: Friedrich Vock <friedrich.vock@gmx.de>
Date: Mon, 13 May 2024 06:14:32 +0200
Subject: [PATCH 41/71] aco: Add Program::is_callee and set it for RT shaders

---
 src/amd/compiler/aco_instruction_selection.cpp | 2 ++
 src/amd/compiler/aco_ir.h                      | 1 +
 2 files changed, 3 insertions(+)

diff --git a/src/amd/compiler/aco_instruction_selection.cpp b/src/amd/compiler/aco_instruction_selection.cpp
index 2c6c03e19a3dc..3d38d9d8bce15 100644
--- a/src/amd/compiler/aco_instruction_selection.cpp
+++ b/src/amd/compiler/aco_instruction_selection.cpp
@@ -11735,6 +11735,8 @@ select_program_rt(isel_context& ctx, unsigned shader_count, struct nir_shader* c
          init_context(&ctx, nir);
          setup_fp_mode(&ctx, nir);
 
+         ctx.program->is_callee = true;
+
          Instruction* startpgm = add_startpgm(&ctx);
          append_logical_start(ctx.block);
          split_arguments(&ctx, startpgm);
diff --git a/src/amd/compiler/aco_ir.h b/src/amd/compiler/aco_ir.h
index 9fe81e5a17d09..13aec1fdf9858 100644
--- a/src/amd/compiler/aco_ir.h
+++ b/src/amd/compiler/aco_ir.h
@@ -2329,6 +2329,7 @@ public:
    /* For shader part with previous shader part that has lds access. */
    bool pending_lds_access = false;
 
+   bool is_callee = false;
    ABI callee_abi = {};
    unsigned short arg_sgpr_count;
    unsigned short arg_vgpr_count;
-- 
GitLab


From 950e2beccf8c107833ef119d2df7b3ad73b1f223 Mon Sep 17 00:00:00 2001
From: Friedrich Vock <friedrich.vock@gmx.de>
Date: Wed, 6 Mar 2024 14:56:06 +0100
Subject: [PATCH 42/71] aco/isel: Use function call structure for RT programs

---
 .../compiler/aco_instruction_selection.cpp    | 120 +++++++++++++-----
 1 file changed, 87 insertions(+), 33 deletions(-)

diff --git a/src/amd/compiler/aco_instruction_selection.cpp b/src/amd/compiler/aco_instruction_selection.cpp
index 3d38d9d8bce15..a00dfe76d4948 100644
--- a/src/amd/compiler/aco_instruction_selection.cpp
+++ b/src/amd/compiler/aco_instruction_selection.cpp
@@ -11690,33 +11690,52 @@ merged_wave_info_to_mask(isel_context* ctx, unsigned i)
    return lanecount_to_mask(ctx, count);
 }
 
-static void
-insert_rt_jump_next(isel_context& ctx, const struct ac_shader_args* args)
+void
+insert_return(isel_context& ctx)
 {
-   unsigned src_count = 0;
-   for (unsigned i = 0; i < ctx.args->arg_count; i++)
-      src_count += !!BITSET_TEST(ctx.output_args, i);
-
+   unsigned return_param_count = 0;
+   for (auto& param_def : ctx.callee_info.param_infos) {
+      if (!param_def.second.is_reg || param_def.second.discardable)
+         continue;
+      ++return_param_count;
+   }
+   unsigned src_count = return_param_count + 2;
+   if (ctx.next_pc != Temp())
+      src_count += ctx.args->arg_count;
    Instruction* ret = create_instruction(aco_opcode::p_return, Format::PSEUDO, src_count, 0);
    ctx.block->instructions.emplace_back(ret);
 
-   src_count = 0;
-   for (unsigned i = 0; i < ctx.args->arg_count; i++) {
-      if (!BITSET_TEST(ctx.output_args, i))
-         continue;
-
-      enum ac_arg_regfile file = ctx.args->args[i].file;
-      unsigned size = ctx.args->args[i].size;
-      unsigned reg = ctx.args->args[i].offset + (file == AC_ARG_SGPR ? 0 : 256);
-      RegClass type = RegClass(file == AC_ARG_SGPR ? RegType::sgpr : RegType::vgpr, size);
-      Operand op = ctx.arg_temps[i].id() ? Operand(ctx.arg_temps[i], PhysReg{reg})
-                                         : Operand(PhysReg{reg}, type);
-      ret->operands[src_count] = op;
-      src_count++;
+   if (ctx.next_pc != Temp()) {
+      for (unsigned i = 0; i < ctx.args->arg_count; i++) {
+         enum ac_arg_regfile file = ctx.args->args[i].file;
+         unsigned size = ctx.args->args[i].size;
+         unsigned reg = ctx.args->args[i].offset + (file == AC_ARG_SGPR ? 0 : 256);
+         RegClass type = RegClass(file == AC_ARG_SGPR ? RegType::sgpr : RegType::vgpr, size);
+         Operand op = ctx.arg_temps[i].id() ? Operand(ctx.arg_temps[i], PhysReg{reg})
+                                            : Operand(PhysReg{reg}, type);
+         ret->operands[i] = op;
+      }
    }
 
-   Builder bld(ctx.program, ctx.block);
-   bld.sop1(aco_opcode::s_setpc_b64, get_arg(&ctx, ctx.args->rt.uniform_shader_addr));
+   unsigned def_idx = ctx.next_pc != Temp() ? ctx.args->arg_count : 0;
+   for (auto& param_def : ctx.callee_info.param_infos) {
+      if (!param_def.second.is_reg || param_def.second.discardable)
+         continue;
+      Temp param_temp = param_def.second.def.getTemp();
+      if (param_def.first == 0 && ctx.next_pc != Temp())
+         param_temp = ctx.next_divergent_pc;
+      else if (param_def.first == 1 && ctx.next_pc != Temp())
+         param_temp = ctx.next_pc;
+      Operand op = Operand(param_temp);
+      op.setFixed(param_def.second.def.physReg());
+      ret->operands[def_idx++] = op;
+   }
+   Operand op = Operand(ctx.callee_info.return_address.def.getTemp());
+   op.setFixed(ctx.callee_info.return_address.def.physReg());
+   ret->operands[def_idx++] = op;
+   Operand stack_op = Operand(ctx.callee_info.stack_ptr.def.getTemp());
+   stack_op.setFixed(ctx.callee_info.stack_ptr.def.physReg());
+   ret->operands[def_idx++] = stack_op;
 }
 
 void
@@ -11735,21 +11754,38 @@ select_program_rt(isel_context& ctx, unsigned shader_count, struct nir_shader* c
          init_context(&ctx, nir);
          setup_fp_mode(&ctx, nir);
 
+         ABI abi;
+         /* TODO: callable abi? */
+         switch (shaders[i]->info.stage) {
+         case MESA_SHADER_RAYGEN:
+         case MESA_SHADER_CLOSEST_HIT:
+         case MESA_SHADER_MISS:
+         case MESA_SHADER_CALLABLE: abi = rtRaygenABI; break;
+         case MESA_SHADER_INTERSECTION: abi = rtTraversalABI; break;
+         case MESA_SHADER_ANY_HIT: abi = rtAnyHitABI; break;
+         default: unreachable("invalid RT shader stage");
+         }
+
+         ctx.callee_abi = make_abi(abi, args, ctx.program);
+         ctx.program->callee_abi = ctx.callee_abi;
+         ctx.callee_info = get_callee_info(ctx.callee_abi, impl->function->num_params,
+                                           impl->function->params, ctx.program);
          ctx.program->is_callee = true;
 
-         Instruction* startpgm = add_startpgm(&ctx);
+         Instruction* startpgm = add_startpgm(&ctx, true);
          append_logical_start(ctx.block);
          split_arguments(&ctx, startpgm);
          visit_cf_list(&ctx, &impl->body);
          append_logical_end(ctx.block);
          ctx.block->kind |= block_kind_uniform;
 
-         /* Fix output registers and jump to next shader. We can skip this when dealing with a
-          * raygen shader without shader calls.
-          */
-         if ((shader_count > 1 || shaders[i]->info.stage != MESA_SHADER_RAYGEN) &&
-             impl == nir_shader_get_entrypoint(nir))
-            insert_rt_jump_next(ctx, args);
+         if (ctx.next_pc != Temp()) {
+            insert_return(ctx);
+
+            Builder(ctx.program, ctx.block).sop1(aco_opcode::s_setpc_b64, Operand(ctx.next_pc));
+         } else {
+            Builder(ctx.program, ctx.block).sopp(aco_opcode::s_endpgm);
+         }
 
          cleanup_context(&ctx);
          first_block = false;
@@ -12571,8 +12607,8 @@ select_rt_prolog(Program* program, ac_shader_config* config,
    calc_min_waves(program);
    Builder bld(program, block);
    block->instructions.reserve(32);
-   unsigned num_sgprs = MAX2(in_args->num_sgprs_used, out_args->num_sgprs_used);
-   unsigned num_vgprs = MAX2(in_args->num_vgprs_used, out_args->num_vgprs_used);
+   unsigned num_sgprs = MAX2(in_args->num_sgprs_used, align(out_args->num_sgprs_used, 2) + 4);
+   unsigned num_vgprs = MAX2(in_args->num_vgprs_used, align(out_args->num_vgprs_used, 2) + 4);
 
    /* Inputs:
     * Ring offsets:                s[0-1]
@@ -12623,7 +12659,6 @@ select_rt_prolog(Program* program, ac_shader_config* config,
     * Shader VA:                   v[4-5]
     * Shader Record Ptr:           v[6-7]
     */
-   PhysReg out_uniform_shader_addr = get_arg_reg(out_args, out_args->rt.uniform_shader_addr);
    PhysReg out_launch_size_x = get_arg_reg(out_args, out_args->rt.launch_sizes[0]);
    PhysReg out_launch_size_y = get_arg_reg(out_args, out_args->rt.launch_sizes[1]);
    PhysReg out_launch_size_z = get_arg_reg(out_args, out_args->rt.launch_sizes[2]);
@@ -12631,10 +12666,22 @@ select_rt_prolog(Program* program, ac_shader_config* config,
    for (unsigned i = 0; i < 3; i++)
       out_launch_ids[i] = get_arg_reg(out_args, out_args->rt.launch_ids[i]);
    PhysReg out_stack_ptr = get_arg_reg(out_args, out_args->rt.dynamic_callable_stack_base);
-   PhysReg out_record_ptr = get_arg_reg(out_args, out_args->rt.shader_record);
+   PhysReg out_payload_offset = get_arg_reg(out_args, out_args->rt.payload_offset);
+
+   unsigned arg_reg = align(out_args->num_sgprs_used, 2);
+   PhysReg out_stack_ptr_param = PhysReg{arg_reg};
+   arg_reg += 2;
+   PhysReg out_return_shader_addr = PhysReg{arg_reg};
+   arg_reg += 2;
+   PhysReg out_uniform_shader_addr = out_return_shader_addr.advance(8);
+
+   PhysReg out_divergent_shader_addr = PhysReg{256u + align(out_args->num_vgprs_used, 2)};
+   PhysReg out_record_ptr =
+      out_divergent_shader_addr.advance(8); /* TODO: perhaps a cleaner way using abi info? */
 
    /* Temporaries: */
    num_sgprs = align(num_sgprs, 2);
+   num_sgprs += 2;
    PhysReg tmp_raygen_sbt = PhysReg{num_sgprs};
    num_sgprs += 2;
    PhysReg tmp_ring_offsets = PhysReg{num_sgprs};
@@ -12645,7 +12692,7 @@ select_rt_prolog(Program* program, ac_shader_config* config,
    PhysReg tmp_invocation_idx = PhysReg{256 + num_vgprs++};
 
    /* Confirm some assumptions about register aliasing */
-   assert(in_ring_offsets == out_uniform_shader_addr);
+   // assert(in_ring_offsets == out_uniform_shader_addr);
    assert(get_arg_reg(in_args, in_args->push_constants) ==
           get_arg_reg(out_args, out_args->push_constants));
    assert(get_arg_reg(in_args, in_args->rt.sbt_descriptors) ==
@@ -12758,6 +12805,13 @@ select_rt_prolog(Program* program, ac_shader_config* config,
                Operand(out_launch_ids[1], v1), Operand(vcc, bld.lm));
    }
 
+   bld.vop3(aco_opcode::v_lshrrev_b64, Definition(out_divergent_shader_addr, v2), Operand::c32(0),
+            Operand(out_uniform_shader_addr, s2));
+   bld.sop1(aco_opcode::s_mov_b64, Definition(out_return_shader_addr, s2), Operand::c32(0));
+   bld.vop1(aco_opcode::v_mov_b32, Definition(out_payload_offset, v1), Operand::c32(0));
+
+   bld.sopk(aco_opcode::s_movk_i32, Definition(out_stack_ptr_param, s1), 0);
+
    /* jump to raygen */
    bld.sop1(aco_opcode::s_setpc_b64, Operand(out_uniform_shader_addr, s2));
 
-- 
GitLab


From 720ffa29f0b0cd9c281fdb8334691549d6092456 Mon Sep 17 00:00:00 2001
From: Friedrich Vock <friedrich.vock@gmx.de>
Date: Sat, 4 May 2024 17:54:14 +0200
Subject: [PATCH 43/71] radv,aco: Pass raygen parameter info to prolog

---
 src/amd/compiler/aco_instruction_selection.cpp | 12 ++++++++++--
 src/amd/compiler/aco_interface.cpp             |  7 ++++---
 src/amd/compiler/aco_interface.h               |  4 ++--
 src/amd/compiler/aco_ir.h                      |  4 +++-
 src/amd/vulkan/radv_pipeline_rt.c              |  5 ++++-
 src/amd/vulkan/radv_shader.c                   |  6 +++---
 src/amd/vulkan/radv_shader.h                   |  3 ++-
 7 files changed, 28 insertions(+), 13 deletions(-)

diff --git a/src/amd/compiler/aco_instruction_selection.cpp b/src/amd/compiler/aco_instruction_selection.cpp
index a00dfe76d4948..97ee7fe317f0f 100644
--- a/src/amd/compiler/aco_instruction_selection.cpp
+++ b/src/amd/compiler/aco_instruction_selection.cpp
@@ -12596,7 +12596,8 @@ calc_nontrivial_instance_id(Builder& bld, const struct ac_shader_args* args,
 void
 select_rt_prolog(Program* program, ac_shader_config* config,
                  const struct aco_compiler_options* options, const struct aco_shader_info* info,
-                 const struct ac_shader_args* in_args, const struct ac_shader_args* out_args)
+                 const struct ac_shader_args* in_args, const struct ac_shader_args* out_args,
+                 unsigned raygen_param_count, nir_parameter* raygen_params)
 {
    init_program(program, compute_cs, info, options->gfx_level, options->family, options->wgp_mode,
                 config);
@@ -12610,6 +12611,9 @@ select_rt_prolog(Program* program, ac_shader_config* config,
    unsigned num_sgprs = MAX2(in_args->num_sgprs_used, align(out_args->num_sgprs_used, 2) + 4);
    unsigned num_vgprs = MAX2(in_args->num_vgprs_used, align(out_args->num_vgprs_used, 2) + 4);
 
+   struct callee_info raygen_info = get_callee_info(make_abi(rtRaygenABI, out_args, program),
+                                                    raygen_param_count, raygen_params, NULL);
+
    /* Inputs:
     * Ring offsets:                s[0-1]
     * Indirect descriptor sets:    s[2]
@@ -12810,7 +12814,11 @@ select_rt_prolog(Program* program, ac_shader_config* config,
    bld.sop1(aco_opcode::s_mov_b64, Definition(out_return_shader_addr, s2), Operand::c32(0));
    bld.vop1(aco_opcode::v_mov_b32, Definition(out_payload_offset, v1), Operand::c32(0));
 
-   bld.sopk(aco_opcode::s_movk_i32, Definition(out_stack_ptr_param, s1), 0);
+   unsigned param_size = raygen_info.scratch_param_size;
+   if (program->gfx_level < GFX9)
+      param_size *= program->wave_size;
+
+   bld.sopk(aco_opcode::s_movk_i32, Definition(out_stack_ptr_param, s1), param_size);
 
    /* jump to raygen */
    bld.sop1(aco_opcode::s_setpc_b64, Operand(out_uniform_shader_addr, s2));
diff --git a/src/amd/compiler/aco_interface.cpp b/src/amd/compiler/aco_interface.cpp
index 396908155d1e8..14d71cbb521dc 100644
--- a/src/amd/compiler/aco_interface.cpp
+++ b/src/amd/compiler/aco_interface.cpp
@@ -303,8 +303,8 @@ aco_compile_shader(const struct aco_compiler_options* options, const struct aco_
 void
 aco_compile_rt_prolog(const struct aco_compiler_options* options,
                       const struct aco_shader_info* info, const struct ac_shader_args* in_args,
-                      const struct ac_shader_args* out_args, aco_callback* build_prolog,
-                      void** binary)
+                      const struct ac_shader_args* out_args, unsigned raygen_param_count,
+                      nir_parameter* raygen_params, aco_callback* build_prolog, void** binary)
 {
    init();
 
@@ -315,7 +315,8 @@ aco_compile_rt_prolog(const struct aco_compiler_options* options,
    program->debug.func = NULL;
    program->debug.private_data = NULL;
 
-   select_rt_prolog(program.get(), &config, options, info, in_args, out_args);
+   select_rt_prolog(program.get(), &config, options, info, in_args, out_args, raygen_param_count,
+                    raygen_params);
    validate(program.get());
    insert_wait_states(program.get());
    insert_NOPs(program.get());
diff --git a/src/amd/compiler/aco_interface.h b/src/amd/compiler/aco_interface.h
index 462727432a1ac..a34e0eb3ada7d 100644
--- a/src/amd/compiler/aco_interface.h
+++ b/src/amd/compiler/aco_interface.h
@@ -49,8 +49,8 @@ void aco_compile_shader(const struct aco_compiler_options* options,
 
 void aco_compile_rt_prolog(const struct aco_compiler_options* options,
                            const struct aco_shader_info* info, const struct ac_shader_args* in_args,
-                           const struct ac_shader_args* out_args, aco_callback* build_prolog,
-                           void** binary);
+                           const struct ac_shader_args* out_args, unsigned raygen_param_count,
+                           nir_parameter* raygen_params, aco_callback* build_prolog, void** binary);
 
 void aco_compile_vs_prolog(const struct aco_compiler_options* options,
                            const struct aco_shader_info* info,
diff --git a/src/amd/compiler/aco_ir.h b/src/amd/compiler/aco_ir.h
index 13aec1fdf9858..842de447b129f 100644
--- a/src/amd/compiler/aco_ir.h
+++ b/src/amd/compiler/aco_ir.h
@@ -22,6 +22,7 @@
 #include <vector>
 
 typedef struct nir_shader nir_shader;
+typedef struct nir_parameter nir_parameter;
 
 namespace aco {
 
@@ -2413,7 +2414,8 @@ void select_trap_handler_shader(Program* program, struct nir_shader* shader,
 void select_rt_prolog(Program* program, ac_shader_config* config,
                       const struct aco_compiler_options* options,
                       const struct aco_shader_info* info, const struct ac_shader_args* in_args,
-                      const struct ac_shader_args* out_args);
+                      const struct ac_shader_args* out_args, unsigned raygen_param_count,
+                      nir_parameter* raygen_params);
 void select_vs_prolog(Program* program, const struct aco_vs_prolog_info* pinfo,
                       ac_shader_config* config, const struct aco_compiler_options* options,
                       const struct aco_shader_info* info, const struct ac_shader_args* args);
diff --git a/src/amd/vulkan/radv_pipeline_rt.c b/src/amd/vulkan/radv_pipeline_rt.c
index 13e5259cbd8a1..dd6fc47d74de3 100644
--- a/src/amd/vulkan/radv_pipeline_rt.c
+++ b/src/amd/vulkan/radv_pipeline_rt.c
@@ -783,8 +783,11 @@ static void
 compile_rt_prolog(struct radv_device *device, struct radv_ray_tracing_pipeline *pipeline)
 {
    const struct radv_physical_device *pdev = radv_device_physical(device);
+   struct nir_function raygen_stub = {};
 
-   pipeline->prolog = radv_create_rt_prolog(device);
+   /* Create a dummy function signature for raygen shaders in order to pass parameter info to the prolog */
+   radv_nir_init_rt_function_params(&raygen_stub, MESA_SHADER_RAYGEN, 0);
+   pipeline->prolog = radv_create_rt_prolog(device, raygen_stub.num_params, raygen_stub.params);
 
    /* create combined config */
    struct ac_shader_config *config = &pipeline->prolog->config;
diff --git a/src/amd/vulkan/radv_shader.c b/src/amd/vulkan/radv_shader.c
index 35d72b81da65a..f160ccf2d939a 100644
--- a/src/amd/vulkan/radv_shader.c
+++ b/src/amd/vulkan/radv_shader.c
@@ -2965,7 +2965,7 @@ radv_aco_build_shader_part(void **bin, uint32_t num_sgprs, uint32_t num_vgprs, c
 }
 
 struct radv_shader *
-radv_create_rt_prolog(struct radv_device *device)
+radv_create_rt_prolog(struct radv_device *device, unsigned raygen_param_count, nir_parameter *raygen_params)
 {
    const struct radv_physical_device *pdev = radv_device_physical(device);
    const struct radv_instance *instance = radv_physical_device_instance(pdev);
@@ -3006,8 +3006,8 @@ radv_create_rt_prolog(struct radv_device *device)
    struct aco_compiler_options ac_opts;
    radv_aco_convert_shader_info(&ac_info, &info, &in_args, &device->cache_key, options.info->gfx_level);
    radv_aco_convert_opts(&ac_opts, &options, &in_args, &stage_key);
-   aco_compile_rt_prolog(&ac_opts, &ac_info, &in_args.ac, &out_args.ac, &radv_aco_build_shader_binary,
-                         (void **)&binary);
+   aco_compile_rt_prolog(&ac_opts, &ac_info, &in_args.ac, &out_args.ac, raygen_param_count, raygen_params,
+                         &radv_aco_build_shader_binary, (void **)&binary);
    binary->info = info;
 
    radv_postprocess_binary_config(device, binary, &in_args);
diff --git a/src/amd/vulkan/radv_shader.h b/src/amd/vulkan/radv_shader.h
index 1db253e0764a3..b405fc7ab5c57 100644
--- a/src/amd/vulkan/radv_shader.h
+++ b/src/amd/vulkan/radv_shader.h
@@ -563,7 +563,8 @@ void radv_free_shader_memory(struct radv_device *device, union radv_shader_arena
 
 struct radv_shader *radv_create_trap_handler_shader(struct radv_device *device);
 
-struct radv_shader *radv_create_rt_prolog(struct radv_device *device);
+struct radv_shader *radv_create_rt_prolog(struct radv_device *device, unsigned raygen_param_count,
+                                          nir_parameter *raygen_params);
 
 struct radv_shader_part *radv_shader_part_create(struct radv_device *device, struct radv_shader_part_binary *binary,
                                                  unsigned wave_size);
-- 
GitLab


From 175ccbfa38fcd9cfce8d19c8013ef966ef1c298b Mon Sep 17 00:00:00 2001
From: Friedrich Vock <friedrich.vock@gmx.de>
Date: Mon, 13 May 2024 06:17:34 +0200
Subject: [PATCH 44/71] aco/ssa_elimination: Don't remove exec writes for last
 blocks of callee shaders

The caller is going to use the exec mask written there.
---
 src/amd/compiler/aco_ssa_elimination.cpp | 3 ++-
 1 file changed, 2 insertions(+), 1 deletion(-)

diff --git a/src/amd/compiler/aco_ssa_elimination.cpp b/src/amd/compiler/aco_ssa_elimination.cpp
index 081c965b642da..e7c4272fc58d0 100644
--- a/src/amd/compiler/aco_ssa_elimination.cpp
+++ b/src/amd/compiler/aco_ssa_elimination.cpp
@@ -538,7 +538,8 @@ eliminate_useless_exec_writes_in_block(ssa_elimination_ctx& ctx, Block& block)
    /* Check if any successor needs the outgoing exec mask from the current block. */
 
    bool exec_write_used;
-   if (block.kind & block_kind_end_with_regs) {
+   if (block.kind & block_kind_end_with_regs ||
+       (block.linear_succs.empty() && ctx.program->is_callee)) {
       /* Last block of a program with succeed shader part should respect final exec write. */
       exec_write_used = true;
    } else {
-- 
GitLab


From a564eec2d9744cd6fa527643a19e2a1eca82dcca Mon Sep 17 00:00:00 2001
From: Friedrich Vock <friedrich.vock@gmx.de>
Date: Thu, 6 Jun 2024 20:34:51 +0200
Subject: [PATCH 45/71] aco: Pass through block/live_out to get_temp_registers

We'll need this for estimating call temp registers.
---
 src/amd/compiler/aco_ir.h                  |  8 +++++---
 src/amd/compiler/aco_live_var_analysis.cpp | 13 +++++++------
 src/amd/compiler/aco_scheduler.cpp         | 12 ++++++++----
 src/amd/compiler/aco_spill.cpp             |  7 +++++--
 4 files changed, 25 insertions(+), 15 deletions(-)

diff --git a/src/amd/compiler/aco_ir.h b/src/amd/compiler/aco_ir.h
index 842de447b129f..5475fbbacb340 100644
--- a/src/amd/compiler/aco_ir.h
+++ b/src/amd/compiler/aco_ir.h
@@ -2502,9 +2502,11 @@ int get_op_fixed_to_def(Instruction* instr);
 
 /* utilities for dealing with register demand */
 RegisterDemand get_live_changes(aco_ptr<Instruction>& instr);
-RegisterDemand get_temp_registers(aco_ptr<Instruction>& instr);
-RegisterDemand get_demand_before(RegisterDemand demand, aco_ptr<Instruction>& instr,
-                                 aco_ptr<Instruction>& instr_before);
+RegisterDemand get_temp_registers(Program *program, Block *block, aco_ptr<Instruction>& instr,
+                                  const IDSet& live_out);
+RegisterDemand get_demand_before(Program *program, Block *block, RegisterDemand demand,
+                                 aco_ptr<Instruction>& instr, aco_ptr<Instruction>& instr_before,
+                                 const IDSet& live_out);
 
 /* number of sgprs that need to be allocated but might notbe addressable as s0-s105 */
 uint16_t get_extra_sgprs(Program* program);
diff --git a/src/amd/compiler/aco_live_var_analysis.cpp b/src/amd/compiler/aco_live_var_analysis.cpp
index 4d96a40c0d67f..7c743cf88651d 100644
--- a/src/amd/compiler/aco_live_var_analysis.cpp
+++ b/src/amd/compiler/aco_live_var_analysis.cpp
@@ -33,7 +33,7 @@ get_live_changes(aco_ptr<Instruction>& instr)
 }
 
 RegisterDemand
-get_temp_registers(aco_ptr<Instruction>& instr)
+get_temp_registers(Program *program, Block *block, aco_ptr<Instruction>& instr, const IDSet& live_out)
 {
    RegisterDemand temp_registers;
 
@@ -79,13 +79,13 @@ get_temp_registers(aco_ptr<Instruction>& instr)
 }
 
 RegisterDemand
-get_demand_before(RegisterDemand demand, aco_ptr<Instruction>& instr,
-                  aco_ptr<Instruction>& instr_before)
+get_demand_before(Program *program, Block *block, RegisterDemand demand, aco_ptr<Instruction>& instr,
+                  aco_ptr<Instruction>& instr_before, const IDSet& live_out)
 {
    demand -= get_live_changes(instr);
-   demand -= get_temp_registers(instr);
+   demand -= get_temp_registers(program, block, instr, live_out);
    if (instr_before)
-      demand += get_temp_registers(instr_before);
+      demand += get_temp_registers(program, block, instr_before, live_out);
    return demand;
 }
 
@@ -199,7 +199,8 @@ process_live_temps_per_block(Program* program, live& lives, Block* block, unsign
          }
       }
 
-      register_demand[idx] += get_temp_registers(block->instructions[idx]);
+      register_demand[idx] +=
+         get_temp_registers(program, block, block->instructions[idx], lives.live_out[block->index]);
    }
 
    /* handle phi definitions */
diff --git a/src/amd/compiler/aco_scheduler.cpp b/src/amd/compiler/aco_scheduler.cpp
index f5b33be9c79de..dece563906e84 100644
--- a/src/amd/compiler/aco_scheduler.cpp
+++ b/src/amd/compiler/aco_scheduler.cpp
@@ -82,9 +82,11 @@ struct UpwardsCursor {
 struct MoveState {
    RegisterDemand max_registers;
 
+   Program* program;
    Block* block;
    Instruction* current;
    RegisterDemand* register_demand; /* demand per instruction */
+   const IDSet* live_out;
    bool improved_rar;
 
    std::vector<bool> depends_on;
@@ -232,8 +234,8 @@ MoveState::downwards_move(DownwardsCursor& cursor, bool add_to_clause)
       return move_fail_pressure;
 
    /* New demand for the moved instruction */
-   const RegisterDemand temp = get_temp_registers(instr);
-   const RegisterDemand temp2 = get_temp_registers(block->instructions[dest_insert_idx - 1]);
+   const RegisterDemand temp = get_temp_registers(program, block, instr, *live_out);
+   const RegisterDemand temp2 = get_temp_registers(program, block, block->instructions[dest_insert_idx - 1], *live_out);
    const RegisterDemand new_demand = register_demand[dest_insert_idx - 1] - temp2 + temp;
    if (new_demand.exceeds(max_registers))
       return move_fail_pressure;
@@ -356,10 +358,10 @@ MoveState::upwards_move(UpwardsCursor& cursor)
    /* check if register pressure is low enough: the diff is negative if register pressure is
     * decreased */
    const RegisterDemand candidate_diff = get_live_changes(instr);
-   const RegisterDemand temp = get_temp_registers(instr);
+   const RegisterDemand temp = get_temp_registers(program, block, instr, *live_out);
    if (RegisterDemand(cursor.total_demand + candidate_diff).exceeds(max_registers))
       return move_fail_pressure;
-   const RegisterDemand temp2 = get_temp_registers(block->instructions[cursor.insert_idx - 1]);
+   const RegisterDemand temp2 = get_temp_registers(program, block, block->instructions[cursor.insert_idx - 1], *live_out);
    const RegisterDemand new_demand =
       register_demand[cursor.insert_idx - 1] - temp2 + candidate_diff + temp;
    if (new_demand.exceeds(max_registers))
@@ -1176,6 +1178,7 @@ schedule_block(sched_ctx& ctx, Program* program, Block* block, live& live_vars)
    ctx.last_SMEM_stall = INT16_MIN;
    ctx.mv.block = block;
    ctx.mv.register_demand = live_vars.register_demand[block->index].data();
+   ctx.mv.live_out = &live_vars.live_out[block->index];
 
    /* go through all instructions and find memory loads */
    unsigned num_stores = 0;
@@ -1246,6 +1249,7 @@ schedule_program(Program* program, live& live_vars)
 
    sched_ctx ctx;
    ctx.gfx_level = program->gfx_level;
+   ctx.mv.program = program;
    ctx.mv.depends_on.resize(program->peekAllocationId());
    ctx.mv.RAR_dependencies.resize(program->peekAllocationId());
    ctx.mv.RAR_dependencies_clause.resize(program->peekAllocationId());
diff --git a/src/amd/compiler/aco_spill.cpp b/src/amd/compiler/aco_spill.cpp
index f38c2b497fcc6..a728de9474338 100644
--- a/src/amd/compiler/aco_spill.cpp
+++ b/src/amd/compiler/aco_spill.cpp
@@ -304,7 +304,8 @@ get_demand_before(spill_ctx& ctx, unsigned block_idx, unsigned idx)
       RegisterDemand demand = ctx.live_vars.register_demand[block_idx][idx];
       aco_ptr<Instruction>& instr = ctx.program->blocks[block_idx].instructions[idx];
       aco_ptr<Instruction> instr_before(nullptr);
-      return get_demand_before(demand, instr, instr_before);
+      return get_demand_before(ctx.program, &ctx.program->blocks[block_idx], demand, instr,
+                               instr_before, ctx.live_vars.live_out[block_idx]);
    } else {
       return ctx.live_vars.register_demand[block_idx][idx - 1];
    }
@@ -1085,7 +1086,9 @@ process_block(spill_ctx& ctx, unsigned block_idx, Block* block, RegisterDemand s
 
                if (recalculate_demand)
                   ctx.live_vars.register_demand[block_idx][idx] =
-                     demand_before + get_live_changes(instr) + get_temp_registers(instr);
+                     demand_before + get_live_changes(instr) +
+                     get_temp_registers(ctx.program, block, instr,
+                                        ctx.live_vars.live_out[block_idx]);
             }
 
             if (avoid_respill) {
-- 
GitLab


From 46b1e8fbd32a749372f1a6fe1409f846a7915cde Mon Sep 17 00:00:00 2001
From: Friedrich Vock <friedrich.vock@gmx.de>
Date: Wed, 6 Mar 2024 14:53:39 +0100
Subject: [PATCH 46/71] aco/isel: Handle calls

---
 .../compiler/aco_instruction_selection.cpp    | 179 ++++++++++++++++++
 1 file changed, 179 insertions(+)

diff --git a/src/amd/compiler/aco_instruction_selection.cpp b/src/amd/compiler/aco_instruction_selection.cpp
index 97ee7fe317f0f..d60b74448aa07 100644
--- a/src/amd/compiler/aco_instruction_selection.cpp
+++ b/src/amd/compiler/aco_instruction_selection.cpp
@@ -10422,6 +10422,184 @@ make_abi(const ABI& base, const ac_shader_args* args, Program* program)
    return abi;
 }
 
+void
+visit_call(isel_context* ctx, nir_call_instr* instr)
+{
+   Builder bld(ctx->program, ctx->block);
+
+   ABI abi;
+   /* TODO: callable abi? */
+   switch (instr->abi) {
+   case nir_call_abi_rt_raygen_amd: abi = make_abi(rtRaygenABI, ctx->args, ctx->program); break;
+   case nir_call_abi_rt_traversal_amd:
+      abi = make_abi(rtTraversalABI, ctx->args, ctx->program);
+      break;
+   case nir_call_abi_rt_any_hit_amd: abi = make_abi(rtAnyHitABI, ctx->args, ctx->program); break;
+   default: unreachable("invalid abi");
+   }
+
+   int sgpr_reg_byte_offset = 16;
+   int vgpr_reg_byte_offset = 0;
+   int scratch_byte_offset = 0;
+
+   unsigned reg_param_count = 0;
+   unsigned reg_return_param_count = 0;
+   unsigned scratch_param_count = 0;
+   unsigned scratch_return_param_count = 0;
+   std::unordered_map<unsigned, PhysReg> param_regs;
+   std::unordered_map<unsigned, unsigned> param_scratch_offsets;
+
+   std::vector<parameter_info> return_infos;
+
+   for (unsigned i = 0; i < instr->callee->num_params; ++i) {
+      int* reg_byte_offset;
+      PhysRegInterval interval;
+      if (instr->callee->params[i].is_divergent) {
+         reg_byte_offset = &vgpr_reg_byte_offset;
+         interval = abi.parameterSpace.vgpr;
+      } else {
+         reg_byte_offset = &sgpr_reg_byte_offset;
+         interval = abi.parameterSpace.sgpr;
+      }
+
+      PhysReg param_reg = interval.lo().advance(*reg_byte_offset);
+      int* byte_offset;
+      unsigned byte_size =
+         align(instr->callee->params[i].bit_size, 32) / 8 * instr->callee->params[i].num_components;
+
+      if (param_reg < interval.hi()) {
+         ++reg_param_count;
+         if (instr->callee->params[i].is_return) {
+            return_infos.emplace_back(parameter_info{
+               .is_reg = true,
+               .def = Definition(),
+            });
+            ++reg_return_param_count;
+         }
+         byte_offset = reg_byte_offset;
+         param_regs.emplace(i, param_reg);
+      } else {
+         ++scratch_param_count;
+         if (instr->callee->params[i].is_return) {
+            return_infos.emplace_back(parameter_info{
+               .is_reg = false,
+               .scratch_offset = static_cast<unsigned>(scratch_byte_offset),
+            });
+            ++scratch_return_param_count;
+         }
+         byte_offset = &scratch_byte_offset;
+         param_scratch_offsets.emplace(i, scratch_byte_offset);
+      }
+      *byte_offset += byte_size;
+   }
+
+   Instruction* stack_instr;
+   Definition stack_ptr;
+   if (ctx->callee_info.stack_ptr.is_reg) {
+      stack_instr =
+         bld.pseudo(aco_opcode::p_callee_stack_ptr, bld.def(s1), Operand::c32(scratch_byte_offset),
+                    Operand(ctx->callee_info.stack_ptr.def.getTemp()));
+      stack_ptr = ctx->callee_info.stack_ptr.def;
+   } else {
+      stack_instr =
+         bld.pseudo(aco_opcode::p_callee_stack_ptr, bld.def(s1), Operand::c32(scratch_byte_offset));
+      stack_ptr = bld.pseudo(aco_opcode::p_parallelcopy, bld.def(s1), Operand::c32(0)).def(0);
+   }
+
+   for (auto& pair : param_scratch_offsets) {
+      parameter_info info = {
+         .is_reg = false,
+         .scratch_offset = pair.second,
+      };
+      store_scratch_param(ctx, bld, info, stack_instr->definitions[0].getTemp(),
+                          scratch_byte_offset, get_ssa_temp(ctx, instr->params[pair.first].ssa));
+   }
+
+   unsigned extra_def_count = 1;
+
+   Temp vcc_backup;
+   if (ctx->program->dev.sgpr_limit <= vcc_hi.reg()) {
+      vcc_backup = bld.copy(bld.def(bld.lm), Operand(vcc, bld.lm));
+      --extra_def_count;
+   }
+
+   unsigned extra_param_count = 3;
+   if (ctx->program->gfx_level < GFX9)
+      ++extra_param_count;
+
+   unsigned param_size = scratch_byte_offset;
+   if (ctx->program->gfx_level < GFX9)
+      param_size *= ctx->program->wave_size;
+
+   Instruction* call_instr =
+      create_instruction(aco_opcode::p_call, Format::PSEUDO_CALL,
+                         reg_param_count + ctx->args->arg_count + extra_param_count,
+                         reg_return_param_count + extra_def_count);
+   call_instr->call().abi = abi;
+   call_instr->operands[0] = Operand(ctx->callee_info.return_address.def.getTemp(),
+                                     ctx->callee_info.return_address.def.physReg());
+   call_instr->operands[1] = Operand(stack_ptr.getTemp(), ctx->callee_info.stack_ptr.def.physReg());
+   call_instr->operands[2] = Operand::c32(param_size);
+   if (ctx->program->gfx_level < GFX9) {
+      call_instr->operands[reg_param_count + ctx->args->arg_count + 3] =
+         Operand(load_scratch_resource(ctx->program, bld, true, false));
+      call_instr->operands[reg_param_count + ctx->args->arg_count + 3].setLateKill(true);
+   }
+
+   unsigned reg_return_param_idx = 0;
+   unsigned return_param_idx = 0;
+   for (unsigned i = 0; i < instr->num_params; ++i) {
+      auto param_reg = param_regs.find(i);
+      if (param_reg == param_regs.end()) {
+         if (instr->callee->params[i].is_return)
+            ++return_param_idx;
+         continue;
+      }
+
+      if (instr->callee->params[i].is_divergent)
+         call_instr->operands[i + 3] =
+            Operand(as_vgpr(ctx, get_ssa_temp(ctx, instr->params[i].ssa)));
+      else
+         call_instr->operands[i + 3] = Operand(get_ssa_temp(ctx, instr->params[i].ssa));
+
+      if (instr->callee->params[i].is_return) {
+         assert(instr->callee->params[i].is_divergent);
+         Definition def =
+            bld.def(RegClass(RegType::vgpr, DIV_ROUND_UP(instr->callee->params[i].bit_size, 32)),
+                    param_reg->second);
+         call_instr->definitions[extra_def_count + reg_return_param_idx++] = def;
+         return_infos[return_param_idx++].def = def;
+      }
+
+      call_instr->operands[i + 3].setFixed(param_reg->second);
+   }
+
+   for (unsigned i = 0; i < ctx->args->arg_count; i++) {
+      enum ac_arg_regfile file = ctx->args->args[i].file;
+      unsigned size = ctx->args->args[i].size;
+      unsigned reg = ctx->args->args[i].offset + (file == AC_ARG_SGPR ? 0 : 256);
+      RegClass type = RegClass(file == AC_ARG_SGPR ? RegType::sgpr : RegType::vgpr, size);
+      Operand op = ctx->arg_temps[i].id() ? Operand(ctx->arg_temps[i], PhysReg{reg})
+                                          : Operand(PhysReg{reg}, type);
+      op.setLateKill(true);
+      call_instr->operands[reg_param_count + 3 + i] = op;
+   }
+
+   if (ctx->program->dev.sgpr_limit <= vcc_hi.reg())
+      bld.copy(bld.def(bld.lm, vcc), Operand(vcc_backup));
+   else
+      call_instr->definitions[0] = bld.def(s2, vcc);
+
+   ctx->block->instructions.emplace_back(static_cast<Instruction*>(call_instr));
+
+   call_info info;
+   info.nir_instr = instr;
+   info.aco_instr = call_instr;
+   info.return_info = std::move(return_infos);
+   info.scratch_param_size = scratch_byte_offset;
+   ctx->call_infos.push_back(info);
+}
+
 void
 visit_block(isel_context* ctx, nir_block* block)
 {
@@ -10447,6 +10625,7 @@ visit_block(isel_context* ctx, nir_block* block)
       case nir_instr_type_undef: visit_undef(ctx, nir_instr_as_undef(instr)); break;
       case nir_instr_type_deref: break;
       case nir_instr_type_jump: visit_jump(ctx, nir_instr_as_jump(instr)); break;
+      case nir_instr_type_call: visit_call(ctx, nir_instr_as_call(instr)); break;
       default: isel_err(instr, "Unknown NIR instr type");
       }
    }
-- 
GitLab


From c7e34a56d5942684df988d8c5e137a54d5efa820 Mon Sep 17 00:00:00 2001
From: Friedrich Vock <friedrich.vock@gmx.de>
Date: Wed, 6 Mar 2024 14:56:16 +0100
Subject: [PATCH 47/71] aco/lower_to_hw_instr: Lower calls

---
 src/amd/compiler/aco_lower_to_hw_instr.cpp | 10 ++++++++++
 1 file changed, 10 insertions(+)

diff --git a/src/amd/compiler/aco_lower_to_hw_instr.cpp b/src/amd/compiler/aco_lower_to_hw_instr.cpp
index 968c2ae80bacf..0bb904ba71c69 100644
--- a/src/amd/compiler/aco_lower_to_hw_instr.cpp
+++ b/src/amd/compiler/aco_lower_to_hw_instr.cpp
@@ -2969,6 +2969,16 @@ lower_to_hw_instr(Program* program)
          } else if (instr->isMIMG() && instr->mimg().strict_wqm) {
             lower_image_sample(&ctx, instr);
             ctx.instructions.emplace_back(std::move(instr));
+         } else if (instr->isCall()) {
+            PhysReg stack_reg = instr->operands[1].physReg();
+            if (instr->operands[2].constantValue())
+               bld.sop2(aco_opcode::s_add_u32, Definition(stack_reg, s1), Definition(scc, s1),
+                        Operand(stack_reg, s1), instr->operands[2]);
+            bld.sop1(aco_opcode::s_swappc_b64, Definition(instr->operands[0].physReg(), s2),
+                     Operand(instr->operands[4].physReg(), s2));
+            if (instr->operands[2].constantValue())
+               bld.sop2(aco_opcode::s_sub_u32, Definition(stack_reg, s1), Definition(scc, s1),
+                        Operand(stack_reg, s1), instr->operands[2]);
          } else {
             ctx.instructions.emplace_back(std::move(instr));
          }
-- 
GitLab


From 219339bde2c66aa590cebd5e015649a9b9794f72 Mon Sep 17 00:00:00 2001
From: Friedrich Vock <friedrich.vock@gmx.de>
Date: Wed, 13 Mar 2024 10:59:52 +0100
Subject: [PATCH 48/71] aco/live_var_analysis: Handle calls

---
 src/amd/compiler/aco_live_var_analysis.cpp | 54 ++++++++++++++++++++++
 1 file changed, 54 insertions(+)

diff --git a/src/amd/compiler/aco_live_var_analysis.cpp b/src/amd/compiler/aco_live_var_analysis.cpp
index 7c743cf88651d..fa25cf8a35e67 100644
--- a/src/amd/compiler/aco_live_var_analysis.cpp
+++ b/src/amd/compiler/aco_live_var_analysis.cpp
@@ -32,6 +32,57 @@ get_live_changes(aco_ptr<Instruction>& instr)
    return changes;
 }
 
+RegisterDemand
+get_blocked_abi_demand(Program* program, Block *block, const Pseudo_call_instruction* instr,
+                       const IDSet& live_out)
+{
+   const unsigned max_vgpr = get_addr_vgpr_from_waves(program, program->min_waves);
+   /* Linear VGPRs can intersect with preserved VGPRs, we insert spill code for them in
+    * spill_preserved.
+    */
+   unsigned linear_vgpr_demand = 0;
+   for (auto temp : live_out)
+      if (program->temp_rc[temp].is_linear_vgpr())
+         linear_vgpr_demand += program->temp_rc[temp].size();
+   for (auto it = block->instructions.rbegin(); it != block->instructions.rend(); ++it) {
+      if (it->get() == instr)
+         break;
+      if ((*it)->opcode == aco_opcode::p_start_linear_vgpr)
+         for (auto& def : (*it)->definitions)
+            linear_vgpr_demand += def.size();
+      else
+         for (auto& op : (*it)->operands)
+            if (op.regClass().is_linear_vgpr() && op.isKill())
+               linear_vgpr_demand += op.size();
+   }
+
+   unsigned preserved_vgprs = max_vgpr - (instr->abi.clobberedRegs.vgpr.hi() - 256);
+   linear_vgpr_demand -= std::min(preserved_vgprs, linear_vgpr_demand);
+
+   unsigned preserved_vgpr_demand =
+      instr->abi.clobberedRegs.vgpr.size -
+      std::min(linear_vgpr_demand, instr->abi.clobberedRegs.vgpr.size);
+   unsigned preserved_sgpr_demand = instr->abi.clobberedRegs.sgpr.size;
+
+   /* Don't count definitions contained in clobbered call regs twice */
+   for (auto& definition : instr->definitions) {
+      if (definition.isTemp() && definition.isFixed()) {
+         auto def_regs = PhysRegInterval{PhysReg{definition.physReg().reg()}, definition.size()};
+         for (auto reg : def_regs) {
+            if (instr->abi.clobberedRegs.sgpr.contains(reg))
+               --preserved_sgpr_demand;
+            if (instr->abi.clobberedRegs.vgpr.contains(reg))
+               --preserved_vgpr_demand;
+         }
+      }
+   }
+   if (instr->abi.clobberedRegs.sgpr.contains(instr->operands[1].physReg()) &&
+       !instr->operands[1].isKill())
+      --preserved_sgpr_demand;
+
+   return RegisterDemand(preserved_vgpr_demand, preserved_sgpr_demand);
+}
+
 RegisterDemand
 get_temp_registers(Program *program, Block *block, aco_ptr<Instruction>& instr, const IDSet& live_out)
 {
@@ -74,6 +125,9 @@ get_temp_registers(Program *program, Block *block, aco_ptr<Instruction>& instr,
    if (op_idx != -1 && !instr->operands[op_idx].isKill())
       demand_between += instr->definitions[0].getTemp();
 
+   if (instr->isCall())
+      temp_registers += get_blocked_abi_demand(program, block, &instr->call(), live_out);
+
    temp_registers.update(demand_between - get_live_changes(instr));
    return temp_registers;
 }
-- 
GitLab


From 393d6d499d702f8200b387a163f2d06765052ad7 Mon Sep 17 00:00:00 2001
From: Friedrich Vock <friedrich.vock@gmx.de>
Date: Sat, 23 Mar 2024 10:29:13 +0100
Subject: [PATCH 49/71] aco/ra: add utility to block interval

---
 src/amd/compiler/aco_register_allocation.cpp | 2 ++
 1 file changed, 2 insertions(+)

diff --git a/src/amd/compiler/aco_register_allocation.cpp b/src/amd/compiler/aco_register_allocation.cpp
index ee707f70496e0..562e8251301dc 100644
--- a/src/amd/compiler/aco_register_allocation.cpp
+++ b/src/amd/compiler/aco_register_allocation.cpp
@@ -243,6 +243,8 @@ public:
          fill(start, rc.size(), 0xFFFFFFFF);
    }
 
+   void block(PhysRegInterval interval) { fill(interval.lo(), interval.size, 0xFFFFFFFF); }
+
    bool is_blocked(PhysReg start) const
    {
       if (regs[start] == 0xFFFFFFFF)
-- 
GitLab


From 51f01b62d09a57a4b5c802ad972cb7318e52c734 Mon Sep 17 00:00:00 2001
From: Friedrich Vock <friedrich.vock@gmx.de>
Date: Sat, 23 Mar 2024 10:31:35 +0100
Subject: [PATCH 50/71] aco/ra: handle clobbered regions by calls

---
 src/amd/compiler/aco_register_allocation.cpp | 51 ++++++++++++++++++++
 1 file changed, 51 insertions(+)

diff --git a/src/amd/compiler/aco_register_allocation.cpp b/src/amd/compiler/aco_register_allocation.cpp
index 562e8251301dc..a94fe73e1ad3f 100644
--- a/src/amd/compiler/aco_register_allocation.cpp
+++ b/src/amd/compiler/aco_register_allocation.cpp
@@ -3128,6 +3128,53 @@ register_allocation(Program* program, live& live_vars, ra_test_policy policy)
                register_file.clear(op);
          }
 
+         if (instr->isCall()) {
+            /* create parallelcopy pair to move blocking vars */
+            RegisterFile tmp_file = register_file;
+            std::vector<unsigned> vars =
+               collect_vars(ctx, tmp_file, instr->call().abi.clobberedRegs.sgpr);
+            std::vector<unsigned> vars2 =
+               collect_vars(ctx, tmp_file, instr->call().abi.clobberedRegs.vgpr);
+
+            /* Allow linear VGPRs in the clobbered range, they are spilled in spill_preserved. */
+            for (auto it = vars2.begin(); it != vars2.end();) {
+               if (program->temp_rc[*it].is_linear_vgpr())
+                  it = vars2.erase(it);
+               else
+                  ++it;
+            }
+            for (auto it = vars.begin(); it != vars.end();) {
+               if (instr->operands[1].tempId() == *it)
+                  it = vars.erase(it);
+               else
+                  ++it;
+            }
+
+            vars.insert(vars.end(), vars2.begin(), vars2.end());
+
+            for (const Operand& op : instr->operands) {
+               if (op.isPrecolored())
+                  tmp_file.block(op.physReg(), op.regClass());
+               else if (op.isTemp() && op.isKillBeforeDef())
+                  tmp_file.fill(op);
+            }
+            tmp_file.block(instr->call().abi.clobberedRegs.sgpr);
+            tmp_file.block(instr->call().abi.clobberedRegs.vgpr);
+
+            adjust_max_used_regs(ctx, RegClass::s1,
+                                 instr->call().abi.clobberedRegs.sgpr.hi().reg() - 1);
+            adjust_max_used_regs(ctx, RegClass::v1,
+                                 instr->call().abi.clobberedRegs.vgpr.hi().reg() - 1);
+
+            ASSERTED bool success = false;
+            success =
+               get_regs_for_copies(ctx, tmp_file, parallelcopy, vars, instr, PhysRegInterval{});
+            assert(success);
+
+            update_renames(ctx, register_file, parallelcopy, instr,
+                           rename_precolored_ops | rename_not_killed_ops);
+         }
+
          optimize_encoding(ctx, register_file, instr);
 
          /* Handle definitions which must have the same register as an operand.
@@ -3162,6 +3209,10 @@ register_allocation(Program* program, live& live_vars, ra_test_policy policy)
                   else if (op.isTemp() && op.isKillBeforeDef())
                      tmp_file.fill(op);
                }
+               if (instr->isCall()) {
+                  tmp_file.block(instr->call().abi.clobberedRegs.sgpr);
+                  tmp_file.block(instr->call().abi.clobberedRegs.vgpr);
+               }
 
                ASSERTED bool success = false;
                success = get_regs_for_copies(ctx, tmp_file, parallelcopy, vars, instr, def_regs);
-- 
GitLab


From 2655f1f14212480df68001152d7886590437943e Mon Sep 17 00:00:00 2001
From: Friedrich Vock <friedrich.vock@gmx.de>
Date: Thu, 2 May 2024 17:28:37 +0200
Subject: [PATCH 51/71] aco/ra: Don't clear blocking by previous operands

---
 src/amd/compiler/aco_register_allocation.cpp | 3 ++-
 1 file changed, 2 insertions(+), 1 deletion(-)

diff --git a/src/amd/compiler/aco_register_allocation.cpp b/src/amd/compiler/aco_register_allocation.cpp
index a94fe73e1ad3f..10583d433f748 100644
--- a/src/amd/compiler/aco_register_allocation.cpp
+++ b/src/amd/compiler/aco_register_allocation.cpp
@@ -2095,7 +2095,8 @@ handle_fixed_operands(ra_ctx& ctx, RegisterFile& register_file,
          continue; /* the copy is already added to the list */
 
       /* clear from register_file so fixed operands are not collected be collect_vars() */
-      tmp_file.clear(src, op.regClass()); // TODO: try to avoid moving block vars to src
+      if (!tmp_file.is_blocked(src))
+         tmp_file.clear(src, op.regClass()); // TODO: try to avoid moving block vars to src
 
       BITSET_SET(mask, i);
 
-- 
GitLab


From b8a2e09af8beb73d1d4ed97fd226b713bf0b056a Mon Sep 17 00:00:00 2001
From: Friedrich Vock <friedrich.vock@gmx.de>
Date: Fri, 3 May 2024 17:37:04 +0200
Subject: [PATCH 52/71] aco/insert_waitcnt: Insert waitcnts before s_swappc too

---
 src/amd/compiler/aco_insert_waitcnt.cpp | 4 ++++
 1 file changed, 4 insertions(+)

diff --git a/src/amd/compiler/aco_insert_waitcnt.cpp b/src/amd/compiler/aco_insert_waitcnt.cpp
index 9369240c17f9a..344872706a06a 100644
--- a/src/amd/compiler/aco_insert_waitcnt.cpp
+++ b/src/amd/compiler/aco_insert_waitcnt.cpp
@@ -503,6 +503,10 @@ kill(wait_imm& imm, alu_delay_info& delay, Instruction* instr, wait_ctx& ctx,
       force_waitcnt(ctx, imm);
    }
 
+   if (instr->opcode == aco_opcode::s_swappc_b64)
+      u_foreach_bit (i, (~counter_vs) & ctx.nonzero)
+         imm[i] = 0;
+
    /* Make sure POPS coherent memory accesses have reached the L2 cache before letting the
     * overlapping waves proceed into the ordered section.
     */
-- 
GitLab


From 6b1b589ab4481f34e5911a709869db6cff7bdce7 Mon Sep 17 00:00:00 2001
From: Friedrich Vock <friedrich.vock@gmx.de>
Date: Mon, 13 May 2024 06:30:07 +0200
Subject: [PATCH 53/71] aco/ra: Add utility to clear PhysRegInterval

---
 src/amd/compiler/aco_register_allocation.cpp | 2 ++
 1 file changed, 2 insertions(+)

diff --git a/src/amd/compiler/aco_register_allocation.cpp b/src/amd/compiler/aco_register_allocation.cpp
index 10583d433f748..54ce17340f2ec 100644
--- a/src/amd/compiler/aco_register_allocation.cpp
+++ b/src/amd/compiler/aco_register_allocation.cpp
@@ -245,6 +245,8 @@ public:
 
    void block(PhysRegInterval interval) { fill(interval.lo(), interval.size, 0xFFFFFFFF); }
 
+   void clear(PhysRegInterval interval) { fill(interval.lo(), interval.size, 0); }
+
    bool is_blocked(PhysReg start) const
    {
       if (regs[start] == 0xFFFFFFFF)
-- 
GitLab


From 126d4ce193f569d5aa64b45d9c3f83a38f5c1f7c Mon Sep 17 00:00:00 2001
From: Friedrich Vock <friedrich.vock@gmx.de>
Date: Sat, 18 May 2024 10:19:58 +0200
Subject: [PATCH 54/71] aco/util: Add aco::unordered_set

---
 src/amd/compiler/aco_util.h | 9 +++++++++
 1 file changed, 9 insertions(+)

diff --git a/src/amd/compiler/aco_util.h b/src/amd/compiler/aco_util.h
index a25f8483e2b04..94734bd420c43 100644
--- a/src/amd/compiler/aco_util.h
+++ b/src/amd/compiler/aco_util.h
@@ -20,6 +20,7 @@
 #include <map>
 #include <type_traits>
 #include <unordered_map>
+#include <unordered_set>
 #include <vector>
 
 namespace aco {
@@ -572,6 +573,14 @@ template <class Key, class T, class Hash = std::hash<Key>, class Pred = std::equ
 using unordered_map =
    std::unordered_map<Key, T, Hash, Pred, aco::monotonic_allocator<std::pair<const Key, T>>>;
 
+/*
+ * aco::unordered_set - alias for std::unordered_map with monotonic_allocator
+ *
+ * This template specialization mimics std::pmr::unordered_set.
+ */
+template <class T, class Hash = std::hash<T>, class Pred = std::equal_to<T>>
+using unordered_set = std::unordered_set<T, Hash, Pred, aco::monotonic_allocator<T>>;
+
 /*
  * Helper class for a integer/bool (access_type) packed into
  * a bigger integer (data_type) with an offset and size.
-- 
GitLab


From 7ab87e66a14aa09ec52eb05414dc3327f9a5e429 Mon Sep 17 00:00:00 2001
From: Friedrich Vock <friedrich.vock@gmx.de>
Date: Mon, 13 May 2024 06:23:55 +0200
Subject: [PATCH 55/71] aco: Add pass for spilling call-related VGPRs

Spills preserved VGPRs for callees and linear VGPRs added by the
spiller.
---
 .../compiler/aco_instruction_selection.cpp    |  65 ++-
 src/amd/compiler/aco_interface.cpp            |   2 +
 src/amd/compiler/aco_ir.h                     |  11 +-
 src/amd/compiler/aco_opcodes.py               |   3 +
 src/amd/compiler/aco_opt_value_numbering.cpp  |   3 +-
 src/amd/compiler/aco_register_allocation.cpp  |  56 ++
 src/amd/compiler/aco_spill_preserved.cpp      | 544 ++++++++++++++++++
 src/amd/compiler/meson.build                  |   1 +
 8 files changed, 667 insertions(+), 18 deletions(-)
 create mode 100644 src/amd/compiler/aco_spill_preserved.cpp

diff --git a/src/amd/compiler/aco_instruction_selection.cpp b/src/amd/compiler/aco_instruction_selection.cpp
index d60b74448aa07..d7770973b83d5 100644
--- a/src/amd/compiler/aco_instruction_selection.cpp
+++ b/src/amd/compiler/aco_instruction_selection.cpp
@@ -106,9 +106,21 @@ append_logical_start(Block* b)
 }
 
 static void
-append_logical_end(Block* b)
+append_logical_end(isel_context* ctx)
 {
-   Builder(NULL, b).pseudo(aco_opcode::p_logical_end);
+   Builder bld(ctx->program, ctx->block);
+
+   Operand stack_ptr_op;
+   if (ctx->program->gfx_level >= GFX9)
+      stack_ptr_op = Operand(ctx->callee_info.stack_ptr.def.getTemp());
+   else
+      stack_ptr_op = Operand(load_scratch_resource(ctx->program, bld, true, true));
+   stack_ptr_op.setLateKill(true);
+   if (ctx->program->is_callee)
+      bld.pseudo(aco_opcode::p_reload_preserved_vgpr, bld.def(s1), bld.def(bld.lm),
+                 bld.def(s1, scc), stack_ptr_op);
+
+   bld.pseudo(aco_opcode::p_logical_end);
 }
 
 Temp
@@ -10215,7 +10227,7 @@ void
 begin_loop(isel_context* ctx, loop_context* lc)
 {
    // TODO: we might want to wrap the loop around a branch if exec_potentially_empty=true
-   append_logical_end(ctx->block);
+   append_logical_end(ctx);
    ctx->block->kind |= block_kind_loop_preheader | block_kind_uniform;
    Builder bld(ctx->program, ctx->block);
    bld.branch(aco_opcode::p_branch, bld.def(s2));
@@ -10247,7 +10259,7 @@ end_loop(isel_context* ctx, loop_context* lc)
    if (!ctx->cf_info.has_branch) {
       unsigned loop_header_idx = ctx->cf_info.parent_loop.header_idx;
       Builder bld(ctx->program, ctx->block);
-      append_logical_end(ctx->block);
+      append_logical_end(ctx);
 
       if (ctx->cf_info.exec_potentially_empty_discard ||
           ctx->cf_info.exec_potentially_empty_break) {
@@ -10309,7 +10321,7 @@ emit_loop_jump(isel_context* ctx, bool is_break)
 {
    Builder bld(ctx->program, ctx->block);
    Block* logical_target;
-   append_logical_end(ctx->block);
+   append_logical_end(ctx);
    unsigned idx = ctx->block->index;
 
    if (is_break) {
@@ -10734,7 +10746,7 @@ begin_divergent_if_then(isel_context* ctx, if_context* ic, Temp cond,
 {
    ic->cond = cond;
 
-   append_logical_end(ctx->block);
+   append_logical_end(ctx);
    ctx->block->kind |= block_kind_branch;
 
    /* branch to linear then block */
@@ -10781,7 +10793,7 @@ begin_divergent_if_else(isel_context* ctx, if_context* ic,
                         nir_selection_control sel_ctrl = nir_selection_control_none)
 {
    Block* BB_then_logical = ctx->block;
-   append_logical_end(BB_then_logical);
+   append_logical_end(ctx);
    /* branch from logical then block to invert block */
    aco_ptr<Instruction> branch;
    branch.reset(create_instruction(aco_opcode::p_branch, Format::PSEUDO_BRANCH, 0, 1));
@@ -10842,7 +10854,7 @@ static void
 end_divergent_if(isel_context* ctx, if_context* ic)
 {
    Block* BB_else_logical = ctx->block;
-   append_logical_end(BB_else_logical);
+   append_logical_end(ctx);
 
    /* branch from logical else block to endif block */
    aco_ptr<Instruction> branch;
@@ -10900,7 +10912,7 @@ begin_uniform_if_then(isel_context* ctx, if_context* ic, Temp cond)
 {
    assert(cond.regClass() == s1);
 
-   append_logical_end(ctx->block);
+   append_logical_end(ctx);
    ctx->block->kind |= block_kind_uniform;
 
    aco_ptr<Instruction> branch;
@@ -10935,7 +10947,7 @@ begin_uniform_if_else(isel_context* ctx, if_context* ic)
    Block* BB_then = ctx->block;
 
    if (!ctx->cf_info.has_branch) {
-      append_logical_end(BB_then);
+      append_logical_end(ctx);
       /* branch from then block to endif block */
       aco_ptr<Instruction> branch;
       branch.reset(create_instruction(aco_opcode::p_branch, Format::PSEUDO_BRANCH, 0, 1));
@@ -10969,7 +10981,7 @@ end_uniform_if(isel_context* ctx, if_context* ic)
    Block* BB_else = ctx->block;
 
    if (!ctx->cf_info.has_branch) {
-      append_logical_end(BB_else);
+      append_logical_end(ctx);
       /* branch from then block to endif block */
       aco_ptr<Instruction> branch;
       branch.reset(create_instruction(aco_opcode::p_branch, Format::PSEUDO_BRANCH, 0, 1));
@@ -11952,13 +11964,34 @@ select_program_rt(isel_context& ctx, unsigned shader_count, struct nir_shader* c
          ctx.program->is_callee = true;
 
          Instruction* startpgm = add_startpgm(&ctx, true);
+
+         Builder bld(ctx.program, ctx.block);
+
+         Operand stack_ptr_op;
+         if (ctx.program->gfx_level >= GFX9)
+            stack_ptr_op = Operand(ctx.callee_info.stack_ptr.def.getTemp());
+         else
+            stack_ptr_op = Operand(load_scratch_resource(ctx.program, bld, true, true));
+         stack_ptr_op.setLateKill(true);
+         bld.pseudo(aco_opcode::p_spill_preserved_vgpr, bld.def(s1), bld.def(bld.lm),
+                    bld.def(s1, scc), stack_ptr_op);
+
          append_logical_start(ctx.block);
          split_arguments(&ctx, startpgm);
          visit_cf_list(&ctx, &impl->body);
-         append_logical_end(ctx.block);
+         append_logical_end(&ctx);
          ctx.block->kind |= block_kind_uniform;
 
          if (ctx.next_pc != Temp()) {
+            bld = Builder(ctx.program, ctx.block);
+            if (ctx.program->gfx_level >= GFX9)
+               stack_ptr_op = Operand(ctx.callee_info.stack_ptr.def.getTemp());
+            else
+               stack_ptr_op = Operand(load_scratch_resource(ctx.program, bld, true, true));
+            stack_ptr_op.setLateKill(true);
+            bld.pseudo(aco_opcode::p_reload_preserved_vgpr, bld.def(s1), bld.def(bld.lm),
+                       bld.def(s1, scc), stack_ptr_op);
+
             insert_return(ctx);
 
             Builder(ctx.program, ctx.block).sop1(aco_opcode::s_setpc_b64, Operand(ctx.next_pc));
@@ -12237,7 +12270,7 @@ select_shader(isel_context& ctx, nir_shader* nir, const bool need_startpgm, cons
    if (need_endpgm) {
       program->config->float_mode = program->blocks[0].fp_mode.val;
 
-      append_logical_end(ctx.block);
+      append_logical_end(&ctx);
       ctx.block->kind |= block_kind_uniform;
 
       if ((!program->info.has_epilog && !is_first_stage_of_merged_shader) ||
@@ -12648,7 +12681,7 @@ select_trap_handler_shader(Program* program, struct nir_shader* shader, ac_shade
 
    program->config->float_mode = program->blocks[0].fp_mode.val;
 
-   append_logical_end(ctx.block);
+   append_logical_end(&ctx);
    ctx.block->kind |= block_kind_uniform;
    bld.sopp(aco_opcode::s_endpgm);
 
@@ -13567,7 +13600,7 @@ select_ps_epilog(Program* program, void* pinfo, ac_shader_config* config,
 
    program->config->float_mode = program->blocks[0].fp_mode.val;
 
-   append_logical_end(ctx.block);
+   append_logical_end(&ctx);
    ctx.block->kind |= block_kind_export_end;
    bld.reset(ctx.block);
    bld.sopp(aco_opcode::s_endpgm);
@@ -13603,7 +13636,7 @@ select_ps_prolog(Program* program, void* pinfo, ac_shader_config* config,
 
    program->config->float_mode = program->blocks[0].fp_mode.val;
 
-   append_logical_end(ctx.block);
+   append_logical_end(&ctx);
 
    build_end_with_regs(&ctx, regs);
 
diff --git a/src/amd/compiler/aco_interface.cpp b/src/amd/compiler/aco_interface.cpp
index 14d71cbb521dc..afefa91f59e8f 100644
--- a/src/amd/compiler/aco_interface.cpp
+++ b/src/amd/compiler/aco_interface.cpp
@@ -173,6 +173,8 @@ aco_postprocess_shader(const struct aco_compiler_options* options,
          validate(program.get());
       }
 
+      spill_preserved(program.get());
+
       ssa_elimination(program.get());
    }
 
diff --git a/src/amd/compiler/aco_ir.h b/src/amd/compiler/aco_ir.h
index 5475fbbacb340..03266b9f83e6e 100644
--- a/src/amd/compiler/aco_ir.h
+++ b/src/amd/compiler/aco_ir.h
@@ -1989,7 +1989,9 @@ is_dead(const std::vector<uint16_t>& uses, const Instruction* instr)
 {
    if (instr->definitions.empty() || instr->isBranch() || instr->isCall() ||
        instr->opcode == aco_opcode::p_startpgm || instr->opcode == aco_opcode::p_init_scratch ||
-       instr->opcode == aco_opcode::p_dual_src_export_gfx11)
+       instr->opcode == aco_opcode::p_dual_src_export_gfx11 ||
+       instr->opcode == aco_opcode::p_spill_preserved_vgpr ||
+       instr->opcode == aco_opcode::p_reload_preserved_vgpr)
       return false;
 
    if (std::any_of(instr->definitions.begin(), instr->definitions.end(),
@@ -2442,6 +2444,7 @@ void optimize_postRA(Program* program);
 void setup_reduce_temp(Program* program);
 void lower_to_cssa(Program* program, live& live_vars);
 void register_allocation(Program* program, live& live_vars, ra_test_policy = {});
+void spill_preserved(Program* program);
 void ssa_elimination(Program* program);
 void lower_to_hw_instr(Program* program);
 void schedule_program(Program* program, live& live_vars);
@@ -2546,4 +2549,10 @@ extern const Info instr_info;
 
 } // namespace aco
 
+namespace std {
+template <> struct hash<aco::PhysReg> {
+   size_t operator()(aco::PhysReg temp) const noexcept { return std::hash<uint32_t>{}(temp.reg_b); }
+};
+} // namespace std
+
 #endif /* ACO_IR_H */
diff --git a/src/amd/compiler/aco_opcodes.py b/src/amd/compiler/aco_opcodes.py
index 5a11ea8e8267b..ed3c285158fd6 100644
--- a/src/amd/compiler/aco_opcodes.py
+++ b/src/amd/compiler/aco_opcodes.py
@@ -341,6 +341,9 @@ insn("p_unit_test")
 
 insn("p_callee_stack_ptr")
 
+insn("p_spill_preserved_vgpr")
+insn("p_reload_preserved_vgpr")
+
 insn("p_create_vector")
 insn("p_extract_vector")
 insn("p_split_vector")
diff --git a/src/amd/compiler/aco_opt_value_numbering.cpp b/src/amd/compiler/aco_opt_value_numbering.cpp
index 4f46f386d6960..2980a9f954c25 100644
--- a/src/amd/compiler/aco_opt_value_numbering.cpp
+++ b/src/amd/compiler/aco_opt_value_numbering.cpp
@@ -307,7 +307,8 @@ can_eliminate(aco_ptr<Instruction>& instr)
    if (instr->definitions.empty() || instr->opcode == aco_opcode::p_phi ||
        instr->opcode == aco_opcode::p_linear_phi ||
        instr->opcode == aco_opcode::p_pops_gfx9_add_exiting_wave_id ||
-       instr->definitions[0].isNoCSE())
+       instr->definitions[0].isNoCSE() || instr->opcode == aco_opcode::p_spill_preserved_vgpr ||
+       instr->opcode == aco_opcode::p_reload_preserved_vgpr)
       return false;
 
    return true;
diff --git a/src/amd/compiler/aco_register_allocation.cpp b/src/amd/compiler/aco_register_allocation.cpp
index 54ce17340f2ec..811d6a9e7fbf5 100644
--- a/src/amd/compiler/aco_register_allocation.cpp
+++ b/src/amd/compiler/aco_register_allocation.cpp
@@ -2455,6 +2455,23 @@ init_reg_file(ra_ctx& ctx, const std::vector<IDSet>& live_out_per_block, Block&
    const IDSet& live_in = live_out_per_block[block.index];
    assert(block.index != 0 || live_in.empty());
 
+   /* Callee shaders only get a chance to spill preserved registers after p_startpgm.
+    * To make sure nothing uses these regs until we can spill them, block them here.
+    */
+   if (block.index == 0 && ctx.program->is_callee) {
+      PhysRegInterval preserved_vgpr_lo = PhysRegInterval{
+         .lo_ = PhysReg{256u + ctx.program->arg_vgpr_count},
+         .size =
+            ctx.program->callee_abi.clobberedRegs.vgpr.lo() - 256 - ctx.program->arg_vgpr_count,
+      };
+      PhysRegInterval preserved_vgpr_hi = PhysRegInterval{
+         .lo_ = ctx.program->callee_abi.clobberedRegs.vgpr.hi(),
+         .size = PhysReg{256u + ctx.vgpr_limit} - ctx.program->callee_abi.clobberedRegs.vgpr.hi(),
+      };
+      register_file.block(preserved_vgpr_hi);
+      register_file.block(preserved_vgpr_lo);
+   }
+
    if (block.kind & block_kind_loop_header) {
       ctx.loop_header.emplace_back(block.index);
       /* already rename phis incoming value */
@@ -3054,6 +3071,31 @@ register_allocation(Program* program, live& live_vars, ra_test_policy policy)
             instructions.emplace_back(std::move(instr));
             break;
          }
+         if (instr->opcode == aco_opcode::p_reload_preserved_vgpr && block.linear_succs.empty()) {
+            PhysRegInterval preserved_vgpr_lo = PhysRegInterval{
+               .lo_ = PhysReg{256u + ctx.program->arg_vgpr_count},
+               .size = ctx.program->callee_abi.clobberedRegs.vgpr.lo() - 256u -
+                       ctx.program->arg_vgpr_count,
+            };
+            PhysRegInterval preserved_vgpr_hi = PhysRegInterval{
+               .lo_ = ctx.program->callee_abi.clobberedRegs.vgpr.hi(),
+               .size =
+                  PhysReg{256u + ctx.vgpr_limit} - ctx.program->callee_abi.clobberedRegs.vgpr.hi(),
+            };
+            std::vector<unsigned> vars = collect_vars(ctx, register_file, preserved_vgpr_lo);
+            std::vector<unsigned> vars2 = collect_vars(ctx, register_file, preserved_vgpr_hi);
+            vars.insert(vars.end(), vars2.begin(), vars2.end());
+
+            register_file.block(preserved_vgpr_lo);
+            register_file.block(preserved_vgpr_hi);
+
+            ASSERTED bool success = false;
+            success = get_regs_for_copies(ctx, register_file, parallelcopy, vars, instr,
+                                          PhysRegInterval{});
+            assert(success);
+
+            update_renames(ctx, register_file, parallelcopy, instr, (UpdateRenames)0);
+         }
 
          assert(!is_phi(instr));
 
@@ -3396,6 +3438,20 @@ register_allocation(Program* program, live& live_vars, ra_test_policy policy)
             instr->format = asVOP3(instr->format);
          }
 
+         if (instr->opcode == aco_opcode::p_spill_preserved_vgpr) {
+            PhysRegInterval preserved_vgpr_lo = PhysRegInterval{
+               .lo_ = PhysReg{256u + ctx.program->arg_vgpr_count},
+               .size = ctx.program->callee_abi.clobberedRegs.vgpr.lo() - 256u -
+                       ctx.program->arg_vgpr_count,
+            };
+            PhysRegInterval preserved_vgpr_hi = PhysRegInterval{
+               .lo_ = ctx.program->callee_abi.clobberedRegs.vgpr.hi(),
+               .size =
+                  PhysReg{256u + ctx.vgpr_limit} - ctx.program->callee_abi.clobberedRegs.vgpr.hi(),
+            };
+            register_file.clear(preserved_vgpr_hi);
+            register_file.clear(preserved_vgpr_lo);
+         }
          instructions.emplace_back(std::move(*instr_it));
 
       } /* end for Instr */
diff --git a/src/amd/compiler/aco_spill_preserved.cpp b/src/amd/compiler/aco_spill_preserved.cpp
new file mode 100644
index 0000000000000..bf005aa509bb2
--- /dev/null
+++ b/src/amd/compiler/aco_spill_preserved.cpp
@@ -0,0 +1,544 @@
+/*
+ * Copyright Â© 2024 Valve Corporation
+ *
+ * SPDX-License-Identifier: MIT
+ */
+
+#include "aco_builder.h"
+#include "aco_ir.h"
+
+#include <set>
+#include <unordered_set>
+
+namespace aco {
+
+struct postdom_info {
+   unsigned logical_imm_postdom;
+   unsigned linear_imm_postdom;
+};
+
+struct spill_preserved_ctx {
+   Program* program;
+   aco::monotonic_buffer_resource memory;
+
+   aco::unordered_map<PhysReg, uint32_t> preserved_spill_offsets;
+   aco::unordered_set<PhysReg> preserved_regs;
+   aco::unordered_set<PhysReg> preserved_linear_regs;
+
+   aco::unordered_map<PhysReg, std::unordered_set<unsigned>> reg_block_uses;
+   std::vector<postdom_info> dom_info;
+
+   unsigned next_preserved_offset;
+
+   explicit spill_preserved_ctx(Program* program_)
+       : program(program_), memory(), preserved_spill_offsets(memory), preserved_regs(memory),
+         preserved_linear_regs(memory), reg_block_uses(memory),
+         next_preserved_offset(
+            DIV_ROUND_UP(program_->config->scratch_bytes_per_wave, program_->wave_size))
+   {
+      dom_info.resize(program->blocks.size(), {-1u, -1u});
+   }
+};
+
+void
+add_instr(spill_preserved_ctx& ctx, unsigned block_index, bool seen_reload,
+          const aco_ptr<Instruction>& instr)
+{
+   for (auto& def : instr->definitions) {
+      assert(def.isFixed());
+      if (def.regClass().type() == RegType::sgpr)
+         continue;
+      /* Round down subdword registers to their base */
+      PhysReg start_reg = PhysReg{def.physReg().reg()};
+      for (auto reg : PhysRegInterval{start_reg, def.regClass().size()}) {
+         if (reg < 256u + ctx.program->arg_vgpr_count)
+            continue;
+         if (ctx.program->callee_abi.clobberedRegs.vgpr.contains(reg) &&
+             !def.regClass().is_linear_vgpr())
+            continue;
+         /* Don't count start_linear_vgpr without a copy as a use since the value doesn't matter.
+          * This allows us to move reloads a bit further up the CF.
+          */
+         if (instr->opcode == aco_opcode::p_start_linear_vgpr && instr->operands.empty())
+            continue;
+
+         if (def.regClass().is_linear_vgpr())
+            ctx.preserved_linear_regs.insert(reg);
+         else
+            ctx.preserved_regs.insert(reg);
+
+         if (seen_reload) {
+            if (def.regClass().is_linear_vgpr())
+               for (auto succ : ctx.program->blocks[block_index].linear_succs)
+                  ctx.reg_block_uses[reg].emplace(succ);
+            else
+               for (auto succ : ctx.program->blocks[block_index].logical_succs)
+                  ctx.reg_block_uses[reg].emplace(succ);
+         } else {
+            ctx.reg_block_uses[reg].emplace(block_index);
+         }
+      }
+   }
+   for (auto& op : instr->operands) {
+      assert(op.isFixed());
+      if (op.regClass().type() == RegType::sgpr)
+         continue;
+      if (op.isConstant())
+         continue;
+      /* Round down subdword registers to their base */
+      PhysReg start_reg = PhysReg{op.physReg().reg()};
+      for (auto reg : PhysRegInterval{start_reg, op.regClass().size()}) {
+         if (reg < 256u + ctx.program->arg_vgpr_count)
+            continue;
+         /* Don't count end_linear_vgpr as a use since the value doesn't matter.
+          * This allows us to move reloads a bit further up the CF.
+          */
+         if (instr->opcode == aco_opcode::p_end_linear_vgpr)
+            continue;
+         if (ctx.program->callee_abi.clobberedRegs.vgpr.contains(reg) &&
+             !op.regClass().is_linear_vgpr())
+            continue;
+         if (op.regClass().is_linear_vgpr())
+            ctx.preserved_linear_regs.insert(reg);
+
+         if (seen_reload) {
+            if (op.regClass().is_linear_vgpr())
+               for (auto succ : ctx.program->blocks[block_index].linear_succs)
+                  ctx.reg_block_uses[reg].emplace(succ);
+            else
+               for (auto succ : ctx.program->blocks[block_index].logical_succs)
+                  ctx.reg_block_uses[reg].emplace(succ);
+         } else {
+            ctx.reg_block_uses[reg].emplace(block_index);
+         }
+      }
+   }
+}
+
+void
+spill_preserved(spill_preserved_ctx& ctx, PhysReg reg, std::vector<std::pair<PhysReg, int>>& spills,
+                std::vector<std::pair<PhysReg, int>>& lvgpr_spills)
+{
+   unsigned offset;
+
+   auto offset_iter = ctx.preserved_spill_offsets.find(reg);
+   if (offset_iter == ctx.preserved_spill_offsets.end()) {
+      offset = ctx.next_preserved_offset;
+      ctx.next_preserved_offset += 4;
+      ctx.preserved_spill_offsets.emplace(reg, offset);
+   } else {
+      offset = offset_iter->second;
+   }
+
+   if (ctx.preserved_linear_regs.find(reg) != ctx.preserved_linear_regs.end())
+      lvgpr_spills.emplace_back(reg, offset);
+   else
+      spills.emplace_back(reg, offset);
+}
+
+void
+emit_spills_reloads_internal(spill_preserved_ctx& ctx, Builder& bld,
+                             std::vector<std::pair<PhysReg, int>>& spills, PhysReg stack_reg,
+                             PhysReg soffset, bool reload, bool linear, bool soffset_valid)
+{
+   if (spills.empty())
+      return;
+
+   int end_offset = spills.back().second;
+   int start_offset = spills.front().second;
+   assert(end_offset - start_offset < ctx.program->dev.scratch_global_offset_max);
+
+   bool overflow =
+      end_offset > ctx.program->dev.scratch_global_offset_max || ctx.program->gfx_level < GFX9;
+   if (overflow) {
+      if (ctx.program->gfx_level >= GFX9)
+         bld.sop2(aco_opcode::s_add_u32, Definition(soffset, s1), Definition(scc, s1),
+                  Operand(stack_reg, s1), Operand::c32(start_offset));
+      else if (soffset_valid)
+         bld.sop2(aco_opcode::s_add_u32, Definition(soffset, s1), Definition(scc, s1),
+                  Operand(soffset, s1), Operand::c32(start_offset * ctx.program->wave_size));
+      else
+         bld.sop1(aco_opcode::s_mov_b32, Definition(soffset, s1),
+                  Operand::c32(start_offset * ctx.program->wave_size));
+   }
+
+   Operand soffset_op;
+   if (ctx.program->gfx_level >= GFX9)
+      soffset_op = Operand(overflow ? soffset : stack_reg, s1);
+   else
+      soffset_op = soffset_valid || overflow ? Operand(soffset, s1) : Operand(sgpr_null, s1);
+
+   for (const auto& spill : spills) {
+      if (ctx.program->gfx_level >= GFX9) {
+         if (reload)
+            bld.scratch(aco_opcode::scratch_load_dword,
+                        Definition(spill.first, linear ? v1.as_linear() : v1), Operand(v1),
+                        soffset_op, overflow ? spill.second - start_offset : spill.second,
+                        memory_sync_info(storage_vgpr_spill, semantic_private));
+         else
+            bld.scratch(aco_opcode::scratch_store_dword, Operand(v1), soffset_op,
+                        Operand(spill.first, linear ? v1.as_linear() : v1),
+                        overflow ? spill.second - start_offset : spill.second,
+                        memory_sync_info(storage_vgpr_spill, semantic_private));
+      } else {
+         if (reload) {
+            Instruction* instr = bld.mubuf(
+               aco_opcode::buffer_load_dword, Definition(spill.first, linear ? v1.as_linear() : v1),
+               Operand(stack_reg, s4), Operand(v1), soffset_op,
+               overflow ? spill.second - start_offset : spill.second, false, true);
+            instr->mubuf().sync = memory_sync_info(storage_vgpr_spill, semantic_private);
+         } else {
+            Instruction* instr =
+               bld.mubuf(aco_opcode::buffer_store_dword, Operand(stack_reg, s4), Operand(v1),
+                         soffset_op, Operand(spill.first, linear ? v1.as_linear() : v1),
+                         overflow ? spill.second - start_offset : spill.second, false, true);
+            instr->mubuf().sync = memory_sync_info(storage_vgpr_spill, semantic_private);
+         }
+      }
+   }
+
+   if (overflow && ctx.program->gfx_level < GFX9)
+      bld.sop2(aco_opcode::s_sub_i32, Definition(soffset, s1), Definition(scc, s1),
+               Operand(soffset, s1), Operand::c32(start_offset * ctx.program->wave_size));
+}
+
+void
+emit_spills_reloads(spill_preserved_ctx& ctx, std::vector<aco_ptr<Instruction>>& instructions,
+                    std::vector<aco_ptr<Instruction>>::iterator& insert_point,
+                    std::vector<std::pair<PhysReg, int>>& spills,
+                    std::vector<std::pair<PhysReg, int>>& lvgpr_spills, bool reload)
+{
+   auto spill_reload_compare = [](const auto& first, const auto& second)
+   { return first.second < second.second; };
+
+   std::sort(spills.begin(), spills.end(), spill_reload_compare);
+   std::sort(lvgpr_spills.begin(), lvgpr_spills.end(), spill_reload_compare);
+
+   PhysReg stack_reg = (*insert_point)->operands[0].physReg();
+   PhysReg soffset = (*insert_point)->definitions[0].physReg();
+   PhysReg exec_backup = (*insert_point)->definitions[1].physReg();
+
+   std::vector<aco_ptr<Instruction>> spill_instructions;
+   Builder bld(ctx.program, &spill_instructions);
+
+   emit_spills_reloads_internal(ctx, bld, spills, stack_reg, soffset, reload, false, false);
+   if (!lvgpr_spills.empty()) {
+      bld.sop1(Builder::s_or_saveexec, Definition(exec_backup, bld.lm), Definition(scc, s1),
+               Definition(exec, bld.lm), Operand::c64(UINT64_MAX), Operand(exec, bld.lm));
+      emit_spills_reloads_internal(ctx, bld, lvgpr_spills, stack_reg, soffset, reload, true, false);
+      bld.sop1(Builder::WaveSpecificOpcode::s_mov, Definition(exec, bld.lm),
+               Operand(exec_backup, bld.lm));
+   }
+
+   insert_point = instructions.erase(insert_point);
+   instructions.insert(insert_point, std::move_iterator(spill_instructions.begin()),
+                       std::move_iterator(spill_instructions.end()));
+}
+
+void
+init_block_info(spill_preserved_ctx& ctx)
+{
+   unsigned cur_loop_header = -1u;
+   for (unsigned index = ctx.program->blocks.size() - 1; index < ctx.program->blocks.size();) {
+      const Block& block = ctx.program->blocks[index];
+
+      if (block.linear_succs.empty()) {
+         ctx.dom_info[index].logical_imm_postdom = block.index;
+         ctx.dom_info[index].linear_imm_postdom = block.index;
+      } else {
+         int new_logical_postdom = -1;
+         int new_linear_postdom = -1;
+         for (unsigned succ_idx : block.logical_succs) {
+            if ((int)ctx.dom_info[succ_idx].logical_imm_postdom == -1) {
+               assert(cur_loop_header == -1u || succ_idx >= cur_loop_header);
+               if (cur_loop_header == -1u)
+                  cur_loop_header = succ_idx;
+               continue;
+            }
+
+            if (new_logical_postdom == -1) {
+               new_logical_postdom = (int)succ_idx;
+               continue;
+            }
+
+            while ((int)succ_idx != new_logical_postdom) {
+               if ((int)succ_idx < new_logical_postdom)
+                  succ_idx = ctx.dom_info[succ_idx].logical_imm_postdom;
+               if ((int)succ_idx > new_logical_postdom)
+                  new_logical_postdom = (int)ctx.dom_info[new_logical_postdom].logical_imm_postdom;
+            }
+         }
+
+         for (unsigned succ_idx : block.linear_succs) {
+            if ((int)ctx.dom_info[succ_idx].linear_imm_postdom == -1) {
+               assert(cur_loop_header == -1u || succ_idx >= cur_loop_header);
+               if (cur_loop_header == -1u)
+                  cur_loop_header = succ_idx;
+               continue;
+            }
+
+            if (new_linear_postdom == -1) {
+               new_linear_postdom = (int)succ_idx;
+               continue;
+            }
+
+            while ((int)succ_idx != new_linear_postdom) {
+               if ((int)succ_idx < new_linear_postdom)
+                  succ_idx = ctx.dom_info[succ_idx].linear_imm_postdom;
+               if ((int)succ_idx > new_linear_postdom)
+                  new_linear_postdom = (int)ctx.dom_info[new_linear_postdom].linear_imm_postdom;
+            }
+         }
+
+         ctx.dom_info[index].logical_imm_postdom = new_logical_postdom;
+         ctx.dom_info[index].linear_imm_postdom = new_linear_postdom;
+      }
+
+      bool seen_reload_vgpr = false;
+      for (auto& instr : block.instructions) {
+         if (instr->opcode == aco_opcode::p_reload_preserved_vgpr) {
+            seen_reload_vgpr = true;
+            continue;
+         }
+
+         add_instr(ctx, index, seen_reload_vgpr, instr);
+      }
+
+      /* Process predecessors of loop headers again, since post-dominance information of the header
+       * was not available the first time
+       */
+      unsigned next_idx = index - 1;
+      if (index == cur_loop_header) {
+         assert(block.kind & block_kind_loop_header);
+         for (auto pred : block.logical_preds)
+            if (ctx.dom_info[pred].logical_imm_postdom == -1u)
+               next_idx = std::max(next_idx, pred);
+         for (auto pred : block.linear_preds)
+            if (ctx.dom_info[pred].linear_imm_postdom == -1u)
+               next_idx = std::max(next_idx, pred);
+         cur_loop_header = -1u;
+      }
+      index = next_idx;
+   }
+}
+
+struct call_spill {
+   unsigned instr_idx;
+   std::vector<std::pair<PhysReg, int>> spills;
+};
+
+void
+emit_call_spills(spill_preserved_ctx& ctx)
+{
+   std::set<PhysReg> linear_vgprs;
+   std::unordered_map<unsigned, std::vector<call_spill>> block_call_spills;
+
+   unsigned max_scratch_offset = ctx.next_preserved_offset;
+
+   for (auto& block : ctx.program->blocks) {
+      for (auto it = block.instructions.begin(); it != block.instructions.end(); ++it) {
+         auto& instr = *it;
+
+         if (instr->opcode == aco_opcode::p_call) {
+            unsigned scratch_offset = ctx.next_preserved_offset;
+            struct call_spill spill = {
+               .instr_idx = (unsigned)(it - block.instructions.begin()),
+            };
+            for (auto& reg : linear_vgprs) {
+               if (!instr->call().abi.clobberedRegs.vgpr.contains(reg))
+                  continue;
+               spill.spills.emplace_back(reg, scratch_offset);
+               scratch_offset += 4;
+            }
+            max_scratch_offset = std::max(max_scratch_offset, scratch_offset);
+
+            block_call_spills[block.index].emplace_back(std::move(spill));
+         } else if (instr->opcode == aco_opcode::p_start_linear_vgpr) {
+            linear_vgprs.insert(instr->definitions[0].physReg());
+         } else if (instr->opcode == aco_opcode::p_end_linear_vgpr) {
+            for (auto& op : instr->operands)
+               linear_vgprs.erase(op.physReg());
+         }
+      }
+   }
+
+   /* XXX: This should also be possible on GFX9, although small negative scratch offsets
+    * seem to hang the GPU, so disable it there now.
+    */
+   if (ctx.program->gfx_level >= GFX10)
+      for (auto& block_calls : block_call_spills)
+         for (auto& call_spills : block_calls.second)
+            for (auto& spill : call_spills.spills)
+               spill.second -= max_scratch_offset;
+
+   for (auto& block_calls : block_call_spills) {
+      for (unsigned i = 0; i < block_calls.second.size(); ++i) {
+         auto& block = ctx.program->blocks[block_calls.first];
+         auto& call = block_calls.second[i];
+         auto& instr = block.instructions[call.instr_idx];
+         auto it = block.instructions.begin() + call.instr_idx;
+         unsigned num_inserted_instrs = 0;
+
+         std::vector<aco_ptr<Instruction>> spill_instructions;
+         Builder bld(ctx.program, &spill_instructions);
+
+         PhysReg stack_reg = instr->operands[1].physReg();
+         PhysReg soffset = PhysReg{UINT32_MAX};
+         PhysReg scratch_rsrc = PhysReg{UINT32_MAX};
+         if (ctx.program->gfx_level < GFX9)
+            scratch_rsrc = instr->operands.back().physReg();
+
+         if (ctx.program->gfx_level >= GFX10) {
+            bld.sop2(aco_opcode::s_add_u32, Definition(stack_reg, s1), Definition(scc, s1),
+                     Operand(stack_reg, s1), Operand::c32(max_scratch_offset));
+            emit_spills_reloads_internal(ctx, bld, call.spills, stack_reg, soffset, false, true,
+                                         false);
+         } else if (ctx.program->gfx_level == GFX9) {
+            emit_spills_reloads_internal(ctx, bld, call.spills, stack_reg, soffset, false, true,
+                                         false);
+            bld.sop2(aco_opcode::s_add_u32, Definition(stack_reg, s1), Definition(scc, s1),
+                     Operand(stack_reg, s1), Operand::c32(max_scratch_offset));
+         } else {
+            emit_spills_reloads_internal(ctx, bld, call.spills, scratch_rsrc, stack_reg, false,
+                                         true, true);
+            bld.sop2(aco_opcode::s_add_u32, Definition(stack_reg, s1), Definition(scc, s1),
+                     Operand(stack_reg, s1),
+                     Operand::c32(max_scratch_offset * ctx.program->wave_size));
+         }
+
+         it = block.instructions.insert(it, std::move_iterator(spill_instructions.begin()),
+                                        std::move_iterator(spill_instructions.end()));
+         it += spill_instructions.size() + 1;
+         num_inserted_instrs += spill_instructions.size();
+
+         spill_instructions.clear();
+
+         if (ctx.program->gfx_level >= GFX10) {
+            emit_spills_reloads_internal(ctx, bld, call.spills, stack_reg, soffset, true, true,
+                                         false);
+            bld.sop2(aco_opcode::s_sub_u32, Definition(stack_reg, s1), Definition(scc, s1),
+                     Operand(stack_reg, s1), Operand::c32(max_scratch_offset));
+         } else if (ctx.program->gfx_level == GFX9) {
+            bld.sop2(aco_opcode::s_sub_u32, Definition(stack_reg, s1), Definition(scc, s1),
+                     Operand(stack_reg, s1), Operand::c32(max_scratch_offset));
+            emit_spills_reloads_internal(ctx, bld, call.spills, stack_reg, soffset, true, true,
+                                         false);
+         } else {
+            bld.sop2(aco_opcode::s_sub_u32, Definition(stack_reg, s1), Definition(scc, s1),
+                     Operand(stack_reg, s1),
+                     Operand::c32(max_scratch_offset * ctx.program->wave_size));
+            emit_spills_reloads_internal(ctx, bld, call.spills, scratch_rsrc, stack_reg, true, true,
+                                         true);
+         }
+
+         block.instructions.insert(it, std::move_iterator(spill_instructions.begin()),
+                                   std::move_iterator(spill_instructions.end()));
+         num_inserted_instrs += spill_instructions.size();
+
+         for (unsigned j = i + 1; j < block_calls.second.size(); ++j)
+            block_calls.second[j].instr_idx += num_inserted_instrs;
+      }
+   }
+
+   ctx.next_preserved_offset = max_scratch_offset;
+}
+
+void
+emit_preserved_spills(spill_preserved_ctx& ctx)
+{
+   std::vector<std::pair<PhysReg, int>> spills;
+   std::vector<std::pair<PhysReg, int>> lvgpr_spills;
+
+   for (auto reg : ctx.preserved_regs)
+      spill_preserved(ctx, reg, spills, lvgpr_spills);
+   for (auto reg : ctx.preserved_linear_regs)
+      spill_preserved(ctx, reg, spills, lvgpr_spills);
+
+   auto start_instr = std::find_if(ctx.program->blocks.front().instructions.begin(),
+                                   ctx.program->blocks.front().instructions.end(),
+                                   [](const auto& instr)
+                                   { return instr->opcode == aco_opcode::p_spill_preserved_vgpr; });
+   emit_spills_reloads(ctx, ctx.program->blocks.front().instructions, start_instr, spills,
+                       lvgpr_spills, false);
+
+   auto block_reloads =
+      std::vector<std::vector<std::pair<PhysReg, int>>>(ctx.program->blocks.size());
+   auto lvgpr_block_reloads =
+      std::vector<std::vector<std::pair<PhysReg, int>>>(ctx.program->blocks.size());
+
+   for (auto it = ctx.reg_block_uses.begin(); it != ctx.reg_block_uses.end();) {
+      bool is_linear = ctx.preserved_linear_regs.find(it->first) != ctx.preserved_linear_regs.end();
+
+      if (!is_linear && ctx.preserved_regs.find(it->first) == ctx.preserved_regs.end()) {
+         it = ctx.reg_block_uses.erase(it);
+         continue;
+      }
+
+      unsigned min_common_postdom = 0;
+
+      for (auto succ_idx : it->second) {
+         while (succ_idx != min_common_postdom) {
+            if (min_common_postdom < succ_idx) {
+               min_common_postdom = is_linear
+                                       ? ctx.dom_info[min_common_postdom].linear_imm_postdom
+                                       : ctx.dom_info[min_common_postdom].logical_imm_postdom;
+            } else {
+               succ_idx = is_linear ? ctx.dom_info[succ_idx].linear_imm_postdom
+                                    : ctx.dom_info[succ_idx].logical_imm_postdom;
+            }
+         }
+      }
+
+      while (std::find_if(ctx.program->blocks[min_common_postdom].instructions.rbegin(),
+                          ctx.program->blocks[min_common_postdom].instructions.rend(),
+                          [](const auto& instr) {
+                             return instr->opcode == aco_opcode::p_reload_preserved_vgpr;
+                          }) == ctx.program->blocks[min_common_postdom].instructions.rend())
+         min_common_postdom = is_linear ? ctx.dom_info[min_common_postdom].linear_imm_postdom
+                                        : ctx.dom_info[min_common_postdom].logical_imm_postdom;
+
+      if (is_linear) {
+         lvgpr_block_reloads[min_common_postdom].emplace_back(
+            it->first, ctx.preserved_spill_offsets[it->first]);
+         ctx.preserved_linear_regs.erase(it->first);
+      } else {
+         block_reloads[min_common_postdom].emplace_back(it->first,
+                                                        ctx.preserved_spill_offsets[it->first]);
+         ctx.preserved_regs.erase(it->first);
+      }
+
+      it = ctx.reg_block_uses.erase(it);
+   }
+
+   for (unsigned i = 0; i < ctx.program->blocks.size(); ++i) {
+      auto instr_it = std::find_if(
+         ctx.program->blocks[i].instructions.rbegin(), ctx.program->blocks[i].instructions.rend(),
+         [](const auto& instr) { return instr->opcode == aco_opcode::p_reload_preserved_vgpr; });
+      if (instr_it == ctx.program->blocks[i].instructions.rend()) {
+         assert(block_reloads[i].empty() && lvgpr_block_reloads[i].empty());
+         continue;
+      }
+      auto end_instr = std::prev(instr_it.base());
+      emit_spills_reloads(ctx, ctx.program->blocks[i].instructions, end_instr, block_reloads[i],
+                          lvgpr_block_reloads[i], true);
+   }
+}
+
+void
+spill_preserved(Program* program)
+{
+   if (!program->is_callee)
+      return;
+
+   spill_preserved_ctx ctx(program);
+
+   init_block_info(ctx);
+
+   if (!program->bypass_reg_preservation)
+      emit_preserved_spills(ctx);
+
+   emit_call_spills(ctx);
+
+   program->config->scratch_bytes_per_wave = ctx.next_preserved_offset * program->wave_size;
+}
+} // namespace aco
diff --git a/src/amd/compiler/meson.build b/src/amd/compiler/meson.build
index f7024fede70d4..afd49d7a2739e 100644
--- a/src/amd/compiler/meson.build
+++ b/src/amd/compiler/meson.build
@@ -62,6 +62,7 @@ libaco_files = files(
   'aco_scheduler.cpp',
   'aco_scheduler_ilp.cpp',
   'aco_spill.cpp',
+  'aco_spill_preserved.cpp',
   'aco_ssa_elimination.cpp',
   'aco_statistics.cpp',
   'aco_util.h',
-- 
GitLab


From 435e1386336b96704c89f35bd20092e2657b8f78 Mon Sep 17 00:00:00 2001
From: Friedrich Vock <friedrich.vock@gmx.de>
Date: Mon, 13 May 2024 06:26:51 +0200
Subject: [PATCH 56/71] aco: Add cur_reg_demand to Program

For checking whether spilling of preserved SGPRs is needed.
---
 src/amd/compiler/aco_ir.h                  | 1 +
 src/amd/compiler/aco_live_var_analysis.cpp | 1 +
 2 files changed, 2 insertions(+)

diff --git a/src/amd/compiler/aco_ir.h b/src/amd/compiler/aco_ir.h
index 03266b9f83e6e..cfca9425df658 100644
--- a/src/amd/compiler/aco_ir.h
+++ b/src/amd/compiler/aco_ir.h
@@ -2290,6 +2290,7 @@ public:
    std::vector<Block> blocks;
    std::vector<RegClass> temp_rc = {s1};
    RegisterDemand max_reg_demand = RegisterDemand();
+   RegisterDemand cur_reg_demand = RegisterDemand();
    ac_shader_config* config;
    struct aco_shader_info info;
    enum amd_gfx_level gfx_level;
diff --git a/src/amd/compiler/aco_live_var_analysis.cpp b/src/amd/compiler/aco_live_var_analysis.cpp
index fa25cf8a35e67..1e5526bb22768 100644
--- a/src/amd/compiler/aco_live_var_analysis.cpp
+++ b/src/amd/compiler/aco_live_var_analysis.cpp
@@ -501,6 +501,7 @@ update_vgpr_sgpr_demand(Program* program, const RegisterDemand new_demand)
    uint16_t sgpr_limit = get_addr_sgpr_from_waves(program, program->min_waves);
    uint16_t vgpr_limit = get_addr_vgpr_from_waves(program, program->min_waves);
 
+   program->cur_reg_demand = new_demand;
    /* this won't compile, register pressure reduction necessary */
    if (new_demand.vgpr > vgpr_limit || new_demand.sgpr > sgpr_limit) {
       program->num_waves = 0;
-- 
GitLab


From 5d09fc568b4a27b7ebbe402b936187e2421a6a3b Mon Sep 17 00:00:00 2001
From: Friedrich Vock <friedrich.vock@gmx.de>
Date: Mon, 13 May 2024 06:28:31 +0200
Subject: [PATCH 57/71] aco: Spill callee-preserved SGPRs

---
 src/amd/compiler/aco_opcodes.py              |   2 +
 src/amd/compiler/aco_register_allocation.cpp |  47 +++++++-
 src/amd/compiler/aco_scheduler.cpp           |   8 ++
 src/amd/compiler/aco_spill.cpp               | 119 +++++++++++++++++--
 4 files changed, 168 insertions(+), 8 deletions(-)

diff --git a/src/amd/compiler/aco_opcodes.py b/src/amd/compiler/aco_opcodes.py
index ed3c285158fd6..4e8109a216bae 100644
--- a/src/amd/compiler/aco_opcodes.py
+++ b/src/amd/compiler/aco_opcodes.py
@@ -342,7 +342,9 @@ insn("p_unit_test")
 insn("p_callee_stack_ptr")
 
 insn("p_spill_preserved_vgpr")
+insn("p_spill_preserved_sgpr")
 insn("p_reload_preserved_vgpr")
+insn("p_reload_preserved_sgpr")
 
 insn("p_create_vector")
 insn("p_extract_vector")
diff --git a/src/amd/compiler/aco_register_allocation.cpp b/src/amd/compiler/aco_register_allocation.cpp
index 811d6a9e7fbf5..9caf2edebff1d 100644
--- a/src/amd/compiler/aco_register_allocation.cpp
+++ b/src/amd/compiler/aco_register_allocation.cpp
@@ -16,6 +16,7 @@
 #include <optional>
 #include <set>
 #include <unordered_map>
+#include <unordered_set>
 #include <vector>
 
 namespace aco {
@@ -2987,11 +2988,35 @@ register_allocation(Program* program, live& live_vars, ra_test_policy policy)
    ra_ctx ctx(program, policy);
    get_affinities(ctx, live_out_per_block);
 
+   std::unordered_set<PhysReg> blocked_sgpr;
+   if (ctx.program->is_callee) {
+      PhysRegInterval preserved_sgpr_lo = PhysRegInterval{
+         .lo_ = PhysReg{ctx.program->arg_sgpr_count},
+         .size = ctx.program->callee_abi.clobberedRegs.sgpr.lo() - ctx.program->arg_sgpr_count,
+      };
+      PhysRegInterval preserved_sgpr_hi = PhysRegInterval{
+         .lo_ = ctx.program->callee_abi.clobberedRegs.sgpr.hi(),
+         .size = PhysReg{ctx.sgpr_limit} - ctx.program->callee_abi.clobberedRegs.sgpr.hi(),
+      };
+      for (auto reg : preserved_sgpr_lo) {
+         blocked_sgpr.insert(reg);
+         adjust_max_used_regs(ctx, RegClass::s1, reg);
+      }
+      for (auto reg : preserved_sgpr_hi) {
+         blocked_sgpr.insert(reg);
+         adjust_max_used_regs(ctx, RegClass::s1, reg);
+      }
+   }
+
    for (Block& block : program->blocks) {
       ctx.block = &block;
 
       /* initialize register file */
       RegisterFile register_file = init_reg_file(ctx, live_out_per_block, block);
+      for (auto& reg : blocked_sgpr) {
+         if (register_file.is_empty_or_blocked(reg))
+            register_file.block(reg, s1);
+      }
       ctx.war_hint.reset();
       ctx.rr_vgpr_it = {PhysReg{256}};
       ctx.rr_sgpr_it = {PhysReg{0}};
@@ -3071,7 +3096,27 @@ register_allocation(Program* program, live& live_vars, ra_test_policy policy)
             instructions.emplace_back(std::move(instr));
             break;
          }
-         if (instr->opcode == aco_opcode::p_reload_preserved_vgpr && block.linear_succs.empty()) {
+         if (instr->opcode == aco_opcode::p_spill_preserved_sgpr) {
+            if (register_file.is_blocked(instr->operands[0].physReg()))
+               register_file.clear(instr->operands[0]);
+            blocked_sgpr.erase(instr->operands[0].physReg());
+            continue;
+         } else if (instr->opcode == aco_opcode::p_reload_preserved_sgpr) {
+            blocked_sgpr.insert(instr->operands[0].physReg());
+            std::vector<unsigned> vars = collect_vars(
+               ctx, register_file, {instr->operands[0].physReg(), instr->operands[0].size()});
+            register_file.block(instr->operands[0].physReg(), instr->operands[0].regClass());
+            ASSERTED bool success = false;
+            success = get_regs_for_copies(ctx, register_file, parallelcopy, vars, instr,
+                                          PhysRegInterval{});
+            assert(success);
+
+            update_renames(ctx, register_file, parallelcopy, instr, (UpdateRenames)0);
+            register_file.block(instr->operands[0].physReg(), instr->operands[0].regClass());
+            emit_parallel_copy(ctx, parallelcopy, instr, instructions, temp_in_scc, register_file);
+            continue;
+         } else if (instr->opcode == aco_opcode::p_reload_preserved_vgpr &&
+                    block.linear_succs.empty()) {
             PhysRegInterval preserved_vgpr_lo = PhysRegInterval{
                .lo_ = PhysReg{256u + ctx.program->arg_vgpr_count},
                .size = ctx.program->callee_abi.clobberedRegs.vgpr.lo() - 256u -
diff --git a/src/amd/compiler/aco_scheduler.cpp b/src/amd/compiler/aco_scheduler.cpp
index dece563906e84..19560eaf3e932 100644
--- a/src/amd/compiler/aco_scheduler.cpp
+++ b/src/amd/compiler/aco_scheduler.cpp
@@ -1276,6 +1276,14 @@ schedule_program(Program* program, live& live_vars)
    assert(ctx.num_waves > 0);
    ctx.mv.max_registers = {int16_t(get_addr_vgpr_from_waves(program, ctx.num_waves * wave_fac) - 2),
                            int16_t(get_addr_sgpr_from_waves(program, ctx.num_waves * wave_fac))};
+   /* If not all preserved SGPRs in callee shaders were spilled, don't try using them for
+    * scheduling.
+    */
+   if (program->is_callee) {
+      ctx.mv.max_registers.sgpr =
+         std::max(std::min(ctx.mv.max_registers.sgpr, program->cur_reg_demand.sgpr),
+                  (int16_t)program->callee_abi.clobberedRegs.sgpr.size);
+   }
 
    /* NGG culling shaders are very sensitive to position export scheduling.
     * Schedule less aggressively when early primitive export is used, and
diff --git a/src/amd/compiler/aco_spill.cpp b/src/amd/compiler/aco_spill.cpp
index a728de9474338..6ac3b9d398ea8 100644
--- a/src/amd/compiler/aco_spill.cpp
+++ b/src/amd/compiler/aco_spill.cpp
@@ -75,6 +75,8 @@ struct spill_ctx {
    std::vector<aco::unordered_map<Temp, uint32_t>> spills_entry;
    std::vector<aco::unordered_map<Temp, uint32_t>> spills_exit;
 
+   std::vector<uint32_t> preserved_spill_ids;
+
    std::vector<bool> processed;
    std::vector<loop_info> loop;
 
@@ -138,11 +140,27 @@ struct spill_ctx {
          for (auto pair : loop.back().spills)
             add_interference(spill_id, pair.second);
       }
+      for (auto id : preserved_spill_ids)
+         add_interference(spill_id, id);
 
       spills[to_spill] = spill_id;
       return spill_id;
    }
 
+   uint32_t add_preserved_spill(RegClass rc,
+                                std::vector<aco::unordered_map<Temp, uint32_t>>& block_spills)
+   {
+      const uint32_t spill_id = allocate_spill_id(rc);
+      for (auto& spills : block_spills)
+         for (auto pair : spills)
+            add_interference(spill_id, pair.second);
+      for (auto id : preserved_spill_ids)
+         add_interference(spill_id, id);
+      preserved_spill_ids.push_back(spill_id);
+
+      return spill_id;
+   }
+
    void add_interference(uint32_t first, uint32_t second)
    {
       if (interferences[first].first.type() != interferences[second].first.type())
@@ -1491,6 +1509,8 @@ end_unused_spill_vgprs(spill_ctx& ctx, Block& block, std::vector<Temp>& vgpr_spi
       if (pair.first.type() == RegType::sgpr && ctx.is_reloaded[pair.second])
          is_used[slots[pair.second] / ctx.wave_size] = true;
    }
+   for (auto preserved : ctx.preserved_spill_ids)
+      is_used[slots[preserved] / ctx.wave_size] = true;
 
    std::vector<Temp> temps;
    for (unsigned i = 0; i < vgpr_spill_temps.size(); i++) {
@@ -1668,6 +1688,13 @@ assign_spill_slots(spill_ctx& ctx, unsigned spills_to_vgpr)
                   }
                }
 
+               if (!(*it)->definitions[0].isTemp()) {
+                  auto id_it = std::find(ctx.preserved_spill_ids.begin(),
+                                         ctx.preserved_spill_ids.end(), spill_id);
+                  assert(id_it != ctx.preserved_spill_ids.end());
+                  ctx.preserved_spill_ids.erase(id_it);
+               }
+
                /* reload sgpr: just add the vgpr temp to operands */
                Instruction* reload = create_instruction(aco_opcode::p_reload, Format::PSEUDO, 2, 1);
                reload->operands[0] = Operand(vgpr_spill_temps[spill_slot / ctx.wave_size]);
@@ -1687,6 +1714,37 @@ assign_spill_slots(spill_ctx& ctx, unsigned spills_to_vgpr)
    ctx.program->config->scratch_bytes_per_wave += ctx.vgpr_spill_slots * 4 * ctx.program->wave_size;
 }
 
+void
+spill_reload_preserved_sgpr(spill_ctx& ctx, std::vector<aco_ptr<Instruction>>& spill_instructions,
+                            std::vector<aco_ptr<Instruction>>& reload_instructions, PhysReg reg)
+{
+   uint32_t spill_id = ctx.add_preserved_spill(RegClass::s1, ctx.spills_exit);
+
+   aco_ptr<Instruction> spill{create_instruction(aco_opcode::p_spill, Format::PSEUDO, 2, 0)};
+   spill->operands[0] = Operand(reg, RegClass::s1);
+   spill->operands[1] = Operand::c32(spill_id);
+
+   aco_ptr<Instruction> unblock{
+      create_instruction(aco_opcode::p_spill_preserved_sgpr, Format::PSEUDO, 1, 0)};
+   unblock->operands[0] = Operand(reg, RegClass::s1);
+
+   spill_instructions.emplace_back(std::move(spill));
+   spill_instructions.emplace_back(std::move(unblock));
+
+   aco_ptr<Instruction> block{
+      create_instruction(aco_opcode::p_reload_preserved_sgpr, Format::PSEUDO, 1, 0)};
+   block->operands[0] = Operand(reg, RegClass::s1);
+
+   aco_ptr<Instruction> reload{create_instruction(aco_opcode::p_reload, Format::PSEUDO, 1, 1)};
+   reload->operands[0] = Operand::c32(spill_id);
+   reload->definitions[0] = Definition(reg, RegClass::s1);
+
+   reload_instructions.emplace_back(std::move(block));
+   reload_instructions.emplace_back(std::move(reload));
+
+   ctx.is_reloaded[spill_id] = true;
+}
+
 } /* end namespace */
 
 void
@@ -1697,8 +1755,16 @@ spill(Program* program, live& live_vars)
 
    program->progress = CompilationProgress::after_spilling;
 
+   const uint16_t sgpr_limit = get_addr_sgpr_from_waves(program, program->min_waves);
+   const uint16_t vgpr_limit = get_addr_vgpr_from_waves(program, program->min_waves);
+   uint16_t abi_sgpr_limit =
+      std::min((uint16_t)(program->callee_abi.clobberedRegs.sgpr.size + program->arg_sgpr_count),
+               sgpr_limit);
+   if (!program->is_callee)
+      abi_sgpr_limit = sgpr_limit;
+
    /* no spilling when register pressure is low enough */
-   if (program->num_waves > 0)
+   if (program->num_waves > 0 && program->cur_reg_demand.sgpr <= abi_sgpr_limit)
       return;
 
    /* lower to CSSA before spilling to ensure correctness w.r.t. phis */
@@ -1706,14 +1772,12 @@ spill(Program* program, live& live_vars)
 
    /* calculate target register demand */
    const RegisterDemand demand = program->max_reg_demand; /* current max */
-   const uint16_t sgpr_limit = get_addr_sgpr_from_waves(program, program->min_waves);
-   const uint16_t vgpr_limit = get_addr_vgpr_from_waves(program, program->min_waves);
    uint16_t extra_vgprs = 0;
    uint16_t extra_sgprs = 0;
 
    /* calculate extra VGPRs required for spilling SGPRs */
-   if (demand.sgpr > sgpr_limit) {
-      unsigned sgpr_spills = demand.sgpr - sgpr_limit;
+   if (demand.sgpr > abi_sgpr_limit) {
+      unsigned sgpr_spills = demand.sgpr - abi_sgpr_limit;
       extra_vgprs = DIV_ROUND_UP(sgpr_spills * 2, program->wave_size) + 1;
    }
    /* add extra SGPRs required for spilling VGPRs */
@@ -1722,9 +1786,9 @@ spill(Program* program, live& live_vars)
          extra_sgprs = 1; /* SADDR */
       else
          extra_sgprs = 5; /* scratch_resource (s4) + scratch_offset (s1) */
-      if (demand.sgpr + extra_sgprs > sgpr_limit) {
+      if (demand.sgpr + extra_sgprs > abi_sgpr_limit) {
          /* re-calculate in case something has changed */
-         unsigned sgpr_spills = demand.sgpr + extra_sgprs - sgpr_limit;
+         unsigned sgpr_spills = demand.sgpr + extra_sgprs - abi_sgpr_limit;
          extra_vgprs = DIV_ROUND_UP(sgpr_spills * 2, program->wave_size) + 1;
       }
    }
@@ -1736,10 +1800,51 @@ spill(Program* program, live& live_vars)
    gather_ssa_use_info(ctx);
    get_rematerialize_info(ctx);
 
+   /* Prepare spilling of preserved SGPRs. Don't insert the instructions yet so live info
+    * stays valid.
+    */
+   std::vector<aco_ptr<Instruction>> preserved_spill_instructions;
+   std::vector<aco_ptr<Instruction>> preserved_reload_instructions;
+   if (demand.sgpr > abi_sgpr_limit && ctx.program->is_callee) {
+      ctx.preserved_spill_ids.reserve(demand.sgpr - abi_sgpr_limit);
+
+      for (PhysReg reg = PhysReg{program->arg_sgpr_count};
+           reg < program->callee_abi.clobberedRegs.sgpr.lo(); reg = reg.advance(4))
+         spill_reload_preserved_sgpr(ctx, preserved_spill_instructions,
+                                     preserved_reload_instructions, reg);
+
+      unsigned max_reg =
+         std::min((unsigned)program->cur_reg_demand.sgpr + extra_sgprs, (unsigned)sgpr_limit);
+      for (PhysReg reg = program->callee_abi.clobberedRegs.sgpr.hi(); reg < max_reg;
+           reg = reg.advance(4))
+         spill_reload_preserved_sgpr(ctx, preserved_spill_instructions,
+                                     preserved_reload_instructions, reg);
+   }
+
    /* create spills and reloads */
    for (unsigned i = 0; i < program->blocks.size(); i++)
       spill_block(ctx, i);
 
+   if (!preserved_spill_instructions.empty()) {
+      auto spill_insert_point = std::find_if(
+         program->blocks.front().instructions.begin(), program->blocks.front().instructions.end(),
+         [](const auto& instr) { return instr->opcode == aco_opcode::p_spill_preserved_vgpr; });
+      assert(spill_insert_point != program->blocks.front().instructions.end());
+
+      spill_insert_point = std::next(spill_insert_point);
+      program->blocks.front().instructions.insert(
+         spill_insert_point, std::move_iterator(preserved_spill_instructions.begin()),
+         std::move_iterator(preserved_spill_instructions.end()));
+
+      auto reload_insert_point = std::find_if(
+         program->blocks.back().instructions.begin(), program->blocks.back().instructions.end(),
+         [](const auto& instr) { return instr->opcode == aco_opcode::p_reload_preserved_vgpr; });
+      assert(reload_insert_point != program->blocks.back().instructions.end());
+      program->blocks.back().instructions.insert(
+         reload_insert_point, std::move_iterator(preserved_reload_instructions.begin()),
+         std::move_iterator(preserved_reload_instructions.end()));
+   }
+
    /* assign spill slots and DCE rematerialized code */
    assign_spill_slots(ctx, extra_vgprs);
 
-- 
GitLab


From 1bca9ecf60e246de318b3b66582752408687993f Mon Sep 17 00:00:00 2001
From: Friedrich Vock <friedrich.vock@gmx.de>
Date: Mon, 13 May 2024 06:29:40 +0200
Subject: [PATCH 58/71] aco/ra: Also consider blocked registers as not
 containing temps

---
 src/amd/compiler/aco_register_allocation.cpp | 12 ++++++++++--
 1 file changed, 10 insertions(+), 2 deletions(-)

diff --git a/src/amd/compiler/aco_register_allocation.cpp b/src/amd/compiler/aco_register_allocation.cpp
index 9caf2edebff1d..76df2fda00acb 100644
--- a/src/amd/compiler/aco_register_allocation.cpp
+++ b/src/amd/compiler/aco_register_allocation.cpp
@@ -217,6 +217,14 @@ public:
       return res;
    }
 
+   unsigned count_zero_or_blocked(PhysRegInterval reg_interval) const
+   {
+      unsigned res = 0;
+      for (PhysReg reg : reg_interval)
+         res += !regs[reg] || regs[reg] == 0xFFFFFFFF;
+      return res;
+   }
+
    /* Returns true if any of the bytes in the given range are allocated or blocked */
    bool test(PhysReg start, unsigned num_bytes) const
    {
@@ -3507,8 +3515,8 @@ register_allocation(Program* program, live& live_vars, ra_test_policy policy)
 
          ASSERTED PhysRegInterval vgpr_bounds = get_reg_bounds(ctx, RegType::vgpr, false);
          ASSERTED PhysRegInterval sgpr_bounds = get_reg_bounds(ctx, RegType::sgpr, false);
-         assert(register_file.count_zero(vgpr_bounds) == ctx.vgpr_bounds);
-         assert(register_file.count_zero(sgpr_bounds) == ctx.sgpr_bounds);
+         assert(register_file.count_zero_or_blocked(vgpr_bounds) == ctx.vgpr_bounds);
+         assert(register_file.count_zero_or_blocked(sgpr_bounds) == ctx.sgpr_bounds);
       } else if (should_compact_linear_vgprs(ctx, live_vars, register_file)) {
          aco_ptr<Instruction> br = std::move(instructions.back());
          instructions.pop_back();
-- 
GitLab


From cf90fc7a434bed0ed4e2c75b120a844aefc3c97c Mon Sep 17 00:00:00 2001
From: Friedrich Vock <friedrich.vock@gmx.de>
Date: Mon, 13 May 2024 06:30:35 +0200
Subject: [PATCH 59/71] aco/ra: Skip blocked regs in get_reg_impl

---
 src/amd/compiler/aco_register_allocation.cpp | 2 +-
 1 file changed, 1 insertion(+), 1 deletion(-)

diff --git a/src/amd/compiler/aco_register_allocation.cpp b/src/amd/compiler/aco_register_allocation.cpp
index 76df2fda00acb..1eb87f8e78fdb 100644
--- a/src/amd/compiler/aco_register_allocation.cpp
+++ b/src/amd/compiler/aco_register_allocation.cpp
@@ -1303,7 +1303,7 @@ get_reg_impl(ra_ctx& ctx, const RegisterFile& reg_file, std::vector<parallelcopy
             continue;
          }
 
-         if (reg_file[j] == 0 || reg_file[j] == last_var)
+         if (reg_file[j] == 0 || reg_file[j] == 0xFFFFFFFF || reg_file[j] == last_var)
             continue;
 
          if (reg_file[j] == 0xF0000000) {
-- 
GitLab


From a453b3c15b64cc168a2fde72ae4916abafc109b2 Mon Sep 17 00:00:00 2001
From: Friedrich Vock <friedrich.vock@gmx.de>
Date: Mon, 13 May 2024 06:31:01 +0200
Subject: [PATCH 60/71] aco/isel: Bypass reg preservation for noreturn shaders

---
 src/amd/compiler/aco_instruction_selection.cpp | 1 +
 src/amd/compiler/aco_ir.h                      | 1 +
 2 files changed, 2 insertions(+)

diff --git a/src/amd/compiler/aco_instruction_selection.cpp b/src/amd/compiler/aco_instruction_selection.cpp
index d7770973b83d5..dc98fa3f322c3 100644
--- a/src/amd/compiler/aco_instruction_selection.cpp
+++ b/src/amd/compiler/aco_instruction_selection.cpp
@@ -11996,6 +11996,7 @@ select_program_rt(isel_context& ctx, unsigned shader_count, struct nir_shader* c
 
             Builder(ctx.program, ctx.block).sop1(aco_opcode::s_setpc_b64, Operand(ctx.next_pc));
          } else {
+            ctx.program->bypass_reg_preservation = true;
             Builder(ctx.program, ctx.block).sopp(aco_opcode::s_endpgm);
          }
 
diff --git a/src/amd/compiler/aco_ir.h b/src/amd/compiler/aco_ir.h
index cfca9425df658..0394d96fa289b 100644
--- a/src/amd/compiler/aco_ir.h
+++ b/src/amd/compiler/aco_ir.h
@@ -2334,6 +2334,7 @@ public:
    bool pending_lds_access = false;
 
    bool is_callee = false;
+   bool bypass_reg_preservation = false;
    ABI callee_abi = {};
    unsigned short arg_sgpr_count;
    unsigned short arg_vgpr_count;
-- 
GitLab


From bd37c717f05f1410aa7aa1e3a6c666995a8dd3e7 Mon Sep 17 00:00:00 2001
From: Friedrich Vock <friedrich.vock@gmx.de>
Date: Fri, 31 May 2024 16:46:28 +0200
Subject: [PATCH 61/71] aco/ra: Add separate counter for blocked registers

We can't assume blocked registers are free in get_reg_impl, but
we don't want to pessimize register usage estimations either.
---
 src/amd/compiler/aco_register_allocation.cpp | 25 ++++++++++++++++----
 1 file changed, 21 insertions(+), 4 deletions(-)

diff --git a/src/amd/compiler/aco_register_allocation.cpp b/src/amd/compiler/aco_register_allocation.cpp
index 1eb87f8e78fdb..70b9056e703d9 100644
--- a/src/amd/compiler/aco_register_allocation.cpp
+++ b/src/amd/compiler/aco_register_allocation.cpp
@@ -82,6 +82,8 @@ struct ra_ctx {
    aco_ptr<Instruction> phi_dummy;
    uint16_t max_used_sgpr = 0;
    uint16_t max_used_vgpr = 0;
+   uint16_t max_blocked_sgpr = 0;
+   uint16_t max_blocked_vgpr = 0;
    uint16_t sgpr_limit;
    uint16_t vgpr_limit;
    std::bitset<512> war_hint;
@@ -735,6 +737,21 @@ adjust_max_used_regs(ra_ctx& ctx, RegClass rc, unsigned reg)
    }
 }
 
+void
+adjust_max_blocked_regs(ra_ctx& ctx, RegType type, unsigned reg)
+{
+   uint16_t max_addressible_sgpr = ctx.sgpr_limit;
+   if (type == RegType::vgpr) {
+      assert(reg >= 256);
+      uint16_t hi = reg - 256 - 1;
+      assert(hi <= 255);
+      ctx.max_blocked_vgpr = std::max(ctx.max_blocked_vgpr, hi);
+   } else if (reg <= max_addressible_sgpr) {
+      uint16_t hi = reg - 1;
+      ctx.max_blocked_sgpr = std::max(ctx.max_blocked_sgpr, std::min(hi, max_addressible_sgpr));
+   }
+}
+
 enum UpdateRenames {
    rename_not_killed_ops = 0x1,
    fill_killed_ops = 0x2,
@@ -3259,10 +3276,10 @@ register_allocation(Program* program, live& live_vars, ra_test_policy policy)
             tmp_file.block(instr->call().abi.clobberedRegs.sgpr);
             tmp_file.block(instr->call().abi.clobberedRegs.vgpr);
 
-            adjust_max_used_regs(ctx, RegClass::s1,
-                                 instr->call().abi.clobberedRegs.sgpr.hi().reg() - 1);
-            adjust_max_used_regs(ctx, RegClass::v1,
-                                 instr->call().abi.clobberedRegs.vgpr.hi().reg() - 1);
+            adjust_max_blocked_regs(ctx, RegType::sgpr,
+                                    instr->call().abi.clobberedRegs.sgpr.hi().reg());
+            adjust_max_blocked_regs(ctx, RegType::vgpr,
+                                    instr->call().abi.clobberedRegs.vgpr.hi().reg());
 
             ASSERTED bool success = false;
             success =
-- 
GitLab


From 6dd7b345871be760e2aa71ebe45f9f23167226fb Mon Sep 17 00:00:00 2001
From: Friedrich Vock <friedrich.vock@gmx.de>
Date: Sat, 1 Jun 2024 11:50:04 +0200
Subject: [PATCH 62/71] aco/spill: Don't spill scratch_rsrc-related temps

These temps are used to create the scratch_rsrc. Spilling them will
never benefit anything, because assign_spill_slots will insert code
that keeps them live. Since the spiller assumes all spilled variables
to be dead, this can cause more variables being live than intended and
spilling to fail.
---
 src/amd/compiler/aco_spill.cpp | 9 ++++++++-
 1 file changed, 8 insertions(+), 1 deletion(-)

diff --git a/src/amd/compiler/aco_spill.cpp b/src/amd/compiler/aco_spill.cpp
index 6ac3b9d398ea8..cb90c4c193e80 100644
--- a/src/amd/compiler/aco_spill.cpp
+++ b/src/amd/compiler/aco_spill.cpp
@@ -426,6 +426,9 @@ init_live_in_vars(spill_ctx& ctx, Block* block, unsigned block_idx)
             if (var.type() != type || ctx.spills_entry[block_idx].count(var) ||
                 !ctx.live_vars.live_out[block_idx - 1].count(t) || var.regClass().is_linear_vgpr())
                continue;
+            if (var == ctx.program->stack_ptr || var == ctx.program->scratch_offset ||
+                var == ctx.program->private_segment_buffer)
+               continue;
 
             unsigned can_remat = ctx.remat.count(var);
             if (can_remat > remat || (can_remat == remat && ctx.ssa_infos[t].score() > score)) {
@@ -466,7 +469,8 @@ init_live_in_vars(spill_ctx& ctx, Block* block, unsigned block_idx)
          for (unsigned t : live_in) {
             Temp var = Temp(t, ctx.program->temp_rc[t]);
             if (var.type() == type && !ctx.spills_entry[block_idx].count(var) &&
-                ctx.ssa_infos[t].score() > score) {
+                ctx.ssa_infos[t].score() > score && var != ctx.program->stack_ptr &&
+                var != ctx.program->scratch_offset && var != ctx.program->private_segment_buffer) {
                to_spill = var;
                score = ctx.ssa_infos[t].score();
             }
@@ -1065,6 +1069,9 @@ process_block(spill_ctx& ctx, unsigned block_idx, Block* block, RegisterDemand s
                if (can_rematerialize > do_rematerialize || loop_variable > avoid_respill ||
                    avoids_operand > avoid_operand || ctx.ssa_infos[t].score() > score) {
                   bool is_spilled_operand = !avoids_operand && reloads.count(var);
+                  if (var == ctx.program->stack_ptr || var == ctx.program->scratch_offset ||
+                      var == ctx.program->private_segment_buffer)
+                     continue;
 
                   to_spill = var;
                   score = ctx.ssa_infos[t].score();
-- 
GitLab


From 0ff2ad3b7a6943a418c67e9380380f29d421b12e Mon Sep 17 00:00:00 2001
From: Friedrich Vock <friedrich.vock@gmx.de>
Date: Wed, 5 Jun 2024 11:06:32 +0200
Subject: [PATCH 63/71] aco/spill: Ignore extra VGPRs/SGPRs for calls

For VGPRs, we make sure they're spilled in the spill_preserved pass.
For SGPRs, we make sure to reinitialize scratch_rsrc after calls.
---
 src/amd/compiler/aco_spill.cpp | 21 ++++++++++++++++-----
 1 file changed, 16 insertions(+), 5 deletions(-)

diff --git a/src/amd/compiler/aco_spill.cpp b/src/amd/compiler/aco_spill.cpp
index cb90c4c193e80..08c4dc7063e75 100644
--- a/src/amd/compiler/aco_spill.cpp
+++ b/src/amd/compiler/aco_spill.cpp
@@ -88,17 +88,21 @@ struct spill_ctx {
    std::set<Instruction*> unused_remats;
    unsigned wave_size;
 
+   RegisterDemand extra_demand;
+
    unsigned sgpr_spill_slots;
    unsigned vgpr_spill_slots;
    Temp scratch_rsrc;
 
-   spill_ctx(const RegisterDemand target_pressure_, Program* program_, live& live_vars_)
+   spill_ctx(const RegisterDemand target_pressure_, RegisterDemand extra_demand_, Program* program_,
+             live& live_vars_)
        : target_pressure(target_pressure_), program(program_), memory(), live_vars(live_vars_),
          renames(program->blocks.size(), aco::map<Temp, Temp>(memory)),
          spills_entry(program->blocks.size(), aco::unordered_map<Temp, uint32_t>(memory)),
          spills_exit(program->blocks.size(), aco::unordered_map<Temp, uint32_t>(memory)),
          processed(program->blocks.size(), false), ssa_infos(program->peekAllocationId()),
-         remat(memory), wave_size(program->wave_size), sgpr_spill_slots(0), vgpr_spill_slots(0)
+         remat(memory), wave_size(program->wave_size), extra_demand(extra_demand_),
+         sgpr_spill_slots(0), vgpr_spill_slots(0)
    {}
 
    void add_affinity(uint32_t first, uint32_t second)
@@ -1026,8 +1030,14 @@ process_block(spill_ctx& ctx, unsigned block_idx, Block* block, RegisterDemand s
          RegisterDemand new_demand = ctx.live_vars.register_demand[block_idx][idx];
          new_demand.update(demand_before);
 
+         RegisterDemand ignored_regs = {};
+
+         /* We spill linear VGPRs for calls in spill_preserved */
+         if (instr->isCall() || (!instructions.empty() && instructions.back()->isCall()))
+            ignored_regs += ctx.extra_demand;
+
          /* if reg pressure is too high, spill variable with furthest next use */
-         while ((new_demand - spilled_registers).exceeds(ctx.target_pressure)) {
+         while ((new_demand - spilled_registers).exceeds(ctx.target_pressure + ignored_regs)) {
             float score = 0.0;
             Temp to_spill;
             unsigned operand_idx = -1u;
@@ -1038,7 +1048,8 @@ process_block(spill_ctx& ctx, unsigned block_idx, Block* block, RegisterDemand s
             unsigned avoid_operand = 0;
 
             RegType type = RegType::sgpr;
-            if (new_demand.vgpr - spilled_registers.vgpr > ctx.target_pressure.vgpr)
+            if (new_demand.vgpr - spilled_registers.vgpr >
+                (ctx.target_pressure.vgpr + ignored_regs.vgpr))
                type = RegType::vgpr;
 
             for (unsigned t : ctx.live_vars.live_out[block_idx]) {
@@ -1803,7 +1814,7 @@ spill(Program* program, live& live_vars)
    const RegisterDemand target(vgpr_limit - extra_vgprs, sgpr_limit - extra_sgprs);
 
    /* initialize ctx */
-   spill_ctx ctx(target, program, live_vars);
+   spill_ctx ctx(target, RegisterDemand(extra_vgprs, extra_sgprs), program, live_vars);
    gather_ssa_use_info(ctx);
    get_rematerialize_info(ctx);
 
-- 
GitLab


From b8c9f0e74a5c0905feff0243b5624f1bad3c36f9 Mon Sep 17 00:00:00 2001
From: Friedrich Vock <friedrich.vock@gmx.de>
Date: Wed, 5 Jun 2024 11:08:23 +0200
Subject: [PATCH 64/71] HACK: aco/spill: Don't call get_demand_before after
 call instructions

---
 src/amd/compiler/aco_spill.cpp | 3 ++-
 1 file changed, 2 insertions(+), 1 deletion(-)

diff --git a/src/amd/compiler/aco_spill.cpp b/src/amd/compiler/aco_spill.cpp
index 08c4dc7063e75..3a5ec0ff6ffb1 100644
--- a/src/amd/compiler/aco_spill.cpp
+++ b/src/amd/compiler/aco_spill.cpp
@@ -1028,7 +1028,8 @@ process_block(spill_ctx& ctx, unsigned block_idx, Block* block, RegisterDemand s
 
          RegisterDemand demand_before = get_demand_before(ctx, block_idx, idx);
          RegisterDemand new_demand = ctx.live_vars.register_demand[block_idx][idx];
-         new_demand.update(demand_before);
+         if (instructions.empty() || !instructions.back()->isCall())
+            new_demand.update(demand_before);
 
          RegisterDemand ignored_regs = {};
 
-- 
GitLab


From d9b8d70e89025ceb795823b44cc3d39112e2a12e Mon Sep 17 00:00:00 2001
From: Friedrich Vock <friedrich.vock@gmx.de>
Date: Sat, 1 Jun 2024 16:38:24 +0200
Subject: [PATCH 65/71] aco: Add and set block->contains_call

---
 src/amd/compiler/aco_instruction_selection.cpp | 1 +
 src/amd/compiler/aco_ir.h                      | 1 +
 2 files changed, 2 insertions(+)

diff --git a/src/amd/compiler/aco_instruction_selection.cpp b/src/amd/compiler/aco_instruction_selection.cpp
index dc98fa3f322c3..b0bdd119307a8 100644
--- a/src/amd/compiler/aco_instruction_selection.cpp
+++ b/src/amd/compiler/aco_instruction_selection.cpp
@@ -10610,6 +10610,7 @@ visit_call(isel_context* ctx, nir_call_instr* instr)
    info.return_info = std::move(return_infos);
    info.scratch_param_size = scratch_byte_offset;
    ctx->call_infos.push_back(info);
+   ctx->block->contains_call = true;
 }
 
 void
diff --git a/src/amd/compiler/aco_ir.h b/src/amd/compiler/aco_ir.h
index 0394d96fa289b..40b7e38e92891 100644
--- a/src/amd/compiler/aco_ir.h
+++ b/src/amd/compiler/aco_ir.h
@@ -2165,6 +2165,7 @@ struct Block {
    /* this information is needed for predecessors to blocks with phis when
     * moving out of ssa */
    bool scc_live_out = false;
+   bool contains_call = true;
 
    Block() : index(0) {}
 };
-- 
GitLab


From 19ee3953d3918aff6cb78cfe5438d88691275e3e Mon Sep 17 00:00:00 2001
From: Friedrich Vock <friedrich.vock@gmx.de>
Date: Sat, 1 Jun 2024 12:00:48 +0200
Subject: [PATCH 66/71] aco/spill: Reset scratch_rsrc on calls

---
 src/amd/compiler/aco_spill.cpp | 46 ++++++++++++++++++++++++++++------
 1 file changed, 39 insertions(+), 7 deletions(-)

diff --git a/src/amd/compiler/aco_spill.cpp b/src/amd/compiler/aco_spill.cpp
index 3a5ec0ff6ffb1..539858bff8208 100644
--- a/src/amd/compiler/aco_spill.cpp
+++ b/src/amd/compiler/aco_spill.cpp
@@ -93,6 +93,7 @@ struct spill_ctx {
    unsigned sgpr_spill_slots;
    unsigned vgpr_spill_slots;
    Temp scratch_rsrc;
+   unsigned scratch_rsrc_block = -1u;
 
    spill_ctx(const RegisterDemand target_pressure_, RegisterDemand extra_demand_, Program* program_,
              live& live_vars_)
@@ -1277,19 +1278,28 @@ setup_vgpr_spill_reload(spill_ctx& ctx, Block& block,
    bool overflow = (ctx.vgpr_spill_slots - 1) * 4 > offset_range;
 
    Builder rsrc_bld(ctx.program);
+   unsigned bld_block = block.index;
    if (block.kind & block_kind_top_level) {
       rsrc_bld.reset(&instructions);
    } else if (ctx.scratch_rsrc == Temp() && (!overflow || ctx.program->gfx_level < GFX9)) {
       Block* tl_block = &block;
-      while (!(tl_block->kind & block_kind_top_level))
+      while (!(tl_block->kind & block_kind_top_level) &&
+             std::find_if(tl_block->instructions.begin(), tl_block->instructions.end(),
+                          [](auto& instr)
+                          { return !instr || instr->isCall(); }) == tl_block->instructions.end())
          tl_block = &ctx.program->blocks[tl_block->linear_idom];
 
       /* find p_logical_end */
-      std::vector<aco_ptr<Instruction>>& prev_instructions = tl_block->instructions;
-      unsigned idx = prev_instructions.size() - 1;
-      while (prev_instructions[idx]->opcode != aco_opcode::p_logical_end)
-         idx--;
-      rsrc_bld.reset(&prev_instructions, std::next(prev_instructions.begin(), idx));
+      if (tl_block->kind & block_kind_top_level) {
+         std::vector<aco_ptr<Instruction>>& prev_instructions = tl_block->instructions;
+         unsigned idx = prev_instructions.size() - 1;
+         while (prev_instructions[idx]->opcode != aco_opcode::p_logical_end)
+            idx--;
+         rsrc_bld.reset(&prev_instructions, std::next(prev_instructions.begin(), idx));
+         bld_block = tl_block->index;
+      } else {
+         rsrc_bld.reset(&instructions);
+      }
    }
 
    /* If spilling overflows the constant offset range at any point, we need to emit the soffset
@@ -1317,10 +1327,13 @@ setup_vgpr_spill_reload(spill_ctx& ctx, Block& block,
                                Operand(ctx.program->stack_ptr), Operand::c32(saddr));
          else
             ctx.scratch_rsrc = offset_bld.copy(offset_bld.def(s1), Operand::c32(saddr));
+         ctx.scratch_rsrc_block = bld_block;
       }
    } else {
-      if (ctx.scratch_rsrc == Temp())
+      if (ctx.scratch_rsrc == Temp()) {
          ctx.scratch_rsrc = load_scratch_resource(ctx.program, rsrc_bld, overflow, true);
+         ctx.scratch_rsrc_block = bld_block;
+      }
 
       if (overflow) {
          uint32_t soffset =
@@ -1604,6 +1617,22 @@ assign_spill_slots(spill_ctx& ctx, unsigned spills_to_vgpr)
    unsigned last_top_level_block_idx = 0;
    for (Block& block : ctx.program->blocks) {
 
+      if (ctx.scratch_rsrc_block < ctx.program->blocks.size() &&
+          !(ctx.program->blocks[ctx.scratch_rsrc_block].kind & block_kind_top_level))
+         ctx.scratch_rsrc = Temp();
+
+      if (block.kind & block_kind_loop_header) {
+         for (unsigned index = block.index;
+              index < ctx.program->blocks.size() &&
+              ctx.program->blocks[index].loop_nest_depth >= block.loop_nest_depth;
+              ++index) {
+            if (ctx.program->blocks[index].contains_call) {
+               ctx.scratch_rsrc = Temp();
+               break;
+            }
+         }
+      }
+
       if (block.kind & block_kind_top_level) {
          last_top_level_block_idx = block.index;
 
@@ -1621,6 +1650,9 @@ assign_spill_slots(spill_ctx& ctx, unsigned spills_to_vgpr)
       Builder bld(ctx.program, &instructions);
       for (it = block.instructions.begin(); it != block.instructions.end(); ++it) {
 
+         if ((*it)->isCall())
+            ctx.scratch_rsrc = Temp();
+
          if ((*it)->opcode == aco_opcode::p_spill) {
             uint32_t spill_id = (*it)->operands[1].constantValue();
 
-- 
GitLab


From 46a9316f1ea7eefb611286eefc26f5859d1c78b6 Mon Sep 17 00:00:00 2001
From: Friedrich Vock <friedrich.vock@gmx.de>
Date: Thu, 6 Jun 2024 08:08:02 +0200
Subject: [PATCH 67/71] radv: Re-enable RT pipelines

---
 src/amd/vulkan/radv_physical_device.c | 4 ----
 1 file changed, 4 deletions(-)

diff --git a/src/amd/vulkan/radv_physical_device.c b/src/amd/vulkan/radv_physical_device.c
index 572dc61e68342..d4708b93bcea7 100644
--- a/src/amd/vulkan/radv_physical_device.c
+++ b/src/amd/vulkan/radv_physical_device.c
@@ -102,10 +102,6 @@ radv_calibrated_timestamps_enabled(const struct radv_physical_device *pdev)
 bool
 radv_enable_rt(const struct radv_physical_device *pdev, bool rt_pipelines)
 {
-   /* Temporarily under construction! */
-   if (rt_pipelines)
-      return false;
-
    if (pdev->info.gfx_level < GFX10_3 && !radv_emulate_rt(pdev))
       return false;
 
-- 
GitLab


From 556dd571d408f47c00e9ceffc7fe09c9f15c03d6 Mon Sep 17 00:00:00 2001
From: Friedrich Vock <friedrich.vock@gmx.de>
Date: Thu, 6 Jun 2024 08:03:36 +0200
Subject: [PATCH 68/71] aco: Make get_blocked_abi_demand public

---
 src/amd/compiler/aco_ir.h | 2 ++
 1 file changed, 2 insertions(+)

diff --git a/src/amd/compiler/aco_ir.h b/src/amd/compiler/aco_ir.h
index 40b7e38e92891..d92a781d8cb97 100644
--- a/src/amd/compiler/aco_ir.h
+++ b/src/amd/compiler/aco_ir.h
@@ -2513,6 +2513,8 @@ RegisterDemand get_temp_registers(Program *program, Block *block, aco_ptr<Instru
 RegisterDemand get_demand_before(Program *program, Block *block, RegisterDemand demand,
                                  aco_ptr<Instruction>& instr, aco_ptr<Instruction>& instr_before,
                                  const IDSet& live_out);
+RegisterDemand get_blocked_abi_demand(Program* program, Block* block, const Pseudo_call_instruction* instr,
+                                      const IDSet& live);
 
 /* number of sgprs that need to be allocated but might notbe addressable as s0-s105 */
 uint16_t get_extra_sgprs(Program* program);
-- 
GitLab


From 5eb1b979b852647aeccf5a6750eac2d8fe06c9d8 Mon Sep 17 00:00:00 2001
From: Friedrich Vock <friedrich.vock@gmx.de>
Date: Thu, 6 Jun 2024 08:03:43 +0200
Subject: [PATCH 69/71] aco: Add separate register usage tracking for
 ABI-preserved regs

If a shader uses fewer registers than are preserved by an ABI, we'll
want to set the register demand to the actual register usage instead of
the demand set by preserved call registers.
---
 src/amd/compiler/aco_ir.h                    |  3 +-
 src/amd/compiler/aco_live_var_analysis.cpp   | 34 ++++++++++++++++----
 src/amd/compiler/aco_register_allocation.cpp | 10 +++---
 src/amd/compiler/aco_scheduler.cpp           | 26 ++++++++++++++-
 4 files changed, 61 insertions(+), 12 deletions(-)

diff --git a/src/amd/compiler/aco_ir.h b/src/amd/compiler/aco_ir.h
index d92a781d8cb97..346ac885815fa 100644
--- a/src/amd/compiler/aco_ir.h
+++ b/src/amd/compiler/aco_ir.h
@@ -2436,7 +2436,8 @@ void select_ps_prolog(Program* program, void* pinfo, ac_shader_config* config,
 void lower_phis(Program* program);
 void lower_subdword(Program* program);
 void calc_min_waves(Program* program);
-void update_vgpr_sgpr_demand(Program* program, const RegisterDemand new_demand);
+void update_vgpr_sgpr_demand(Program* program, const RegisterDemand new_demand,
+                             const RegisterDemand new_real_demand);
 live live_var_analysis(Program* program);
 std::vector<uint16_t> dead_code_analysis(Program* program);
 void dominator_tree(Program* program);
diff --git a/src/amd/compiler/aco_live_var_analysis.cpp b/src/amd/compiler/aco_live_var_analysis.cpp
index 1e5526bb22768..25d77407d12dc 100644
--- a/src/amd/compiler/aco_live_var_analysis.cpp
+++ b/src/amd/compiler/aco_live_var_analysis.cpp
@@ -495,7 +495,8 @@ max_suitable_waves(Program* program, uint16_t waves)
 }
 
 void
-update_vgpr_sgpr_demand(Program* program, const RegisterDemand new_demand)
+update_vgpr_sgpr_demand(Program* program, const RegisterDemand new_demand,
+                        const RegisterDemand new_real_demand)
 {
    assert(program->min_waves >= 1);
    uint16_t sgpr_limit = get_addr_sgpr_from_waves(program, program->min_waves);
@@ -507,9 +508,10 @@ update_vgpr_sgpr_demand(Program* program, const RegisterDemand new_demand)
       program->num_waves = 0;
       program->max_reg_demand = new_demand;
    } else {
-      program->num_waves = program->dev.physical_sgprs / get_sgpr_alloc(program, new_demand.sgpr);
+      program->num_waves =
+         program->dev.physical_sgprs / get_sgpr_alloc(program, new_real_demand.sgpr);
       uint16_t vgpr_demand =
-         get_vgpr_alloc(program, new_demand.vgpr) + program->config->num_shared_vgprs / 2;
+         get_vgpr_alloc(program, new_real_demand.vgpr) + program->config->num_shared_vgprs / 2;
       program->num_waves =
          std::min<uint16_t>(program->num_waves, program->dev.physical_vgprs / vgpr_demand);
       program->num_waves = std::min(program->num_waves, program->dev.max_waves_per_simd);
@@ -530,6 +532,7 @@ live_var_analysis(Program* program)
    unsigned worklist = program->blocks.size();
    std::vector<PhiInfo> phi_info(program->blocks.size());
    RegisterDemand new_demand;
+   RegisterDemand real_new_demand;
 
    program->needs_vcc = program->gfx_level >= GFX10;
 
@@ -550,16 +553,35 @@ live_var_analysis(Program* program)
       result.register_demand[block.index].back().sgpr -= phi_info[block.index].linear_phi_ops;
 
       /* update block's register demand */
-      if (program->progress < CompilationProgress::after_ra)
-         for (RegisterDemand& demand : result.register_demand[block.index])
+      RegisterDemand real_block_demand;
+      if (program->progress < CompilationProgress::after_ra) {
+         for (unsigned i = 0; i < result.register_demand[block.index].size(); ++i) {
+            RegisterDemand& demand = result.register_demand[block.index][i];
+
+            const unsigned max_vgpr = get_addr_vgpr_from_waves(program, program->min_waves);
+            const unsigned max_sgpr = get_addr_sgpr_from_waves(program, program->min_waves);
+
+            const auto* instr = block.instructions[i].get();
+
+            if (instr->isCall() &&
+                instr->call().abi.clobberedRegs.vgpr.hi() == PhysReg{256 + max_vgpr} &&
+                instr->call().abi.clobberedRegs.sgpr.hi() == PhysReg{max_sgpr})
+               real_block_demand.update(demand -
+                                        get_blocked_abi_demand(program, &block, &instr->call(),
+                                                               result.live_out[block.index]));
+            else
+               real_block_demand.update(demand);
             block.register_demand.update(demand);
+         }
+      }
 
       new_demand.update(block.register_demand);
+      real_new_demand.update(real_block_demand);
    }
 
    /* calculate the program's register demand and number of waves */
    if (program->progress < CompilationProgress::after_ra)
-      update_vgpr_sgpr_demand(program, new_demand);
+      update_vgpr_sgpr_demand(program, new_demand, real_new_demand);
 
    return result;
 }
diff --git a/src/amd/compiler/aco_register_allocation.cpp b/src/amd/compiler/aco_register_allocation.cpp
index 70b9056e703d9..70da071b3d0c6 100644
--- a/src/amd/compiler/aco_register_allocation.cpp
+++ b/src/amd/compiler/aco_register_allocation.cpp
@@ -1441,14 +1441,16 @@ increase_register_file(ra_ctx& ctx, RegClass rc)
 {
    if (rc.type() == RegType::vgpr && ctx.num_linear_vgprs == 0 &&
        ctx.vgpr_bounds < ctx.vgpr_limit) {
+      RegisterDemand new_demand =
+         RegisterDemand(ctx.vgpr_bounds + 1, ctx.program->max_reg_demand.sgpr);
       /* If vgpr_bounds is less than max_reg_demand.vgpr, this should be a no-op. */
-      update_vgpr_sgpr_demand(
-         ctx.program, RegisterDemand(ctx.vgpr_bounds + 1, ctx.program->max_reg_demand.sgpr));
+      update_vgpr_sgpr_demand(ctx.program, new_demand, new_demand);
 
       ctx.vgpr_bounds = ctx.program->max_reg_demand.vgpr;
    } else if (rc.type() == RegType::sgpr && ctx.program->max_reg_demand.sgpr < ctx.sgpr_limit) {
-      update_vgpr_sgpr_demand(
-         ctx.program, RegisterDemand(ctx.program->max_reg_demand.vgpr, ctx.sgpr_bounds + 1));
+      RegisterDemand new_demand =
+         RegisterDemand(ctx.program->max_reg_demand.vgpr, ctx.sgpr_bounds + 1);
+      update_vgpr_sgpr_demand(ctx.program, new_demand, new_demand);
 
       ctx.sgpr_bounds = ctx.program->max_reg_demand.sgpr;
    } else {
diff --git a/src/amd/compiler/aco_scheduler.cpp b/src/amd/compiler/aco_scheduler.cpp
index 19560eaf3e932..5ea167075c89d 100644
--- a/src/amd/compiler/aco_scheduler.cpp
+++ b/src/amd/compiler/aco_scheduler.cpp
@@ -1301,10 +1301,34 @@ schedule_program(Program* program, live& live_vars)
 
    /* update max_reg_demand and num_waves */
    RegisterDemand new_demand;
+   RegisterDemand real_new_demand;
    for (Block& block : program->blocks) {
       new_demand.update(block.register_demand);
+      if (block.contains_call) {
+         for (unsigned i = 0; i < block.instructions.size(); ++i) {
+            if (!block.instructions[i]->isCall()) {
+               real_new_demand.update(live_vars.register_demand[block.index][i]);
+               continue;
+            }
+
+            Pseudo_call_instruction* instr = &block.instructions[i]->call();
+
+            const unsigned max_vgpr = get_addr_vgpr_from_waves(program, program->min_waves);
+            const unsigned max_sgpr = get_addr_sgpr_from_waves(program, program->min_waves);
+
+            if (instr->abi.clobberedRegs.vgpr.hi() == PhysReg{256 + max_vgpr} &&
+                instr->abi.clobberedRegs.sgpr.hi() == PhysReg{max_sgpr})
+               real_new_demand.update(
+                  live_vars.register_demand[block.index][i] -
+                  get_blocked_abi_demand(program, &block, instr, live_vars.live_out[block.index]));
+            else
+               real_new_demand.update(live_vars.register_demand[block.index][i]);
+         }
+      } else {
+         real_new_demand.update(block.register_demand);
+      }
    }
-   update_vgpr_sgpr_demand(program, new_demand);
+   update_vgpr_sgpr_demand(program, new_demand, real_new_demand);
 
 /* if enabled, this code asserts that register_demand is updated correctly */
 #if 0
-- 
GitLab


From de6046efd7cf52f7e81ea56c3eee2139a081e404 Mon Sep 17 00:00:00 2001
From: Friedrich Vock <friedrich.vock@gmx.de>
Date: Tue, 4 Jun 2024 15:08:48 +0200
Subject: [PATCH 70/71] aco/spill: Restore registers spilled by call
 immediately

Makes for better latency hiding if we're not short on registers
otherwise.
---
 src/amd/compiler/aco_spill.cpp | 21 +++++++++++++++++++++
 1 file changed, 21 insertions(+)

diff --git a/src/amd/compiler/aco_spill.cpp b/src/amd/compiler/aco_spill.cpp
index 539858bff8208..cf04b98abdb54 100644
--- a/src/amd/compiler/aco_spill.cpp
+++ b/src/amd/compiler/aco_spill.cpp
@@ -990,6 +990,8 @@ process_block(spill_ctx& ctx, unsigned block_idx, Block* block, RegisterDemand s
 
    auto& current_spills = ctx.spills_exit[block_idx];
 
+   std::vector<Temp> call_spills;
+
    while (idx < block->instructions.size()) {
       aco_ptr<Instruction>& instr = block->instructions[idx];
 
@@ -1004,6 +1006,22 @@ process_block(spill_ctx& ctx, unsigned block_idx, Block* block, RegisterDemand s
 
       std::map<Temp, std::pair<Temp, uint32_t>> reloads;
 
+      if (!call_spills.empty()) {
+         RegisterDemand demand = ctx.live_vars.register_demand[block_idx][idx];
+         while (!(demand - spilled_registers).exceeds(ctx.target_pressure) &&
+                !call_spills.empty()) {
+            Temp old_tmp = call_spills.back();
+            call_spills.pop_back();
+
+            Temp new_tmp = ctx.program->allocateTmp(ctx.program->temp_rc[old_tmp.id()]);
+            ctx.renames[block_idx][old_tmp] = new_tmp;
+            reloads[new_tmp] = std::make_pair(old_tmp, current_spills[old_tmp]);
+            current_spills.erase(old_tmp);
+            spilled_registers -= new_tmp;
+         }
+         call_spills.clear();
+      }
+
       /* rename and reload operands */
       for (Operand& op : instr->operands) {
          if (!op.isTemp())
@@ -1136,6 +1154,9 @@ process_block(spill_ctx& ctx, unsigned block_idx, Block* block, RegisterDemand s
             }
 
             uint32_t spill_id = ctx.add_to_spills(to_spill, current_spills);
+            if (instr->isCall())
+               call_spills.emplace_back(to_spill);
+
             /* add interferences with reloads */
             for (std::pair<const Temp, std::pair<Temp, uint32_t>>& pair : reloads)
                ctx.add_interference(spill_id, pair.second.second);
-- 
GitLab


From 4a9e4116d1666b8b696c3aa504dd80ede9abf69a Mon Sep 17 00:00:00 2001
From: Friedrich Vock <friedrich.vock@gmx.de>
Date: Tue, 4 Jun 2024 15:12:21 +0200
Subject: [PATCH 71/71] aco/vn: Don't combine expressions across calls

This increases live state across calls, which in turn increases spilling
and makes for slower shaders overall.
---
 src/amd/compiler/aco_opt_value_numbering.cpp | 24 ++++++++++++++++++++
 1 file changed, 24 insertions(+)

diff --git a/src/amd/compiler/aco_opt_value_numbering.cpp b/src/amd/compiler/aco_opt_value_numbering.cpp
index 2980a9f954c25..02237bc2d191c 100644
--- a/src/amd/compiler/aco_opt_value_numbering.cpp
+++ b/src/amd/compiler/aco_opt_value_numbering.cpp
@@ -43,6 +43,8 @@ struct InstrHash {
       for (const Operand& op : instr->operands)
          hash = murmur_32_scramble(hash, op.constantValue());
 
+      hash = murmur_32_scramble(hash, instr->pass_flags >> 16);
+
       size_t data_size = get_instr_data_size(instr->format);
 
       /* skip format, opcode and pass_flags and op/def spans */
@@ -241,6 +243,9 @@ struct vn_ctx {
    expr_set expr_values;
    aco::unordered_map<uint32_t, Temp> renames;
 
+   /* For each block, a counter of how many calls were encountered in the linear/logical CFG. */
+   std::vector<std::pair<uint32_t, uint32_t>> call_indices;
+
    /* The exec id should be the same on the same level of control flow depth.
     * Together with the check for dominator relations, it is safe to assume
     * that the same exec_id also means the same execution mask.
@@ -255,6 +260,7 @@ struct vn_ctx {
       for (Block& block : program->blocks)
          size += block.instructions.size();
       expr_values.reserve(size);
+      call_indices.resize(program->blocks.size(), {0, 0});
    }
 };
 
@@ -335,6 +341,13 @@ process_block(vn_ctx& ctx, Block& block)
    std::vector<aco_ptr<Instruction>> new_instructions;
    new_instructions.reserve(block.instructions.size());
 
+   uint32_t linear_call_idx = 0;
+   uint32_t logical_call_idx = 0;
+   for (auto index : block.linear_preds)
+      linear_call_idx = std::max(linear_call_idx, ctx.call_indices[index].first);
+   for (auto index : block.logical_preds)
+      logical_call_idx = std::max(logical_call_idx, ctx.call_indices[index].second);
+
    for (aco_ptr<Instruction>& instr : block.instructions) {
       /* first, rename operands */
       for (Operand& op : instr->operands) {
@@ -348,6 +361,10 @@ process_block(vn_ctx& ctx, Block& block)
       if (instr->opcode == aco_opcode::p_discard_if ||
           instr->opcode == aco_opcode::p_demote_to_helper || instr->opcode == aco_opcode::p_end_wqm)
          ctx.exec_id++;
+      if (instr->isCall()) {
+         ++linear_call_idx;
+         ++logical_call_idx;
+      }
 
       /* simple copy-propagation through renaming */
       bool copy_instr =
@@ -364,7 +381,12 @@ process_block(vn_ctx& ctx, Block& block)
          continue;
       }
 
+      bool use_linear_call_idx =
+         std::any_of(instr->definitions.begin(), instr->definitions.end(),
+                     [](const auto& def) { return def.regClass().is_linear(); });
+
       instr->pass_flags = ctx.exec_id;
+      instr->pass_flags |= (use_linear_call_idx ? linear_call_idx : logical_call_idx) << 16;
       std::pair<expr_set::iterator, bool> res = ctx.expr_values.emplace(instr.get(), block.index);
 
       /* if there was already an expression with the same value number */
@@ -397,6 +419,8 @@ process_block(vn_ctx& ctx, Block& block)
       }
    }
 
+   ctx.call_indices[block.index] = {linear_call_idx, logical_call_idx};
+
    block.instructions = std::move(new_instructions);
 }
 
-- 
GitLab

